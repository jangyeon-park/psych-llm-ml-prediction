{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e452bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.config import PROJECT_ROOT, MODEL_SEED, LABELS, MODELS_TO_RUN\n",
    "from src.variables import LLM_COLS, LAB_COLS, CODE_COLS, CATEGORY_COLS\n",
    "from src.preprocessing import create_preprocessor\n",
    "from src.models import get_models_and_search_space, to_bayes_space\n",
    "from src.evaluation import youden_threshold, metrics_at_threshold, decision_curve\n",
    "from src.feature_utils import (\n",
    "    find_feature_list_file, read_feature_list, load_xy,\n",
    "    load_feature_name_map, prettify_name, prettify_names,\n",
    "    clean_ct_feature_name, inject_psych_scale_aliases, prettify_psych_name,\n",
    "    FRIENDLY_OVERRIDES, LEVEL_MAP, to_bayes_space\n",
    ")\n",
    "\n",
    "import os, re, joblib, json, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    precision_recall_fscore_support, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_curve, confusion_matrix,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update({\"figure.dpi\": 130, \"axes.spines.top\": False, \"axes.spines.right\": False})\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# ⚠️⚠️⚠️⚠️0) 경로/조합 설정⚠️⚠️⚠️⚠️\n",
    "# ---------------------------\n",
    "OUT_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMP_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"data/processed_imp/260114_split_corr_LLM_ADER/imputation/simple_imput\")\n",
    ")\n",
    "FS_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/Feature_Selection/simple_20/step2_FS\")\n",
    ")\n",
    "\n",
    "# ⚠️Train/Test 파일 이름⚠️\n",
    "DATA_FILES = {\n",
    "    \"label_30d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_30d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_30d_test.csv\",\n",
    "    },\n",
    "    \"label_60d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_60d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_60d_test.csv\",\n",
    "    },\n",
    "    \"label_90d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_90d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_90d_test.csv\",\n",
    "    },\n",
    "    \"label_180d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_180d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_180d_test.csv\",\n",
    "    },\n",
    "    \"label_365d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_365d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_365d_test.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 당신의 탐색 결과를 반영한 권장조합 (필요시 수정)\n",
    "BEST_COMBOS = {\n",
    "    \"label_30d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_60d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_90d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_180d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_365d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "}\n",
    "\n",
    "TARGET_COL_FALLBACK = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79530cc1",
   "metadata": {},
   "source": [
    "# Variables imported from `src.variables`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35307380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) 유틸: FS/데이터 로드\n",
    "# ---------------------------\n",
    "def find_feature_list_file(label: str) -> Path:\n",
    "    # step2_FS 폴더 내 label 키워드 포함 & 35개 선정 파일을 우선 탐색\n",
    "    # (환경에 맞게 패턴 필요시 조정)\n",
    "    pats = [f\"final_features_{label}.csv\"]\n",
    "    cands = []\n",
    "    for p in pats:\n",
    "        cands += list(FS_DIR.glob(p))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"[FS] No feature list file for {label} in {FS_DIR}\")\n",
    "    return sorted(cands)[-1]\n",
    "\n",
    "\n",
    "def read_feature_list(fpath: Path) -> List[str]:\n",
    "    df = pd.read_csv(fpath)\n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    if \"feature\" in cols_lower:\n",
    "        col = df.columns[cols_lower.index(\"feature\")]\n",
    "    elif \"variable\" in cols_lower:\n",
    "        col = df.columns[cols_lower.index(\"variable\")]\n",
    "    else:\n",
    "        col = df.columns[0]\n",
    "    feats = df[col].dropna().astype(str).tolist()\n",
    "    # 중복 제거 & 공백 정리\n",
    "    feats = list(dict.fromkeys([f.strip() for f in feats]))\n",
    "    return feats\n",
    "\n",
    "\n",
    "def load_xy(\n",
    "    path: Path, label: str, features: List[str]\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    df = pd.read_csv(path)\n",
    "    y_col = (\n",
    "        label\n",
    "        if label in df.columns\n",
    "        else (TARGET_COL_FALLBACK if TARGET_COL_FALLBACK in df.columns else None)\n",
    "    )\n",
    "    if y_col is None:\n",
    "        raise KeyError(\n",
    "            f\"Target column not found in {path.name}. Expected '{label}' or '{TARGET_COL_FALLBACK}'.\"\n",
    "        )\n",
    "    use_cols = [c for c in features if c in df.columns]\n",
    "    missing = sorted(set(features) - set(use_cols))\n",
    "    if missing:\n",
    "        print(\n",
    "            f\"[{label}] Missing {len(missing)} FS features in {path.name}: {missing[:5]}{'...' if len(missing)>5 else ''}\"\n",
    "        )\n",
    "    X = df[use_cols].copy()\n",
    "    y = df[y_col].astype(int).copy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def youden_threshold(y_true: np.ndarray, y_prob: np.ndarray):\n",
    "    \"\"\"\n",
    "    ROC기반 Youden index 최대가 되는 threshold 반환.\n",
    "    반환: thr*, tpr_at_thr, fpr_at_thr\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    youden = tpr - fpr\n",
    "    idx = int(np.nanargmax(youden))\n",
    "    return float(thresholds[idx]), float(tpr[idx]), float(fpr[idx])\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> dict:\n",
    "    \"\"\"\n",
    "    주어진 threshold에서의 성능 지표 계산: Sens, Spec, PPV, NPV, Acc, F1, TP/TN/FP/FN\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Recall\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    # F1 (positive class)\n",
    "    f1 = (2 * ppv * sens) / (ppv + sens) if (ppv + sens) > 0 else 0.0\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": int(tp),\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "    }\n",
    "\n",
    "\n",
    "def to_bayes_space(grid_dict):\n",
    "    \"\"\"\n",
    "    Grid(dict of lists)를 BayesSearchCV의 search_spaces(dict of skopt spaces)로 변환.\n",
    "    리스트는 Categorical로 감쌉니다.\n",
    "    \"\"\"\n",
    "    space = {}\n",
    "    for k, v in grid_dict.items():\n",
    "        # v가 (low, high, type) 같은 튜플이 아니라면 전부 Categorical 처리\n",
    "        # (원하시면 Real/Integer 범위로 바꾸세요)\n",
    "        space[k] = Categorical(v)  # v가 list라고 가정\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802861c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # 1) 변수 깔끔하게 처리\n",
    "# # ---------------------------\n",
    "# LEVEL_MAP = {\n",
    "#     \"sleep\": {\n",
    "#         \"0\": \"Normal sleep\",\n",
    "#         \"1\": \"Insomnia\",\n",
    "#         \"2\": \"Hypersomnia\",\n",
    "#         \"0.0\": \"Normal sleep\",\n",
    "#         \"1.0\": \"Insomnia\",\n",
    "#         \"2.0\": \"Hypersomnia\",\n",
    "#     },\n",
    "#     \"appetite\": {\n",
    "#         \"0\": \"appetite (No change)\",\n",
    "#         \"1\": \"Decreased appetite\",\n",
    "#         \"2\": \"Increased appetite\",\n",
    "#         \"0.0\": \"appetite (No change)\",\n",
    "#         \"1.0\": \"Decreased appetite\",\n",
    "#         \"2.0\": \"Increased appetite\",\n",
    "#     },\n",
    "#     \"weight\": {\n",
    "#         \"0\": \"Weight (No change)\",\n",
    "#         \"1\": \"Weight loss\",\n",
    "#         \"2\": \"Weight gain\",\n",
    "#         \"0.0\": \"Weight (No change)\",\n",
    "#         \"1.0\": \"Weight loss\",\n",
    "#         \"2.0\": \"Weight gain\",\n",
    "#     },\n",
    "# }\n",
    "\n",
    "\n",
    "# def _safe_col(df, candidates):\n",
    "#     for c in candidates:\n",
    "#         if c in df.columns:\n",
    "#             return c\n",
    "#     raise KeyError(f\"Columns not found: {candidates}\")\n",
    "\n",
    "\n",
    "# def load_feature_name_map(csv_path: str | Path) -> dict[str, str]:\n",
    "#     \"\"\"\n",
    "#     feature_summary.csv에서 변수 원칭(예: BL2014) -> 보기 좋은 이름(예: Hematocrit (BL2014)) 매핑을 만든다.\n",
    "#     - 파일의 하단부 'Variabel Mapping'/'검사코드'와 '검사명' 영역을 사용.\n",
    "#     - 중복/결측은 건너뛴다.\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     # 한국어 헤더가 환경에 따라 'Unnamed: 9' 등으로 읽히는 경우를 처리\n",
    "#     code_col = _safe_col(df, [\"Variabel Mapping\", \"Variable\"])\n",
    "#     name_col = _safe_col(df, [\"Unnamed: 9\", \"Term\"])\n",
    "#     # 코드/검사명 행만 필터 (상단부 모델 feature 리스트 행들 제거)\n",
    "#     sub = df[[code_col, name_col]].dropna()\n",
    "#     sub = sub.rename(columns={code_col: \"code\", name_col: \"name\"})\n",
    "#     sub[\"code\"] = sub[\"code\"].astype(str).str.strip()\n",
    "#     sub[\"name\"] = sub[\"name\"].astype(str).str.strip()\n",
    "\n",
    "#     # BL**** 형태만 선택 (원하면 다른 규칙 추가 가능)\n",
    "#     sub = sub[sub[\"code\"].str.match(r\"^BL\\d+\")]\n",
    "#     # 보기 좋은 이름: \"검사명 (코드)\" 형태\n",
    "#     sub[\"pretty\"] = sub[\"name\"] + \" (\" + sub[\"code\"] + \")\"\n",
    "\n",
    "#     # dict: 원코드 -> 예쁜이름\n",
    "#     mapping = dict(zip(sub[\"code\"], sub[\"pretty\"]))\n",
    "\n",
    "#     # 추가로 사람이 읽기 좋은 치환(선택): 밑의 예시처럼 카테고리 변수들도 별칭 줄 수 있음\n",
    "#     friendly_overrides = {\n",
    "#         \"age\": \"Age\",\n",
    "#         \"sex\": \"Sex (M=1)\",\n",
    "#         \"edu\": \"Education Level\",\n",
    "#         \"job\": \"Employment\",\n",
    "#         \"marry\": \"Marital Status\",\n",
    "#         \"smoke\": \"Smoking\",\n",
    "#         \"drink\": \"Drinking\",\n",
    "#         \"benzodiazepine\": \"Benzodiazepine\",\n",
    "#         \"quetiapine\": \"Quetiapine\",\n",
    "#         \"lithium\": \"Lithium\",\n",
    "#         \"substance_abuse\": \"Substance Abuse\",\n",
    "#         \"WorkingMemoryIndex-Compositescore\": \"Working Memory Index\",\n",
    "#         \"PerceptualReasoningIndex-Compositescore\": \"Perceptual Reasoning Index\",\n",
    "#         \"psy_family\": \"Psychiatric family history\",\n",
    "#         \"stay_day\": \"Hospitalization period\",\n",
    "#         \"AD_more_three\": \"≥3 Admissions\",\n",
    "#         \"ER_more_two\": \"≥2 ER Visits\",\n",
    "#         # LLM 8개도 보기 좋게:\n",
    "#         \"aggression\": \"Aggression (LLM)\",\n",
    "#         \"interpersonal_conflict\": \"Interpersonal Conflict (LLM)\",\n",
    "#         \"impaired_social_functioning\": \"Social Function Impairment (LLM)\",\n",
    "#         \"alcohol_use_issues\": \"Alcohol Use Issues (LLM)\",\n",
    "#         \"hallucinations_delusions_paranoia\": \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "#         \"lack_family_social_support\": \"Lack of Support (LLM)\",\n",
    "#         \"loss_of_family\": \"Family Loss (LLM)\",\n",
    "#         \"abuse_and_sexual_victimization\": \"Abuse/Victimization (LLM)\",\n",
    "#         # CODE_COLS (0/1/2인코딩)\n",
    "#         \"sleep\": \"Sleep\",\n",
    "#         \"appetite\": \"Appetite\",\n",
    "#         \"weight\": \"Weight Change\",\n",
    "#     }\n",
    "#     mapping.update(friendly_overrides)\n",
    "#     return mapping\n",
    "\n",
    "\n",
    "# def clean_ct_feature_name(raw_name: str) -> str:\n",
    "#     \"\"\"\n",
    "#     ColumnTransformer가 붙이는 접두사 제거:\n",
    "#     e.g., 'num__BL2014' -> 'BL2014', 'cat__sex' -> 'sex', 'code__sleep' -> 'sleep'\n",
    "#     또한 'num_', 'cat_', 'code_' 같은 2차 접두사도 제거.\n",
    "#     \"\"\"\n",
    "#     name = re.sub(r\"^[^_]+__\", \"\", raw_name)  # 'num__', 'cat__', 'code__' 제거\n",
    "#     name = re.sub(r\"^(num_|cat_|code_)\", \"\", name)  # 'num_', 'cat_', 'code_' 제거\n",
    "#     return name\n",
    "\n",
    "\n",
    "# # def prettify_names(raw_names: list[str], mapping: dict[str, str]) -> list[str]:\n",
    "# #     \"\"\"\n",
    "# #     전처리 후 피처명 리스트(raw_names)를 깔끔하게:\n",
    "# #     1) 접두사 제거 → 2) 매핑 적용 → 3) 매핑 없으면 원래 이름\n",
    "# #     \"\"\"\n",
    "# #     cleaned = [clean_ct_feature_name(n) for n in raw_names]\n",
    "# #     pretty = [mapping.get(n, n) for n in cleaned]\n",
    "# #     return pretty\n",
    "\n",
    "\n",
    "# def prettify_names(raw_name: str) -> str:\n",
    "#     \"\"\"\n",
    "#     'sleep_1.0' → 'Sleep: Insomnia'\n",
    "#     'appetite_2' → 'Appetite: Increased appetite'\n",
    "#     'weight_0' → 'Weight Change: No change'\n",
    "#     그 외 변수는 friendly_overrides 만 적용.\n",
    "#     \"\"\"\n",
    "#     base = clean_ct_feature_name(raw_name)\n",
    "\n",
    "#     # 원-핫 패턴 분리 (예: sleep_1, sleep_1.0)\n",
    "#     m = re.match(r\"^(.*?)[_](.+)$\", base)\n",
    "#     if m:\n",
    "#         var, level = m.group(1), m.group(2)\n",
    "#         var_friendly = friendly_overrides.get(var, var)\n",
    "#         level_friendly = LEVEL_MAP.get(var, {}).get(level)\n",
    "#         if level_friendly is not None:\n",
    "#             return f\"{var_friendly}: {level_friendly}\"\n",
    "#         # level 매핑이 없으면 있는 그대로 표기\n",
    "#         return f\"{var_friendly}: {level}\"\n",
    "#     # 원-핫이 아닌 일반 변수\n",
    "#     return friendly_overrides.get(base, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) 전역 상수로 이동 (또는 파일 상단에 정의)\n",
    "FRIENDLY_OVERRIDES = {\n",
    "    \"age\": \"Age\",\n",
    "    \"sex\": \"Sex (M=1)\",\n",
    "    \"edu\": \"Education Level\",\n",
    "    \"job\": \"Employment\",\n",
    "    \"marry\": \"Marital Status\",\n",
    "    \"smoke\": \"Smoking\",\n",
    "    \"drink\": \"Drinking\",\n",
    "    \"benzodiazepine\": \"Benzodiazepine\",\n",
    "    \"quetiapine\": \"Quetiapine\",\n",
    "    \"lithium\": \"Lithium\",\n",
    "    \"divalproex\": \"Divalproex\",\n",
    "    \"substance_abuse\": \"Substance Abuse\",\n",
    "    \"Suicidalattempt\": \"Suicidal Attempt\",\n",
    "    \"Suicidalplan\": \"Suicidal Plan\",\n",
    "    \"trauma_stressor_related\": \"Trauma Stress\",\n",
    "    \"WorkingMemoryIndex-Compositescore\": \"Working Memory\",\n",
    "    \"PerceptualReasoningIndex-Compositescore\": \"Perceptual Reasoning\",\n",
    "    \"ProcessingSpeedIndex-Compositescore\": \"Processing Speed\",\n",
    "    \"psy_family\": \"Psychiatric family history\",\n",
    "    \"stay_day\": \"Hospitalization period\",\n",
    "    \"AD_more_three\": \"≥3 Admissions\",\n",
    "    \"ER_more_two\": \"≥2 ER Visits\",\n",
    "    \"psychotic_other\": \"Other Psychotic\",\n",
    "    \"somatic_symptom_disorder\": \"Somatic Symptom Disorder\",\n",
    "    \"anxiety\": \"Anxiety\",\n",
    "    # LLM\n",
    "    \"Impaired_Social_Function\": \"Social Function Impairment (LLM)\",\n",
    "    \"Religious_Affiliation\": \"Religious Affiliation (LLM)\",\n",
    "    \"Violence_and_Impulsivity\": \"Aggression/Impulsivity (LLM)\",\n",
    "    \"Domestic_Violence\": \"Domestic Violence (LLM)\",\n",
    "    \"Physical_Abuse\": \"Physical Abuse (LLM)\",\n",
    "    \"Divorce\": \"Divorce Experience (LLM)\",\n",
    "    \"Death_of_Family_Member\": \"Family Loss (LLM)\",\n",
    "    \"Emotional_Abuse\": \"Emotional Abuse (LLM)\",\n",
    "    \"Lack_of_Family_Support\": \"Lack of Family Support (LLM)\",\n",
    "    \"Social_Isolation_and_Lack_of_Support\": \"Social Isolation (LLM)\",\n",
    "    \"Psychotic_Symptoms\": \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "    \"Interpersonal_Conflict\": \"Interpersonal Conflict (LLM)\",\n",
    "    \"Exposure_to_Suicide\": \"Suicide Exposure (LLM)\",\n",
    "    \"Alcohol_Use_Problems\": \"Alcohol Use Issues (LLM)\",\n",
    "    \"Sexual_Abuse\": \"Sexual Victimization (LLM)\",\n",
    "    \"Physical_and_Emotional_Neglect\": \"Neglect (LLM)\",\n",
    "    # CODE_COLS\n",
    "    \"sleep\": \"Sleep\",\n",
    "    \"appetite\": \"Appetite\",\n",
    "    \"weight\": \"Weight Change\",\n",
    "}\n",
    "\n",
    "# 1) LEVEL_MAP 정리: 중복되는 변수명 제거 + 레벨 키 정규화에 대비\n",
    "LEVEL_MAP = {\n",
    "    \"sleep\": {\n",
    "        \"0\": \"Normal sleep\",\n",
    "        \"1\": \"Insomnia\",\n",
    "        \"2\": \"Hypersomnia\",\n",
    "    },\n",
    "    \"appetite\": {\n",
    "        \"0\": \"No change\",\n",
    "        \"1\": \"Decreased appetite\",\n",
    "        \"2\": \"Increased appetite\",\n",
    "    },\n",
    "    \"weight\": {\n",
    "        \"0\": \"No change\",\n",
    "        \"1\": \"Weight loss\",\n",
    "        \"2\": \"Weight gain\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def _norm_level(level: str) -> str:\n",
    "    try:\n",
    "        return str(int(float(level)))\n",
    "    except Exception:\n",
    "        return str(level).strip()\n",
    "\n",
    "\n",
    "def _safe_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"Columns not found: {candidates}\")\n",
    "\n",
    "\n",
    "def load_feature_name_map(csv_path: str | Path) -> dict[str, str]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    code_col = _safe_col(df, [\"Variabel Mapping\", \"Variable\"])\n",
    "    name_col = _safe_col(df, [\"Unnamed: 9\", \"Term\"])\n",
    "    sub = df[[code_col, name_col]].dropna()\n",
    "    sub = sub.rename(columns={code_col: \"code\", name_col: \"name\"})\n",
    "    sub[\"code\"] = sub[\"code\"].astype(str).str.strip()\n",
    "    sub[\"name\"] = sub[\"name\"].astype(str).str.strip()\n",
    "    sub = sub[sub[\"code\"].str.match(r\"^BL\\d+\")]\n",
    "    sub[\"pretty\"] = sub[\"name\"] + \" (\" + sub[\"code\"] + \")\"\n",
    "    mapping = dict(zip(sub[\"code\"], sub[\"pretty\"]))\n",
    "    # 전역 오버라이드도 함께 병합해 한 딕셔너리로 사용\n",
    "    mapping.update(FRIENDLY_OVERRIDES)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def clean_ct_feature_name(raw_name: str) -> str:\n",
    "    name = re.sub(r\"^[^_]+__\", \"\", raw_name)  # 'num__', 'cat__', 'code__'\n",
    "    name = re.sub(r\"^(num_|cat_|code_)\", \"\", name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def prettify_name(raw_name: str, mapping: dict[str, str] | None = None) -> str:\n",
    "    \"\"\"\n",
    "    'sleep_1.0' → 'Sleep: Insomnia'\n",
    "    'appetite_2' → 'Appetite: Increased appetite'\n",
    "    'weight_0' → 'Weight Change: No change'\n",
    "    BL코드/일반 변수는 mapping(+FRIENDLY_OVERRIDES)에 따라 변환.\n",
    "    \"\"\"\n",
    "    base = clean_ct_feature_name(raw_name)\n",
    "    mapping = mapping or {}\n",
    "\n",
    "    m = re.match(r\"^(.*?)[_](.+)$\", base)\n",
    "    if m:\n",
    "        var, level = m.group(1), _norm_level(m.group(2))\n",
    "        var_friendly = mapping.get(var, FRIENDLY_OVERRIDES.get(var, var))\n",
    "        level_friendly = LEVEL_MAP.get(var, {}).get(level)\n",
    "        if level_friendly is not None:\n",
    "            return f\"{var_friendly}: {level_friendly}\"\n",
    "        # 원-핫이지만 레벨 매핑이 없으면 원문 유지\n",
    "        return f\"{var_friendly}: {level}\"\n",
    "\n",
    "    # 원-핫이 아닌 일반 변수(BL코드 포함)\n",
    "    return mapping.get(base, FRIENDLY_OVERRIDES.get(base, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_psych_scale_aliases(mapping: dict[str, str]) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    L, Pd, Vr, mf 등 심리검사 약어 → 사람이 읽기 좋은 라벨로 보강.\n",
    "    - 대소문자 무시\n",
    "    - 접미어(_t, _raw 등)가 있어도 동작\n",
    "    \"\"\"\n",
    "    # 기본 약어 사전 (필요시 여기 계속 추가)\n",
    "    base_alias = {\n",
    "        # MMPI Validity & Clinical\n",
    "        \"L\": \"MMPI Validity: Lie (L)\",\n",
    "        \"Pd\": \"MMPI Clinical: Psychopathic Deviate (Pd)\",\n",
    "        \"Mf\": \"MMPI Clinical: Masculinity–Femininity (Mf)\",\n",
    "        \"mf\": \"MMPI Clinical: Masculinity–Femininity (Mf)\",\n",
    "        \"TR\": \"MMPI Validity: TRIN\",  # (True response inconsistency)\n",
    "        \"Vr\": \"MMPI Validity: VRIN\",  # (Variable Response Inconsistency)\n",
    "        \"F\": \"MMPI Validity: Infrequency (F)\",\n",
    "        \"K\": \"MMPI Validity: Defensiveness (K)\",\n",
    "        \"Hs\": \"MMPI Clinical: Hypochondriasis (Hs)\",\n",
    "        \"D\": \"MMPI Clinical: Depression (D)\",\n",
    "        \"Hy\": \"MMPI Clinical: Hysteria (Hy)\",\n",
    "        \"Pa\": \"MMPI Clinical: Paranoia (Pa)\",\n",
    "        \"Sc\": \"MMPI Clinical: Schizophrenia (Sc)\",\n",
    "        \"Ma\": \"MMPI Clinical: Hypomania (Ma)\",\n",
    "        \"Si\": \"MMPI Clinical: Social Introversion (Si)\",\n",
    "    }\n",
    "\n",
    "    # 대소문자 무시 매칭을 위해 보조 룰 추가:\n",
    "    # - 'pd', 'PD', 'Pd_t', 'mf_raw'처럼 변형된 컬럼명에도 적용\n",
    "    def add_case_insensitive(alias_key: str, pretty: str):\n",
    "        # 정확히 일치하는 키도 덮어씀\n",
    "        mapping[alias_key] = pretty\n",
    "        # 자주 보이는 변형 패턴 → 정규식 룰\n",
    "        # 1) 소문자/대문자 변형\n",
    "        mapping[alias_key.lower()] = pretty\n",
    "        mapping[alias_key.upper()] = pretty\n",
    "        # 2) 접미어(_t, _raw, _score, _z 등)\n",
    "        #   → 정규식으로 처리할 수 있도록, 나중 단계 prettify에서 사용\n",
    "        #   여기선 원키만 심어두고, 정규식은 prettify 단계에서 적용\n",
    "        #   (아래 prettify_psych_name에서 처리)\n",
    "        return\n",
    "\n",
    "    for k, v in base_alias.items():\n",
    "        add_case_insensitive(k, v)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def prettify_psych_name(clean_name: str, mapping: dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    clean_name(접두사 제거된 원 컬럼명)에 심리검사 스케일 약어 매핑 적용.\n",
    "    - 대소문자 무시\n",
    "    - 접미어(_t/_raw 등)는 유지하되, 앞의 약어만 사람이 읽는 표기로 교체\n",
    "    \"\"\"\n",
    "    # 원본 보존\n",
    "    name = clean_name\n",
    "\n",
    "    # 약어 + 선택적 접미어 패턴: 예) 'Pd', 'pd_t', 'MF_raw', 'Vr_score'\n",
    "    m = re.match(r\"^([A-Za-z]{1,3})(?:[_\\-](.+))?$\", name)\n",
    "    if m:\n",
    "        key = m.group(1)  # Pd / pd / MF / Vr ...\n",
    "        suffix = m.group(2)  # t / raw / z 등 (없을 수도)\n",
    "        # 매핑에 있으면 교체\n",
    "        pretty_core = mapping.get(\n",
    "            key, mapping.get(key.lower(), mapping.get(key.upper(), None))\n",
    "        )\n",
    "        if pretty_core:\n",
    "            if suffix:\n",
    "                # 접미어는 괄호 뒤에 그대로 덧붙여서 정보 보존\n",
    "                return f\"{pretty_core} [{suffix}]\"\n",
    "            else:\n",
    "                return pretty_core\n",
    "    # 매칭 안 되면 원래 이름 반환\n",
    "    return name\n",
    "\n",
    "\n",
    "def prettify_names(raw_names: list[str], mapping: dict[str, str]) -> list[str]:\n",
    "    # (1) ColumnTransformer 접두사 제거\n",
    "    cleaned = [clean_ct_feature_name(n) for n in raw_names]\n",
    "\n",
    "    # ★ (2) 심리검사 약어 보정 먼저 처리\n",
    "    mapping = inject_psych_scale_aliases(mapping)\n",
    "    cleaned2 = [prettify_psych_name(n, mapping) for n in cleaned]\n",
    "\n",
    "    # (3) BL코드 / LLM 등 일반 매핑 적용\n",
    "    pretty = [mapping.get(n, n) for n in cleaned2]\n",
    "    return pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab015a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_MAP_CSV = str(PROJECT_ROOT / \"results/Feature_selection/w_LLM_v5/step2_FS/feature_summary.csv\")\n",
    "NAME_MAP = load_feature_name_map(FEATURE_MAP_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) 전처리 & 모델 팩토리/그리드\n",
    "# ---------------------------\n",
    "def create_preprocessor(numeric_features, categorical_features, code_features):\n",
    "    \"\"\"연속형 스케일링, 범주형 One-hot 인코딩을 위한 ColumnTransformer 생성\"\"\"\n",
    "    transformers = []\n",
    "    if numeric_features:\n",
    "        transformers.append((\"num\", StandardScaler(), numeric_features))\n",
    "    if categorical_features:  # 일반 범주형은 그대로 통과 (트리 모델용)\n",
    "        transformers.append((\"cat\", \"passthrough\", categorical_features))\n",
    "    if code_features:\n",
    "        transformers.append(\n",
    "            (\"code\", OneHotEncoder(handle_unknown=\"ignore\"), code_features)\n",
    "        )\n",
    "\n",
    "    return ColumnTransformer(transformers, remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "def get_models_and_search_space():\n",
    "    \"\"\"5개 모델과 BayesSearchCV 탐색 공간을 반환\"\"\"\n",
    "    models = {\n",
    "        \"LR\": (\n",
    "            LogisticRegression(random_state=42, max_iter=500),\n",
    "            {\n",
    "                \"clf__penalty\": [\"elasticnet\"],\n",
    "                \"clf__C\": [1e-4, 1e-3, 1e-2, 0.1, 1.0, 10.0, 100.0],\n",
    "                \"clf__solver\": [\"saga\"],\n",
    "                \"clf__l1_ratio\": [\n",
    "                    0.0,\n",
    "                    0.25,\n",
    "                    0.5,\n",
    "                    0.75,\n",
    "                    1.0,\n",
    "                ],  # only effective when penalty=\"elasticnet\"\n",
    "            },\n",
    "        ),\n",
    "        \"RF\": (\n",
    "            RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "            {\n",
    "                \"clf__n_estimators\": [100, 200, 300, 500, 1000],\n",
    "                \"clf__max_depth\": [None, 5, 10, 15, 20, 30],\n",
    "                \"clf__min_samples_split\": [2, 5, 10],\n",
    "                \"clf__min_samples_leaf\": [1, 2, 4, 10],\n",
    "                \"clf__max_features\": [\"sqrt\", \"log2\", None, 0.5, 0.7, 1.0],\n",
    "                \"clf__bootstrap\": [True, False],\n",
    "            },\n",
    "        ),\n",
    "        \"XGB\": (\n",
    "            XGBClassifier(\n",
    "                use_label_encoder=False,\n",
    "                eval_metric=\"logloss\",\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                tree_method=\"hist\",\n",
    "            ),\n",
    "            {\n",
    "                \"clf__n_estimators\": [200, 500, 1000],\n",
    "                \"clf__max_depth\": [3, 5, 7, 10],\n",
    "                \"clf__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "                \"clf__subsample\": [0.7, 0.9, 1.0],\n",
    "                \"clf__colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "                \"clf__min_child_weight\": [1, 3, 5],\n",
    "                \"clf__gamma\": [0, 0.1, 0.2, 0.5],\n",
    "                \"clf__reg_alpha\": [0, 0.1, 1, 10],\n",
    "                \"clf__reg_lambda\": [0.1, 1, 10, 50],\n",
    "            },\n",
    "        ),\n",
    "        \"LGBM\": (\n",
    "            LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1),\n",
    "            {\n",
    "                \"clf__n_estimators\": [200, 500, 1000, 2000],\n",
    "                \"clf__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "                \"clf__max_depth\": [-1, 5, 7, 10, 15],\n",
    "                \"clf__num_leaves\": [31, 63, 127],\n",
    "                \"clf__min_data_in_leaf\": [10, 20, 50],\n",
    "                \"clf__feature_fraction\": [0.7, 0.9, 1.0],\n",
    "                \"clf__bagging_fraction\": [0.7, 0.9, 1.0],\n",
    "                \"clf__bagging_freq\": [0, 1, 5],\n",
    "                \"clf__lambda_l1\": [0, 0.1, 0.5, 1.0, 5.0],\n",
    "                \"clf__lambda_l2\": [0, 0.1, 0.5, 1.0, 5.0],\n",
    "            },\n",
    "        ),\n",
    "        \"CATBOOST\": (\n",
    "            CatBoostClassifier(\n",
    "                random_state=42, verbose=0, thread_count=-1, loss_function=\"Logloss\"\n",
    "            ),\n",
    "            {\n",
    "                \"clf__iterations\": [300, 600, 1000, 1500],\n",
    "                \"clf__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "                \"clf__depth\": [4, 5, 6, 8, 10],\n",
    "                \"clf__l2_leaf_reg\": [1, 3, 5, 10],\n",
    "                \"clf__border_count\": [32, 64, 128],\n",
    "                \"clf__bagging_temperature\": [0, 0.5, 1, 2],\n",
    "                \"clf__random_strength\": [0, 0.5, 1, 2],\n",
    "            },\n",
    "        ),\n",
    "    }\n",
    "    return {name: models[name] for name in MODELS_TO_RUN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfeb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) DCA (Decision Curve Analysis)\n",
    "# ---------------------------\n",
    "def decision_curve(\n",
    "    y_true: np.ndarray, y_prob: np.ndarray, thresholds: np.ndarray = None\n",
    ") -> pd.DataFrame:\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    out = []\n",
    "    N = len(y_true)\n",
    "    for th in thresholds:\n",
    "        y_pred = (y_prob >= th).astype(int)\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        nb = (tp / N) - (fp / N) * (th / (1 - th))\n",
    "        out.append((th, nb))\n",
    "    return pd.DataFrame(out, columns=[\"threshold\", \"net_benefit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Nested-CV on TRAIN → TEST 평가\n",
    "# ---------------------------\n",
    "def run_for_label(\n",
    "    label: str, model_name: str, feature_set: str, random_state: int = 42\n",
    ") -> Dict:\n",
    "    print(f\"\\n==== {label} | model={model_name} | features={feature_set}====\")\n",
    "    # FS\n",
    "    fs_file = find_feature_list_file(label)\n",
    "    fs_feats = read_feature_list(fs_file)\n",
    "\n",
    "    # --- feature_set에 따라 LLM/Lab 컬럼 추가 ---\n",
    "    fs_low = feature_set.lower()\n",
    "    base_features = list(fs_feats)  # 원본 FS 리스트\n",
    "\n",
    "    if fs_low == \"all_features\":\n",
    "        # 1) All_Features: 불러온 FS + LLM\n",
    "        # (정의: 불러온 feature 파일에서 LLM cols만 추가)\n",
    "        final_features_list = [*base_features, *LLM_COLS]\n",
    "\n",
    "    elif fs_low == \"base_lab\":\n",
    "        # 2) Base_Lab: 불러온 FS 그대로\n",
    "        # (정의: 그냥 불러온 feature 파일 사용)\n",
    "        final_features_list = base_features\n",
    "\n",
    "    elif fs_low == \"base\":\n",
    "        # 3) Base: 불러온 FS에서 Lab 제외\n",
    "        # (정의: 불러온 feature 파일에서 lab_cols에 해당하는 변수가 있으면 제외)\n",
    "        lab_cols_set = set(LAB_COLS)\n",
    "        final_features_list = [f for f in base_features if f not in lab_cols_set]\n",
    "\n",
    "    elif fs_low == \"base_llm\":\n",
    "        # 4) Base_LLM: 불러온 FS에서 Lab 제외 + LLM 추가\n",
    "        # (정의: 불러온 feature 파일에서 lab_cols 제외하고 LLM cols 추가)\n",
    "        lab_cols_set = set(LAB_COLS)\n",
    "        base_only = [f for f in base_features if f not in lab_cols_set]\n",
    "        final_features_list = [*base_only, *LLM_COLS]\n",
    "\n",
    "    else:\n",
    "        # 정의되지 않은 경우, 기본 'Base_Lab'으로 동작 (fs_feats 그대로 사용)\n",
    "        print(\n",
    "            f\"[Warning] Unknown feature_set '{feature_set}'. Defaulting to 'Base_Lab' logic (using fs_feats as-is).\"\n",
    "        )\n",
    "        final_features_list = base_features\n",
    "\n",
    "    # 중복 제거 (순서 유지)\n",
    "    features_final = list(dict.fromkeys(final_features_list))\n",
    "\n",
    "    print(f\"  [Info] Using {len(features_final)} features for '{feature_set}'.\")\n",
    "\n",
    "    # Load Train/Test\n",
    "    train_path = DATA_FILES[label][\"train\"]\n",
    "    test_path = DATA_FILES[label][\"test\"]\n",
    "    X_tr, y_tr = load_xy(train_path, label, features_final)\n",
    "    X_te, y_te = load_xy(test_path, label, features_final)\n",
    "\n",
    "    # --- 전처리기: 카테고리/코드/수치 분리 (교집합만) ---\n",
    "    # LLM/CATEGORY/CODE 리스트 중 실제 존재하는 것만 사용\n",
    "    categorical_features = [c for c in CATEGORY_COLS if c in X_tr.columns]\n",
    "    code_features = [c for c in CODE_COLS if c in X_tr.columns]\n",
    "    # 수치형 = 나머지\n",
    "    numeric_features = [\n",
    "        c for c in X_tr.columns if c not in set(categorical_features + code_features)\n",
    "    ]\n",
    "\n",
    "    preprocessor = create_preprocessor(\n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=categorical_features,\n",
    "        code_features=code_features,\n",
    "    )\n",
    "\n",
    "    # --- 모델 & 탐색공간 ---\n",
    "    models = get_models_and_search_space()\n",
    "    clf, grid = models[model_name.upper()]\n",
    "    search_space = to_bayes_space(grid)\n",
    "\n",
    "    # Pipeline (selector → SMOTE → estimator[+scaler])\n",
    "    pipe = ImbPipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"smote\", SMOTE(sampling_strategy=\"minority\", random_state=42)),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Inner search\n",
    "    inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    search = BayesSearchCV(\n",
    "        estimator=pipe,\n",
    "        search_spaces=search_space,\n",
    "        n_iter=50,  # code test : 5, Real test : 30 or 50\n",
    "        cv=inner,\n",
    "        scoring=\"roc_auc\",\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Outer CV on Train (OOF)\n",
    "    outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    oof_prob = np.zeros(len(y_tr))\n",
    "    oof_pred = np.zeros(len(y_tr), dtype=int)\n",
    "    oof_fold = np.zeros(len(y_tr), dtype=int)\n",
    "\n",
    "    fold_metrics = []\n",
    "    for f, (tr_idx, va_idx) in enumerate(outer.split(X_tr, y_tr), start=1):\n",
    "        X_tr_i, X_va_i = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "        y_tr_i, y_va_i = y_tr.iloc[tr_idx], y_tr.iloc[va_idx]\n",
    "        search.fit(X_tr_i, y_tr_i)\n",
    "        best = search.best_estimator_\n",
    "\n",
    "        proba = best.predict_proba(X_va_i)[:, 1]\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "        oof_prob[va_idx] = proba\n",
    "        oof_pred[va_idx] = pred\n",
    "        oof_fold[va_idx] = f\n",
    "\n",
    "        auc = roc_auc_score(y_va_i, proba)\n",
    "        prauc = average_precision_score(y_va_i, proba)\n",
    "        brier = brier_score_loss(y_va_i, proba)\n",
    "        acc = accuracy_score(y_va_i, pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_va_i, pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "\n",
    "        fold_metrics.append(\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"fold\": f,\n",
    "                \"model\": model_name,\n",
    "                \"auc\": auc,\n",
    "                \"prauc\": prauc,\n",
    "                \"brier\": brier,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": prec,\n",
    "                \"recall\": rec,\n",
    "                \"f1\": f1,\n",
    "                \"best_params\": search.best_params_,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  [Outer {f}] AUC={auc:.3f} | PR-AUC={prauc:.3f} | F1={f1:.3f}\")\n",
    "\n",
    "    # 저장: OOF & Fold metrics & summary\n",
    "    oof_df = pd.DataFrame(\n",
    "        {\n",
    "            \"label\": label,\n",
    "            \"oof_fold\": oof_fold,\n",
    "            \"y_true\": y_tr.values,\n",
    "            \"y_prob\": oof_prob,\n",
    "            \"y_pred\": oof_pred,\n",
    "        }\n",
    "    )\n",
    "    oof_df.to_csv(OUT_DIR / f\"{label}__oof_predictions.csv\", index=False)\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_metrics)\n",
    "    fold_df.to_csv(OUT_DIR / f\"{label}__train_fold_metrics.csv\", index=False)\n",
    "\n",
    "    train_summary = (\n",
    "        fold_df.agg(\n",
    "            {\n",
    "                \"auc\": [\"mean\", \"std\"],\n",
    "                \"prauc\": [\"mean\", \"std\"],\n",
    "                \"brier\": [\"mean\", \"std\"],\n",
    "                \"accuracy\": [\"mean\", \"std\"],\n",
    "                \"precision\": [\"mean\", \"std\"],\n",
    "                \"recall\": [\"mean\", \"std\"],\n",
    "                \"f1\": [\"mean\", \"std\"],\n",
    "            }\n",
    "        )\n",
    "        .T.reset_index()\n",
    "        .rename(columns={\"index\": \"metric\"})\n",
    "    )\n",
    "    train_summary.insert(0, \"split\", \"train_oof\")\n",
    "    train_summary.insert(0, \"model\", model_name)\n",
    "    train_summary.insert(0, \"label\", label)\n",
    "\n",
    "    # OOF mean±SD 요약 저장 (Nested-CV 결과 제시용)\n",
    "    train_summary.to_csv(OUT_DIR / f\"{label}__train_oof_summary.csv\", index=False)\n",
    "\n",
    "    # ⭐️⭐️⭐️ --- (A) OOF에서 Youden index 기반 최적 threshold --- ⭐️⭐️⭐️\n",
    "    thr_oof, tpr_oof, fpr_oof = youden_threshold(\n",
    "        oof_df[\"y_true\"].values, oof_df[\"y_prob\"].values\n",
    "    )\n",
    "    youden_oof = tpr_oof - fpr_oof\n",
    "    oof_thr_metrics = metrics_at_threshold(\n",
    "        oof_df[\"y_true\"].values, oof_df[\"y_prob\"].values, thr_oof\n",
    "    )\n",
    "\n",
    "    # 저장\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"model\": model_name,\n",
    "                \"split\": \"train_oof\",\n",
    "                \"youden_index\": youden_oof,\n",
    "                **oof_thr_metrics,\n",
    "            }\n",
    "        ]\n",
    "    ).to_csv(OUT_DIR / f\"{label}__youden_metrics_OOF.csv\", index=False)\n",
    "    # ⭐️⭐️⭐️ --- (A) OOF에서 Youden index 기반 최적 threshold --- ⭐️⭐️⭐️\n",
    "\n",
    "    # OOF ROC / Calibration / DCA\n",
    "    fpr, tpr, _ = roc_curve(oof_df[\"y_true\"], oof_df[\"y_prob\"])\n",
    "    auc_oof = roc_auc_score(oof_df[\"y_true\"], oof_df[\"y_prob\"])\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC={auc_oof:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\")\n",
    "    # ⭐️⭐️⭐️Youden 최적점 표시 ⭐️⭐️⭐️\n",
    "    plt.scatter(\n",
    "        [fpr_oof], [tpr_oof], s=30, marker=\"o\", label=f\"Youden*: thr={thr_oof:.3f}\"\n",
    "    )\n",
    "    # ⭐️⭐️⭐️Youden 최적점 표시 ⭐️⭐️⭐️\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC (OOF) - {label} ({model_name})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__ROC_OOF.png\")\n",
    "    plt.close()\n",
    "\n",
    "    pt, pp = calibration_curve(oof_df[\"y_true\"], oof_df[\"y_prob\"], n_bins=10)\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.plot(pp, pt, marker=\"o\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Observed\")\n",
    "    plt.title(\n",
    "        f\"Calibration (OOF) - {label} ({model_name})\\nBrier={brier_score_loss(oof_df['y_true'], oof_df['y_prob']):.3f}\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__Calibration_OOF.png\")\n",
    "    plt.close()\n",
    "\n",
    "    dca_oof = decision_curve(oof_df[\"y_true\"].values, oof_df[\"y_prob\"].values)\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.plot(dca_oof[\"threshold\"], dca_oof[\"net_benefit\"], lw=2, label=\"Model\")\n",
    "    prev = oof_df[\"y_true\"].mean()\n",
    "    treat_all = dca_oof[\"threshold\"].apply(\n",
    "        lambda th: prev - (1 - prev) * (th / (1 - th))\n",
    "    )\n",
    "    plt.plot(dca_oof[\"threshold\"], treat_all, \"--\", label=\"Treat-all\")\n",
    "    plt.plot(\n",
    "        dca_oof[\"threshold\"],\n",
    "        np.zeros_like(dca_oof[\"threshold\"]),\n",
    "        \"--\",\n",
    "        label=\"Treat-none\",\n",
    "    )\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Net benefit\")\n",
    "    plt.title(f\"DCA (OOF) - {label} ({model_name})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__DCA_OOF.png\")\n",
    "    plt.close()\n",
    "    dca_oof.to_csv(OUT_DIR / f\"{label}__DCA_OOF.csv\", index=False)\n",
    "\n",
    "    # Train full로 재학습 → Test 평가\n",
    "    print(\"  [Final fit on full Train] ...\")\n",
    "    search.fit(X_tr, y_tr)\n",
    "    best_full = search.best_estimator_\n",
    "\n",
    "    # --- ⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️ ---\n",
    "    # --- ⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️ ---\n",
    "    # (A) 최종 파이프라인 저장 (preprocessor + SMOTE + clf 모두 포함)\n",
    "    model_path = OUT_DIR / f\"{label}__best_model.pkl\"\n",
    "    joblib.dump(best_full, model_path)\n",
    "\n",
    "    # (B) 이 라벨에서 실제로 사용한 \"원본 컬럼명 리스트\" 저장 (재현용)\n",
    "    features_path = OUT_DIR / f\"{label}__features_final.json\"\n",
    "    with open(features_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(features_final, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # (C) 전처리 후 피처명 저장 (원핫 포함된 최종 입력 피처명)\n",
    "    preproc = best_full.named_steps[\"preprocessor\"]\n",
    "    try:\n",
    "        raw_names = list(preproc.get_feature_names_out())\n",
    "    except Exception:\n",
    "        raw_names = [f\"f{i}\" for i in range(preproc.transform(X_tr.iloc[:1]).shape[1])]\n",
    "\n",
    "    # 접두사 정리(선택): num__/cat__/code__ 및 num_/cat_/code_ 제거\n",
    "    import re\n",
    "\n",
    "    def clean_name(name: str) -> str:\n",
    "        name = re.sub(r\"^[^_]+__\", \"\", name)\n",
    "        name = re.sub(r\"^(num_|cat_|code_)\", \"\", name)\n",
    "        return name\n",
    "\n",
    "    # feat_names_clean = [clean_name(n) for n in raw_names]\n",
    "    feat_names_clean = prettify_names(raw_names, NAME_MAP)\n",
    "    featnames_path = OUT_DIR / f\"{label}__transformed_feature_names.json\"\n",
    "    with open(featnames_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(feat_names_clean, f, ensure_ascii=False, indent=2)\n",
    "    # --- ⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️ ---\n",
    "    # --- ⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️⭐️ ---\n",
    "\n",
    "    # --- ⭐️⭐️⭐️ (추가) 최종 모델의 best_params 저장 ⭐️⭐️⭐️ ---\n",
    "    final_best_params = search.best_params_\n",
    "    pd.DataFrame([{\"label\": label, \"model\": model_name, **final_best_params}]).to_csv(\n",
    "        OUT_DIR / f\"{label}__best_params_final.csv\", index=False\n",
    "    )\n",
    "\n",
    "    prob_te = best_full.predict_proba(X_te)[:, 1]\n",
    "    pred_te = (prob_te >= thr_oof).astype(int)\n",
    "\n",
    "    # Test metrics\n",
    "    auc_te = roc_auc_score(y_te, prob_te)\n",
    "    pr_te = average_precision_score(y_te, prob_te)\n",
    "    brier_te = brier_score_loss(y_te, prob_te)\n",
    "    acc_te = accuracy_score(y_te, pred_te)\n",
    "    prec_te, rec_te, f1_te, _ = precision_recall_fscore_support(\n",
    "        y_te, pred_te, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # --- ⭐️⭐️⭐️ Youden(Test) 먼저 계산 → 이후 test_metrics에 합치기 ⭐️⭐️⭐️ ---\n",
    "    thr_te, tpr_te, fpr_te = youden_threshold(y_te.values, prob_te)\n",
    "    youden_te = tpr_te - fpr_te\n",
    "    test_thr_metrics = metrics_at_threshold(y_te.values, prob_te, thr_te)\n",
    "\n",
    "    test_metrics = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"model\": model_name,\n",
    "                \"split\": \"test\",\n",
    "                \"auc\": auc_te,\n",
    "                \"prauc\": pr_te,\n",
    "                \"brier\": brier_te,\n",
    "                \"accuracy\": acc_te,\n",
    "                \"precision\": prec_te,\n",
    "                \"recall\": rec_te,\n",
    "                \"f1\": f1_te,\n",
    "                \"best_params\": search.best_params_,\n",
    "                \"youden_index\": youden_te,\n",
    "                \"thr_youden\": thr_te,\n",
    "                \"sens_at_thr\": test_thr_metrics[\"sensitivity\"],\n",
    "                \"spec_at_thr\": test_thr_metrics[\"specificity\"],\n",
    "                \"ppv_at_thr\": test_thr_metrics[\"ppv\"],\n",
    "                \"npv_at_thr\": test_thr_metrics[\"npv\"],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    test_metrics.to_csv(OUT_DIR / f\"{label}__test_metrics.csv\", index=False)\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"model\": model_name,\n",
    "                \"split\": \"test\",\n",
    "                \"youden_index\": youden_te,\n",
    "                **test_thr_metrics,\n",
    "            }\n",
    "        ]\n",
    "    ).to_csv(OUT_DIR / f\"{label}__youden_metrics_Test.csv\", index=False)\n",
    "\n",
    "    # --- 변경: OOF 임계값(thr_oof) 기준을 메인으로 보고 ---\n",
    "    # (1) OOF 임계값으로 Test 지표 계산 → 메인 보고\n",
    "    test_thr_metrics_oof = metrics_at_threshold(y_te.values, prob_te, thr_oof)\n",
    "    test_metrics = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"model\": model_name,\n",
    "                \"split\": \"test\",\n",
    "                \"auc\": auc_te,\n",
    "                \"prauc\": pr_te,\n",
    "                \"brier\": brier_te,\n",
    "                # 분류 지표도 thr_oof 기준으로 재계산해 반영\n",
    "                \"accuracy\": accuracy_score(y_te, (prob_te >= thr_oof).astype(int)),\n",
    "                \"precision\": precision_score(\n",
    "                    y_te, (prob_te >= thr_oof).astype(int), zero_division=0\n",
    "                ),\n",
    "                \"recall\": recall_score(y_te, (prob_te >= thr_oof).astype(int)),\n",
    "                \"f1\": f1_score(y_te, (prob_te >= thr_oof).astype(int)),\n",
    "                \"best_params\": search.best_params_,\n",
    "                \"thr_source\": \"oof\",\n",
    "                \"thr_youden\": thr_oof,\n",
    "                \"sens_at_thr\": test_thr_metrics_oof[\"sensitivity\"],\n",
    "                \"spec_at_thr\": test_thr_metrics_oof[\"specificity\"],\n",
    "                \"ppv_at_thr\": test_thr_metrics_oof[\"ppv\"],\n",
    "                \"npv_at_thr\": test_thr_metrics_oof[\"npv\"],\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    test_metrics.to_csv(OUT_DIR / f\"{label}__test_metrics.csv\", index=False)\n",
    "\n",
    "    # (선택) 참고용: Test에서 다시 구한 thr_te 결과는 별도 파일로만 저장 (부록)\n",
    "    thr_te, tpr_te, fpr_te = youden_threshold(y_te.values, prob_te)\n",
    "    youden_te = tpr_te - fpr_te\n",
    "    test_thr_metrics_te = metrics_at_threshold(y_te.values, prob_te, thr_te)\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"label\": label,\n",
    "                \"model\": model_name,\n",
    "                \"split\": \"test\",\n",
    "                \"thr_source\": \"test\",\n",
    "                \"thr_youden\": thr_te,\n",
    "                \"youden_index\": youden_te,\n",
    "                **test_thr_metrics_te,\n",
    "            }\n",
    "        ]\n",
    "    ).to_csv(OUT_DIR / f\"{label}__youden_metrics_Test_fromTestThr.csv\", index=False)\n",
    "\n",
    "    # --- ⭐️⭐️⭐️(B) Test에서 Youden index 기반 최적 threshold --- ⭐️⭐️⭐️\n",
    "\n",
    "    test_pred = pd.DataFrame(\n",
    "        {\"label\": label, \"y_true\": y_te.values, \"y_prob\": prob_te, \"y_pred\": pred_te}\n",
    "    )\n",
    "    test_pred.to_csv(OUT_DIR / f\"{label}__test_predictions.csv\", index=False)\n",
    "\n",
    "    # Test ROC / Calibration / DCA\n",
    "    fpr, tpr, _ = roc_curve(y_te, prob_te)\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"AUC={auc_te:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\")\n",
    "    # --- ⭐️⭐️⭐️ 추가 ⭐️⭐️⭐️\n",
    "\n",
    "    def _tpr_fpr_at_thr(y_true, y_prob, thr):\n",
    "        yhat = (y_prob >= thr).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, yhat).ravel()\n",
    "        tpr = tp / (tp + fn + 1e-12)\n",
    "        fpr = fp / (fp + tn + 1e-12)\n",
    "        return tpr, fpr\n",
    "\n",
    "    tpr_oof_on_test, fpr_oof_on_test = _tpr_fpr_at_thr(y_te.values, prob_te, thr_oof)\n",
    "    plt.scatter(\n",
    "        [fpr_oof_on_test],\n",
    "        [tpr_oof_on_test],\n",
    "        s=30,\n",
    "        marker=\"o\",\n",
    "        label=f\"Youden thr={thr_oof:.3f}\",\n",
    "    )\n",
    "    # --- ⭐️⭐️⭐️ 추가 ⭐️⭐️⭐️\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(f\"ROC (Test) - {label} ({model_name})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__ROC_Test.png\")\n",
    "    plt.close()\n",
    "\n",
    "    pt, pp = calibration_curve(y_te, prob_te, n_bins=10)\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.plot(pp, pt, marker=\"o\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Observed\")\n",
    "    plt.title(f\"Calibration (Test) - {label} ({model_name})\\nBrier={brier_te:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__Calibration_Test.png\")\n",
    "    plt.close()\n",
    "\n",
    "    dca_te = decision_curve(y_te.values, prob_te)\n",
    "    plt.figure(figsize=(4.6, 4.2))\n",
    "    plt.plot(dca_te[\"threshold\"], dca_te[\"net_benefit\"], lw=2, label=\"Model\")\n",
    "    prev = y_te.mean()\n",
    "    treat_all = dca_te[\"threshold\"].apply(\n",
    "        lambda th: prev - (1 - prev) * (th / (1 - th))\n",
    "    )\n",
    "    plt.plot(dca_te[\"threshold\"], treat_all, \"--\", label=\"Treat-all\")\n",
    "    plt.plot(\n",
    "        dca_te[\"threshold\"],\n",
    "        np.zeros_like(dca_te[\"threshold\"]),\n",
    "        \"--\",\n",
    "        label=\"Treat-none\",\n",
    "    )\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Net benefit\")\n",
    "    plt.title(f\"DCA (Test) - {label} ({model_name})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__DCA_Test.png\")\n",
    "    plt.close()\n",
    "    dca_te.to_csv(OUT_DIR / f\"{label}__DCA_Test.csv\", index=False)\n",
    "\n",
    "    # SHAP (full Train 재학습 모델 기준, Test 특성 사용)\n",
    "    print(\"  [SHAP] compute on Test ...\")\n",
    "\n",
    "    preproc = best_full.named_steps[\"preprocessor\"]\n",
    "    clf_final = best_full.named_steps[\"clf\"]\n",
    "\n",
    "    # 1) 전처리 후 데이터 준비\n",
    "    rng = np.random.RandomState(42)\n",
    "    bg_idx = rng.choice(len(X_tr), size=min(200, len(X_tr)), replace=False)\n",
    "    X_bg_t = preproc.transform(X_tr.iloc[bg_idx])\n",
    "    X_te_t = preproc.transform(X_te)\n",
    "\n",
    "    # ⭐️⛔️⭐️⛔️⭐️⛔️ 2) 전처리 후 피처 이름 + 접두사 정리 ⭐️⛔️⭐️⛔️⭐️⛔️⭐️⛔️\n",
    "    # try:\n",
    "    #     raw_names = list(preproc.get_feature_names_out())\n",
    "    # except Exception:\n",
    "    #     raw_names = [f\"f{i}\" for i in range(X_te_t.shape[1])]\n",
    "    # 2) 전처리 후 피처 이름\n",
    "    try:\n",
    "        raw_names = list(preproc.get_feature_names_out())\n",
    "    except Exception:\n",
    "        raw_names = [f\"f{i}\" for i in range(X_te_t.shape[1])]\n",
    "    # ⭐️⛔️⭐️⛔️⭐️⛔️ ⭐️⛔️⭐️⛔️⭐️⛔️⭐️⛔️\n",
    "\n",
    "    def clean_name(name: str) -> str:\n",
    "        # ① ColumnTransformer 접두사 제거: \"num__\", \"cat__\", \"code__\"\n",
    "        name = re.sub(r\"^[^_]+__\", \"\", name)\n",
    "        # ② 남아있을 수 있는 \"num_\", \"cat_\", \"code_\"도 제거\n",
    "        name = re.sub(r\"^(num_|cat_|code_)\", \"\", name)\n",
    "        return name\n",
    "\n",
    "    # ⭐️⛔️⭐️⛔️⭐️⛔️ ⭐️⛔️⭐️⛔️⭐️⛔️⭐️⛔️\n",
    "    # feat_names = [clean_name(n) for n in raw_names]\n",
    "    feat_names = prettify_names(raw_names, NAME_MAP)\n",
    "    # ⭐️⛔️⭐️⛔️⭐️⛔️ ⭐️⛔️⭐️⛔️⭐️⛔️⭐️⛔️\n",
    "\n",
    "    # 3) 모델 타입별 Explainer (배경 데이터 제공 → interventional 경고 해소)\n",
    "    try:\n",
    "        if isinstance(\n",
    "            clf_final,\n",
    "            (XGBClassifier, LGBMClassifier, CatBoostClassifier, RandomForestClassifier),\n",
    "        ):\n",
    "            explainer = shap.TreeExplainer(\n",
    "                clf_final,\n",
    "                data=X_bg_t,\n",
    "                feature_perturbation=\"interventional\",\n",
    "                model_output=\"probability\",\n",
    "            )\n",
    "            shap_expl = explainer(X_te_t)  # Explanation\n",
    "        elif isinstance(clf_final, LogisticRegression):\n",
    "            explainer = shap.LinearExplainer(clf_final, X_bg_t)\n",
    "            shap_expl = explainer(X_te_t)  # ndarray 또는 Explanation\n",
    "            if not isinstance(shap_expl, shap.Explanation):\n",
    "                shap_expl = shap.Explanation(\n",
    "                    values=shap_expl,\n",
    "                    base_values=np.zeros(X_te_t.shape[0]),\n",
    "                    data=X_te_t,\n",
    "                    feature_names=feat_names,\n",
    "                )\n",
    "        else:\n",
    "            explainer = shap.Explainer(clf_final, X_bg_t)\n",
    "            shap_expl = explainer(X_te_t)\n",
    "    except Exception:\n",
    "        explainer = shap.Explainer(clf_final, X_bg_t)\n",
    "        shap_expl = explainer(X_te_t)\n",
    "\n",
    "    # 4) (중요) 다중 출력이면 양성 클래스(= index 1)만 선택 → beeswarm 오류 해결\n",
    "    try:\n",
    "        vals = shap_expl.values\n",
    "        if getattr(vals, \"ndim\", 2) == 3:\n",
    "            # (n, p, outputs) → outputs=1 선택\n",
    "            shap_expl = shap_expl[:, :, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 5) 정리된 피처명 주입\n",
    "    try:\n",
    "        shap_expl.feature_names = feat_names\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 6) bar에서 “sum of other features” 제거: 상위 K개만 남긴 Explanation 생성\n",
    "    K = 20\n",
    "    # mean(|SHAP|) 기준 상위 K 인덱스\n",
    "    mean_abs = np.abs(np.array(shap_expl.values)).mean(axis=0)\n",
    "    topk_idx = np.argsort(mean_abs)[::-1][:K]\n",
    "    shap_top = shap_expl[:, topk_idx]  # Explanation 슬라이싱 지원\n",
    "\n",
    "    # 7) 시각화 (공식 API 사용)\n",
    "    plt.figure()\n",
    "    shap.plots.beeswarm(\n",
    "        shap_expl,\n",
    "        max_display=K,\n",
    "        show=False,\n",
    "        group_remaining_features=False,\n",
    "    )\n",
    "    plt.title(f\"SHAP Summary (Test) - {label} ({model_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__SHAP_summary_Test.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    shap.plots.bar(\n",
    "        shap_top, max_display=K, show=False\n",
    "    )  # group_remaining_features=False\n",
    "    plt.title(f\"SHAP Top Features (Test) - {label} ({model_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{label}__SHAP_bar_Test.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # =============================\n",
    "    # 🔽 [NEW] SHAP 값을 CSV로 저장\n",
    "    # =============================\n",
    "    try:\n",
    "        # ---- 기본 배열/이름 준비\n",
    "        shap_vals = np.asarray(shap_expl.values)  # (n_test, n_features)\n",
    "        n_te, p = shap_vals.shape\n",
    "        feat_names_arr = np.array(feat_names[:p])  # 안전 슬라이스\n",
    "        row_ids = getattr(X_te, \"index\", np.arange(n_te))\n",
    "\n",
    "        # ---- (1) 변수별 요약: 중요도 및 방향성\n",
    "        mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "        mean_val = shap_vals.mean(axis=0)\n",
    "        std_val = shap_vals.std(axis=0)\n",
    "        med_abs = np.median(np.abs(shap_vals), axis=0)\n",
    "        q1_abs = np.quantile(np.abs(shap_vals), 0.25, axis=0)\n",
    "        q3_abs = np.quantile(np.abs(shap_vals), 0.75, axis=0)\n",
    "        nonzero_ratio = np.count_nonzero(shap_vals, axis=0) / float(n_te)\n",
    "\n",
    "        fi_df = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"label\": label,\n",
    "                    \"model\": model_name,\n",
    "                    \"feature\": feat_names_arr,\n",
    "                    \"mean_abs_shap\": mean_abs,\n",
    "                    \"mean_shap\": mean_val,\n",
    "                    \"std_shap\": std_val,\n",
    "                    \"median_abs_shap\": med_abs,\n",
    "                    \"q1_abs_shap\": q1_abs,\n",
    "                    \"q3_abs_shap\": q3_abs,\n",
    "                    \"nonzero_ratio\": nonzero_ratio,\n",
    "                }\n",
    "            )\n",
    "            .sort_values(\"mean_abs_shap\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        fi_df.insert(0, \"rank\", np.arange(1, len(fi_df) + 1))\n",
    "        fi_df.to_csv(\n",
    "            OUT_DIR / f\"{label}__SHAP_feature_importance_Test.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # ---- (2) 상위 K개만 별도 저장 (그림 max_display와 통일)\n",
    "        topk = fi_df.head(K).copy()\n",
    "        topk.to_csv(OUT_DIR / f\"{label}__SHAP_top{K}_Test.csv\", index=False)\n",
    "\n",
    "        # ---- (3) 환자×변수 wide 매트릭스\n",
    "        shap_wide = pd.DataFrame(shap_vals, index=row_ids, columns=feat_names_arr)\n",
    "        shap_wide.index.name = \"row_id\"\n",
    "        shap_wide.to_csv(OUT_DIR / f\"{label}__SHAP_matrix_Test.csv\")\n",
    "\n",
    "        # ✅ (추가) 테스트 데이터(전처리 후 feature 값) wide matrix\n",
    "        X_te_wide = pd.DataFrame(X_te_t, index=row_ids, columns=feat_names_arr)\n",
    "        X_te_wide.index.name = \"row_id\"\n",
    "\n",
    "        # ---- (4) long 포맷 (row_id, feature, shap_value, y_true, y_prob)\n",
    "        test_prob_series = pd.Series(prob_te, index=row_ids, name=\"y_prob\")\n",
    "        test_true_series = pd.Series(y_te.values, index=row_ids, name=\"y_true\")\n",
    "\n",
    "        shap_long = shap_wide.reset_index().melt(\n",
    "            id_vars=[\"row_id\"], var_name=\"feature\", value_name=\"shap_value\"\n",
    "        )\n",
    "\n",
    "        # ✅ (추가) feature_value 컬럼 병합\n",
    "        feat_long = X_te_wide.reset_index().melt(\n",
    "            id_vars=[\"row_id\"], var_name=\"feature\", value_name=\"feature_value\"\n",
    "        )\n",
    "        shap_long = shap_long.merge(feat_long, on=[\"row_id\", \"feature\"], how=\"left\")\n",
    "\n",
    "        shap_long = shap_long.merge(\n",
    "            test_true_series.reset_index(), on=\"row_id\", how=\"left\"\n",
    "        ).merge(test_prob_series.reset_index(), on=\"row_id\", how=\"left\")\n",
    "        shap_long.insert(0, \"label\", label)\n",
    "        shap_long.insert(1, \"model\", model_name)\n",
    "        shap_long.to_csv(OUT_DIR / f\"{label}__SHAP_long_Test.csv\", index=False)\n",
    "\n",
    "        print(\n",
    "            f\"  [SHAP] CSV saved: \"\n",
    "            f\"{label}__SHAP_feature_importance_Test.csv, \"\n",
    "            f\"{label}__SHAP_top{K}_Test.csv, \"\n",
    "            f\"{label}__SHAP_matrix_Test.csv, \"\n",
    "            f\"{label}__SHAP_long_Test.csv\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to write SHAP CSVs: {e}\")\n",
    "\n",
    "    # 통합 요약 반환\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"model\": model_name,\n",
    "        \"train_oof_summary_path\": str(\n",
    "            (OUT_DIR / f\"{label}__train_fold_metrics.csv\").resolve()\n",
    "        ),\n",
    "        \"test_metrics_path\": str((OUT_DIR / f\"{label}__test_metrics.csv\").resolve()),\n",
    "        \"oof_pred_path\": str((OUT_DIR / f\"{label}__oof_predictions.csv\").resolve()),\n",
    "        \"test_pred_path\": str((OUT_DIR / f\"{label}__test_predictions.csv\").resolve()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e647c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    manifest = []\n",
    "    for label, cfg in BEST_COMBOS.items():\n",
    "        res = run_for_label(\n",
    "            label=label,\n",
    "            model_name=cfg[\"model\"],\n",
    "            feature_set=cfg[\"feature_set\"],\n",
    "            random_state=42,\n",
    "        )\n",
    "        manifest.append(res)\n",
    "    pd.DataFrame(manifest).to_csv(OUT_DIR / \"final_runs_manifest.csv\", index=False)\n",
    "    print(\"\\nAll done. Results saved to:\", OUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b7659",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}