{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Stage 1 Modeling — Nested CV & Ablation Study\n",
    "\n",
    "Nested CV (5-fold outer, 3-fold inner) with BayesSearchCV.\n",
    "Ablation: Base / Base+LLM / Base+Lab / All_Features.\n",
    "DeLong test for AUC comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from src.config import PROJECT_ROOT, MODEL_SEED, LABELS, MODELS_TO_RUN\n",
    "from src.variables import LLM_COLS, LAB_COLS, CODE_COLS, CATEGORY_COLS\n",
    "from src.preprocessing import create_preprocessor\n",
    "from src.models import get_models_and_search_space\n",
    "from src.evaluation import delong_test, auc_diff_with_ci\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Path configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ⚠️ Adjust these paths per experiment ⚠️\n",
    "DATA_DIR = PROJECT_ROOT / \"data/processed_imp/260114_split_corr_LLM_ADER/imputation/simple_imput\"\n",
    "FS_DIR = PROJECT_ROOT / \"results/new_analysis/260114_qwen/Feature_Selection/simple_20/step2_FS\"\n",
    "OUT_DIR = PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step1_modeling/simple_20\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nested CV & Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_cv_results = []\n",
    "predictions_for_delong = {}\n",
    "\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=MODEL_SEED)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=MODEL_SEED)\n",
    "\n",
    "for label in LABELS:\n",
    "    print(f\"\\n{'='*20} {label} {'='*20}\")\n",
    "    train_file = DATA_DIR / f\"simple_{label}_train.csv\"\n",
    "    if not train_file.exists():\n",
    "        print(f\"File not found, skipping: {train_file.name}\")\n",
    "        continue\n",
    "\n",
    "    df_train = pd.read_csv(train_file)\n",
    "    final_features = pd.read_csv(FS_DIR / f\"final_features_{label}.csv\")[\"feature\"].tolist()\n",
    "\n",
    "    selected_lab = [f for f in final_features if f in LAB_COLS]\n",
    "    selected_base = [f for f in final_features if f not in LAB_COLS and f not in LLM_COLS]\n",
    "\n",
    "    variable_sets = {\n",
    "        \"Base\": selected_base,\n",
    "        \"Base_LLM\": selected_base + LLM_COLS,\n",
    "        \"Base_Lab\": selected_base + selected_lab,\n",
    "        \"All_Features\": selected_base + selected_lab + LLM_COLS,\n",
    "    }\n",
    "\n",
    "    predictions_for_delong[label] = {}\n",
    "\n",
    "    for set_name, features in variable_sets.items():\n",
    "        print(f\"\\n--- {set_name} ({len(features)} features) ---\")\n",
    "        X_train_full = df_train[features]\n",
    "        y_train_full = df_train[label]\n",
    "        models = get_models_and_search_space(MODELS_TO_RUN)\n",
    "        predictions_for_delong[label][set_name] = {}\n",
    "\n",
    "        for model_name, (model, search_space) in models.items():\n",
    "            outer_scores = {\"auc\": [], \"f1\": [], \"precision\": [], \"recall\": [], \"accuracy\": []}\n",
    "            fold_predictions = {\"idx\": [], \"y_true\": [], \"y_pred_proba\": []}\n",
    "\n",
    "            for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X_train_full, y_train_full)):\n",
    "                X_train_outer = X_train_full.iloc[train_idx]\n",
    "                y_train_outer = y_train_full.iloc[train_idx]\n",
    "                X_val_outer = X_train_full.iloc[val_idx]\n",
    "                y_val_outer = y_train_full.iloc[val_idx]\n",
    "\n",
    "                numeric_features = [c for c in X_train_outer.columns if c not in (CODE_COLS + CATEGORY_COLS)]\n",
    "                categorical_features = [c for c in CATEGORY_COLS if c in X_train_outer.columns and c not in CODE_COLS]\n",
    "                code_features = [c for c in CODE_COLS if c in X_train_outer.columns]\n",
    "                preprocessor = create_preprocessor(numeric_features, categorical_features, code_features)\n",
    "\n",
    "                pipeline = ImbPipeline([\n",
    "                    (\"preprocessor\", preprocessor),\n",
    "                    (\"smote\", SMOTE(sampling_strategy=\"minority\", random_state=MODEL_SEED)),\n",
    "                    (\"clf\", model),\n",
    "                ])\n",
    "\n",
    "                bayes_search = BayesSearchCV(\n",
    "                    estimator=pipeline, search_spaces=search_space,\n",
    "                    n_iter=10, cv=inner_cv, scoring=\"roc_auc\",\n",
    "                    n_jobs=-1, random_state=MODEL_SEED, refit=True,\n",
    "                )\n",
    "                bayes_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "                y_pred_proba = bayes_search.predict_proba(X_val_outer)[:, 1]\n",
    "                y_pred = bayes_search.predict(X_val_outer)\n",
    "\n",
    "                outer_scores[\"auc\"].append(roc_auc_score(y_val_outer, y_pred_proba))\n",
    "                outer_scores[\"f1\"].append(f1_score(y_val_outer, y_pred))\n",
    "                outer_scores[\"precision\"].append(precision_score(y_val_outer, y_pred, zero_division=0))\n",
    "                outer_scores[\"recall\"].append(recall_score(y_val_outer, y_pred))\n",
    "                outer_scores[\"accuracy\"].append(accuracy_score(y_val_outer, y_pred))\n",
    "\n",
    "                fold_predictions[\"idx\"].append(X_val_outer.index.values)\n",
    "                fold_predictions[\"y_true\"].append(y_val_outer.values if hasattr(y_val_outer, \"values\") else y_val_outer)\n",
    "                fold_predictions[\"y_pred_proba\"].append(y_pred_proba)\n",
    "\n",
    "            df_pred = pd.DataFrame({\n",
    "                \"idx\": np.concatenate(fold_predictions[\"idx\"]),\n",
    "                \"y_true\": np.concatenate(fold_predictions[\"y_true\"]),\n",
    "                \"y_pred_proba\": np.concatenate(fold_predictions[\"y_pred_proba\"]),\n",
    "            }).sort_values(\"idx\").reset_index(drop=True)\n",
    "            predictions_for_delong[label][set_name][model_name] = df_pred\n",
    "\n",
    "            result = {\n",
    "                \"label\": label, \"variable_set\": set_name, \"model\": model_name,\n",
    "                \"mean_auc\": np.mean(outer_scores[\"auc\"]), \"std_auc\": np.std(outer_scores[\"auc\"]),\n",
    "                \"mean_f1\": np.mean(outer_scores[\"f1\"]), \"std_f1\": np.std(outer_scores[\"f1\"]),\n",
    "                \"mean_precision\": np.mean(outer_scores[\"precision\"]), \"std_precision\": np.std(outer_scores[\"precision\"]),\n",
    "                \"mean_recall\": np.mean(outer_scores[\"recall\"]), \"std_recall\": np.std(outer_scores[\"recall\"]),\n",
    "                \"mean_accuracy\": np.mean(outer_scores[\"accuracy\"]), \"std_accuracy\": np.std(outer_scores[\"accuracy\"]),\n",
    "                \"cv_auc_scores\": outer_scores[\"auc\"],\n",
    "            }\n",
    "            all_cv_results.append(result)\n",
    "            print(f\"  {model_name}: AUC={result['mean_auc']:.4f}±{result['std_auc']:.4f}, F1={result['mean_f1']:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_cv_results)\n",
    "results_df.to_csv(OUT_DIR / \"modeling_ablation_results_full.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nNested CV & Ablation complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DeLong's test — AUC improvement with 95% CI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "improvement_rows = []\n",
    "\n",
    "for label in LABELS:\n",
    "    for model_name in MODELS_TO_RUN:\n",
    "        try:\n",
    "            base = predictions_for_delong[label][\"Base\"][model_name]\n",
    "            y_true = base[\"y_true\"].values\n",
    "\n",
    "            for comp_name in [\"Base_LLM\", \"Base_Lab\", \"All_Features\"]:\n",
    "                if comp_name not in predictions_for_delong[label]:\n",
    "                    continue\n",
    "                if model_name not in predictions_for_delong[label][comp_name]:\n",
    "                    continue\n",
    "                comp = predictions_for_delong[label][comp_name][model_name]\n",
    "                stat = auc_diff_with_ci(y_true, comp[\"y_pred_proba\"].values, base[\"y_pred_proba\"].values)\n",
    "                improvement_rows.append({\n",
    "                    \"label\": label, \"model\": model_name,\n",
    "                    \"comparison\": f\"{comp_name} vs Base\",\n",
    "                    \"auc_new\": stat[\"auc_new\"], \"auc_base\": stat[\"auc_old\"],\n",
    "                    \"delta_auc\": stat[\"delta_auc\"],\n",
    "                    \"delta_auc_%\": 100.0 * stat[\"delta_auc\"] / stat[\"auc_old\"] if stat[\"auc_old\"] > 0 else np.nan,\n",
    "                    \"ci_low\": stat[\"ci_low\"], \"ci_high\": stat[\"ci_high\"],\n",
    "                    \"z\": stat[\"z\"], \"p\": stat[\"p\"],\n",
    "                    \"significant (p<0.05)\": \"Yes\" if stat[\"p\"] < 0.05 else \"No\",\n",
    "                })\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Skipped: {label}, {model_name} -> {e}\")\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_rows)\n",
    "improvement_df[\"comparison\"] = pd.Categorical(\n",
    "    improvement_df[\"comparison\"],\n",
    "    categories=[\"Base_LLM vs Base\", \"Base_Lab vs Base\", \"All_Features vs Base\"],\n",
    "    ordered=True,\n",
    ")\n",
    "improvement_df = improvement_df.sort_values([\"label\", \"model\", \"comparison\"]).reset_index(drop=True)\n",
    "improvement_df.to_csv(OUT_DIR / \"auc_improvement_with_ci.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"DeLong AUC improvement results saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for label in LABELS:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    subset = results_df[results_df[\"label\"] == label]\n",
    "    sns.barplot(data=subset, x=\"model\", y=\"mean_auc\", hue=\"variable_set\")\n",
    "    plt.title(f\"Model Performance - {label}\")\n",
    "    plt.ylabel(\"Mean ROC AUC (5-fold)\")\n",
    "    plt.ylim(bottom=max(0.5, subset[\"mean_auc\"].min() - 0.05))\n",
    "    plt.legend(title=\"Variable Set\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / f\"performance_comparison_{label}.png\")\n",
    "    plt.close()\n",
    "print(\"Figures saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}