{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.config import PROJECT_ROOT, MODEL_SEED, LABELS\n",
    "from src.variables import LLM_COLS, LAB_COLS, CODE_COLS, CATEGORY_COLS\n",
    "\n",
    "import os\n",
    "import re\n",
    "import joblib, json\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    roc_curve,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# Tree models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import from_contents, plot\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update(\n",
    "    {\"figure.dpi\": 130, \"axes.spines.top\": False, \"axes.spines.right\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 0) 경로/조합 설정\n",
    "# ---------------------------\n",
    "OUT_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMP_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"data/processed_imp/260114_split_corr_LLM_ADER/imputation/simple_imput\")\n",
    ")\n",
    "FS_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/Feature_Selection/simple_20/step2_FS\")\n",
    ")\n",
    "\n",
    "# Train/Test 파일 이름\n",
    "DATA_FILES = {\n",
    "    \"label_30d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_30d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_30d_test.csv\",\n",
    "    },\n",
    "    \"label_60d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_60d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_60d_test.csv\",\n",
    "    },\n",
    "    \"label_90d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_90d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_90d_test.csv\",\n",
    "    },\n",
    "    \"label_180d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_180d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_180d_test.csv\",\n",
    "    },\n",
    "    \"label_365d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_365d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_365d_test.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 당신의 탐색 결과를 반영한 권장조합 (필요시 수정)\n",
    "BEST_COMBOS = {\n",
    "    \"label_30d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_60d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_90d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_180d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_365d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "}\n",
    "\n",
    "TARGET_COL_FALLBACK = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_RUN = [\"LR\", \"RF\", \"XGB\", \"LGBM\", \"CATBOOST\"]\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "# 변수 그룹 정의\n",
    "LLM_COLS = [\n",
    "    \"Impaired_Social_Function\",  # 사회기능 저하\n",
    "    \"Religious_Affiliation\",  # 종교적 소속\n",
    "    \"Violence_and_Impulsivity\",  # 폭력성 및 충동성\n",
    "    \"Domestic_Violence\",  # 가정폭력\n",
    "    \"Physical_Abuse\",  # 신체적 학대\n",
    "    \"Divorce\",  # 이혼 경험\n",
    "    \"Death_of_Family_Member\",  # 가족의 죽음\n",
    "    \"Emotional_Abuse\",  # 정서적 학대\n",
    "    \"Lack_of_Family_Support\",  # 가족 지지 부족\n",
    "    \"Social_Isolation_and_Lack_of_Support\",  # 사회적 지지 부족 및 사회적 고립\n",
    "    \"Psychotic_Symptoms\",  # 환청/망상/피해사고\n",
    "    \"Interpersonal_Conflict\",  # 대인관계 갈등\n",
    "    \"Exposure_to_Suicide\",  # 타인의 자살 목격 및 노출\n",
    "    \"Alcohol_Use_Problems\",  # 알코올 사용 문제\n",
    "    \"Sexual_Abuse\",  # 성적피해\n",
    "    \"Physical_and_Emotional_Neglect\",  # 신체적 및 정서적 방임\n",
    "]\n",
    "\n",
    "# Lab 변수 목록\n",
    "LAB_COLS = [\n",
    "    \"BL3125\",\n",
    "    \"BL3137\",\n",
    "    \"BL3140\",\n",
    "    \"BL3141\",\n",
    "    \"BL3142\",\n",
    "    \"BL314201\",\n",
    "    \"BL3603\",\n",
    "    \"NR4303\",\n",
    "    \"BL2011\",\n",
    "    \"BL2012\",\n",
    "    \"BL2013\",\n",
    "    \"BL2014\",\n",
    "    \"BL201401\",\n",
    "    \"BL201402\",\n",
    "    \"BL201403\",\n",
    "    \"BL2016\",\n",
    "    \"BL201801\",\n",
    "    \"BL201802\",\n",
    "    \"BL201803\",\n",
    "    \"BL201804\",\n",
    "    \"BL201805\",\n",
    "    \"BL201806\",\n",
    "    \"BL201807\",\n",
    "    \"BL201808\",\n",
    "    \"BL201809\",\n",
    "    \"BL201810\",\n",
    "    \"BL201811\",\n",
    "    \"BL201812\",\n",
    "    \"BL201813\",\n",
    "    \"BL201814\",\n",
    "    \"BL201815\",\n",
    "    \"BL201816\",\n",
    "    \"BL201818\",\n",
    "    \"BL3111\",\n",
    "    \"BL3112\",\n",
    "    \"BL3131\",\n",
    "    \"BL3132\",\n",
    "    \"BL3133\",\n",
    "    \"BL311201\",\n",
    "    \"BL311202\",\n",
    "    \"BL3113\",\n",
    "    \"BL3114\",\n",
    "    \"BL3115\",\n",
    "    \"BL3116\",\n",
    "    \"BL3117\",\n",
    "    \"BL3118\",\n",
    "    \"BL3119\",\n",
    "    \"BL3120\",\n",
    "    \"BL312001\",\n",
    "    \"BL312002\",\n",
    "    \"BL3121\",\n",
    "    \"BL3122\",\n",
    "    \"BL3123\",\n",
    "]\n",
    "\n",
    "CODE_COLS = [\"sleep\", \"appetite\", \"weight\"]\n",
    "\n",
    "CATEGORY_COLS = [\n",
    "    \"sex\",\n",
    "    \"edu\",\n",
    "    \"job\",\n",
    "    \"marry\",\n",
    "    \"drink\",\n",
    "    \"smoke\",\n",
    "    \"substance_abuse\",\n",
    "    \"psy_family\",\n",
    "    \"AD_more_three\",\n",
    "    \"ER_more_two\",\n",
    "    \"Suicidalidea\",\n",
    "    \"Suicidalplan\",\n",
    "    \"Suicidalattempt\",\n",
    "    \"benzodiazepine\",\n",
    "    \"quetiapine\",\n",
    "    \"aripiprazole\",\n",
    "    \"lithium\",\n",
    "    \"divalproex\",\n",
    "    \"olanzapine\",\n",
    "    \"bipolar\",\n",
    "    \"depression\",\n",
    "    \"schizophrenia\",\n",
    "    \"anxiety\",\n",
    "    \"trauma_stressor_related\",\n",
    "    \"somatic_symptom_disorder\",\n",
    "    \"psychotic_other\",\n",
    "    # LLM 변수도 범주형으로 취급\n",
    "    \"Impaired_Social_Function\",  # 사회기능 저하\n",
    "    \"Religious_Affiliation\",  # 종교적 소속\n",
    "    \"Violence_and_Impulsivity\",  # 폭력성 및 충동성\n",
    "    \"Domestic_Violence\",  # 가정폭력\n",
    "    \"Physical_Abuse\",  # 신체적 학대\n",
    "    \"Divorce\",  # 이혼 경험\n",
    "    \"Death_of_Family_Member\",  # 가족의 죽음\n",
    "    \"Emotional_Abuse\",  # 정서적 학대\n",
    "    \"Lack_of_Family_Support\",  # 가족 지지 부족\n",
    "    \"Social_Isolation_and_Lack_of_Support\",  # 사회적 지지 부족 및 사회적 고립\n",
    "    \"Psychotic_Symptoms\",  # 환청/망상/피해사고\n",
    "    \"Interpersonal_Conflict\",  # 대인관계 갈등\n",
    "    \"Exposure_to_Suicide\",  # 타인의 자살 목격 및 노출\n",
    "    \"Alcohol_Use_Problems\",  # 알코올 사용 문제\n",
    "    \"Sexual_Abuse\",  # 성적피해\n",
    "    \"Physical_and_Emotional_Neglect\",  # 신체적 및 정서적 방임\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c000b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) 유틸: FS/데이터 로드\n",
    "# ---------------------------\n",
    "def find_feature_list_file(label: str) -> Path:\n",
    "    # step2_FS 폴더 내 label 키워드 포함 & 35개 선정 파일을 우선 탐색\n",
    "    # (환경에 맞게 패턴 필요시 조정)\n",
    "    pats = [f\"final_features_{label}.csv\"]\n",
    "    cands = []\n",
    "    for p in pats:\n",
    "        cands += list(FS_DIR.glob(p))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"[FS] No feature list file for {label} in {FS_DIR}\")\n",
    "    return sorted(cands)[-1]\n",
    "\n",
    "\n",
    "def read_feature_list(fpath: Path) -> List[str]:\n",
    "    df = pd.read_csv(fpath)\n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    if \"feature\" in cols_lower:\n",
    "        col = df.columns[cols_lower.index(\"feature\")]\n",
    "    elif \"variable\" in cols_lower:\n",
    "        col = df.columns[cols_lower.index(\"variable\")]\n",
    "    else:\n",
    "        col = df.columns[0]\n",
    "    feats = df[col].dropna().astype(str).tolist()\n",
    "    # 중복 제거 & 공백 정리\n",
    "    feats = list(dict.fromkeys([f.strip() for f in feats]))\n",
    "    return feats\n",
    "\n",
    "\n",
    "def load_xy(\n",
    "    path: Path, label: str, features: List[str]\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    df = pd.read_csv(path)\n",
    "    y_col = (\n",
    "        label\n",
    "        if label in df.columns\n",
    "        else (TARGET_COL_FALLBACK if TARGET_COL_FALLBACK in df.columns else None)\n",
    "    )\n",
    "    if y_col is None:\n",
    "        raise KeyError(\n",
    "            f\"Target column not found in {path.name}. Expected '{label}' or '{TARGET_COL_FALLBACK}'.\"\n",
    "        )\n",
    "    use_cols = [c for c in features if c in df.columns]\n",
    "    missing = sorted(set(features) - set(use_cols))\n",
    "    if missing:\n",
    "        print(\n",
    "            f\"[{label}] Missing {len(missing)} FS features in {path.name}: {missing[:5]}{'...' if len(missing)>5 else ''}\"\n",
    "        )\n",
    "    X = df[use_cols].copy()\n",
    "    y = df[y_col].astype(int).copy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def youden_threshold(y_true: np.ndarray, y_prob: np.ndarray):\n",
    "    \"\"\"\n",
    "    ROC기반 Youden index 최대가 되는 threshold 반환.\n",
    "    반환: thr*, tpr_at_thr, fpr_at_thr\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    youden = tpr - fpr\n",
    "    idx = int(np.nanargmax(youden))\n",
    "    return float(thresholds[idx]), float(tpr[idx]), float(fpr[idx])\n",
    "\n",
    "\n",
    "def metrics_at_threshold(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> dict:\n",
    "    \"\"\"\n",
    "    주어진 threshold에서의 성능 지표 계산: Sens, Spec, PPV, NPV, Acc, F1, TP/TN/FP/FN\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Recall\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "    # F1 (positive class)\n",
    "    f1 = (2 * ppv * sens) / (ppv + sens) if (ppv + sens) > 0 else 0.0\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": int(tp),\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "    }\n",
    "\n",
    "\n",
    "def to_bayes_space(grid_dict):\n",
    "    \"\"\"\n",
    "    Grid(dict of lists)를 BayesSearchCV의 search_spaces(dict of skopt spaces)로 변환.\n",
    "    리스트는 Categorical로 감쌉니다.\n",
    "    \"\"\"\n",
    "    space = {}\n",
    "    for k, v in grid_dict.items():\n",
    "        # v가 (low, high, type) 같은 튜플이 아니라면 전부 Categorical 처리\n",
    "        # (원하시면 Real/Integer 범위로 바꾸세요)\n",
    "        space[k] = Categorical(v)  # v가 list라고 가정\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedaeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) 전역 상수로 이동 (또는 파일 상단에 정의)\n",
    "FRIENDLY_OVERRIDES = {\n",
    "    \"age\": \"Age\",\n",
    "    \"sex\": \"Sex (M=1)\",\n",
    "    \"edu\": \"Education Level\",\n",
    "    \"job\": \"Employment\",\n",
    "    \"marry\": \"Marital Status\",\n",
    "    \"smoke\": \"Smoking\",\n",
    "    \"drink\": \"Drinking\",\n",
    "    \"benzodiazepine\": \"Benzodiazepine\",\n",
    "    \"quetiapine\": \"Quetiapine\",\n",
    "    \"lithium\": \"Lithium\",\n",
    "    \"divalproex\": \"Divalproex\",\n",
    "    \"substance_abuse\": \"Substance Abuse\",\n",
    "    \"Suicidalattempt\": \"Suicidal Attempt\",\n",
    "    \"Suicidalplan\": \"Suicidal Plan\",\n",
    "    \"trauma_stressor_related\": \"Trauma Stress\",\n",
    "    \"WorkingMemoryIndex-Compositescore\": \"Working Memory\",\n",
    "    \"PerceptualReasoningIndex-Compositescore\": \"Perceptual Reasoning\",\n",
    "    \"ProcessingSpeedIndex-Compositescore\": \"Processing Speed\",\n",
    "    \"psy_family\": \"Psychiatric family history\",\n",
    "    \"stay_day\": \"Hospitalization period\",\n",
    "    \"AD_more_three\": \"≥3 Admissions\",\n",
    "    \"ER_more_two\": \"≥2 ER Visits\",\n",
    "    \"psychotic_other\": \"Other Psychotic\",\n",
    "    \"somatic_symptom_disorder\": \"Somatic Symptom Disorder\",\n",
    "    \"anxiety\": \"Anxiety\",\n",
    "    # LLM\n",
    "    \"Impaired_Social_Function\": \"Social Function Impairment (LLM)\",\n",
    "    \"Religious_Affiliation\": \"Religious Affiliation (LLM)\",\n",
    "    \"Violence_and_Impulsivity\": \"Aggression/Impulsivity (LLM)\",\n",
    "    \"Domestic_Violence\": \"Domestic Violence (LLM)\",\n",
    "    \"Physical_Abuse\": \"Physical Abuse (LLM)\",\n",
    "    \"Divorce\": \"Divorce Experience (LLM)\",\n",
    "    \"Death_of_Family_Member\": \"Family Loss (LLM)\",\n",
    "    \"Emotional_Abuse\": \"Emotional Abuse (LLM)\",\n",
    "    \"Lack_of_Family_Support\": \"Lack of Family Support (LLM)\",\n",
    "    \"Social_Isolation_and_Lack_of_Support\": \"Social Isolation (LLM)\",\n",
    "    \"Psychotic_Symptoms\": \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "    \"Interpersonal_Conflict\": \"Interpersonal Conflict (LLM)\",\n",
    "    \"Exposure_to_Suicide\": \"Suicide Exposure (LLM)\",\n",
    "    \"Alcohol_Use_Problems\": \"Alcohol Use Issues (LLM)\",\n",
    "    \"Sexual_Abuse\": \"Sexual Victimization (LLM)\",\n",
    "    \"Physical_and_Emotional_Neglect\": \"Neglect (LLM)\",\n",
    "    # CODE_COLS\n",
    "    \"sleep\": \"Sleep\",\n",
    "    \"appetite\": \"Appetite\",\n",
    "    \"weight\": \"Weight Change\",\n",
    "}\n",
    "\n",
    "# 1) LEVEL_MAP 정리: 중복되는 변수명 제거 + 레벨 키 정규화에 대비\n",
    "LEVEL_MAP = {\n",
    "    \"sleep\": {\n",
    "        \"0\": \"Normal sleep\",\n",
    "        \"1\": \"Insomnia\",\n",
    "        \"2\": \"Hypersomnia\",\n",
    "    },\n",
    "    \"appetite\": {\n",
    "        \"0\": \"No change\",\n",
    "        \"1\": \"Decreased appetite\",\n",
    "        \"2\": \"Increased appetite\",\n",
    "    },\n",
    "    \"weight\": {\n",
    "        \"0\": \"No change\",\n",
    "        \"1\": \"Weight loss\",\n",
    "        \"2\": \"Weight gain\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def _norm_level(level: str) -> str:\n",
    "    try:\n",
    "        return str(int(float(level)))\n",
    "    except Exception:\n",
    "        return str(level).strip()\n",
    "\n",
    "\n",
    "def _safe_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"Columns not found: {candidates}\")\n",
    "\n",
    "\n",
    "def load_feature_name_map(csv_path: str | Path) -> dict[str, str]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    code_col = _safe_col(df, [\"Variabel Mapping\", \"Variable\"])\n",
    "    name_col = _safe_col(df, [\"Unnamed: 9\", \"Term\"])\n",
    "    sub = df[[code_col, name_col]].dropna()\n",
    "    sub = sub.rename(columns={code_col: \"code\", name_col: \"name\"})\n",
    "    sub[\"code\"] = sub[\"code\"].astype(str).str.strip()\n",
    "    sub[\"name\"] = sub[\"name\"].astype(str).str.strip()\n",
    "    sub = sub[sub[\"code\"].str.match(r\"^BL\\d+\")]\n",
    "    sub[\"pretty\"] = sub[\"name\"] + \" (\" + sub[\"code\"] + \")\"\n",
    "    mapping = dict(zip(sub[\"code\"], sub[\"pretty\"]))\n",
    "    # 전역 오버라이드도 함께 병합해 한 딕셔너리로 사용\n",
    "    mapping.update(FRIENDLY_OVERRIDES)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def clean_ct_feature_name(raw_name: str) -> str:\n",
    "    name = re.sub(r\"^[^_]+__\", \"\", raw_name)  # 'num__', 'cat__', 'code__'\n",
    "    name = re.sub(r\"^(num_|cat_|code_)\", \"\", name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def prettify_name(raw_name: str, mapping: dict[str, str] | None = None) -> str:\n",
    "    \"\"\"\n",
    "    'sleep_1.0' → 'Sleep: Insomnia'\n",
    "    'appetite_2' → 'Appetite: Increased appetite'\n",
    "    'weight_0' → 'Weight Change: No change'\n",
    "    BL코드/일반 변수는 mapping(+FRIENDLY_OVERRIDES)에 따라 변환.\n",
    "    \"\"\"\n",
    "    base = clean_ct_feature_name(raw_name)\n",
    "    mapping = mapping or {}\n",
    "\n",
    "    m = re.match(r\"^(.*?)[_](.+)$\", base)\n",
    "    if m:\n",
    "        var, level = m.group(1), _norm_level(m.group(2))\n",
    "        var_friendly = mapping.get(var, FRIENDLY_OVERRIDES.get(var, var))\n",
    "        level_friendly = LEVEL_MAP.get(var, {}).get(level)\n",
    "        if level_friendly is not None:\n",
    "            return f\"{var_friendly}: {level_friendly}\"\n",
    "        # 원-핫이지만 레벨 매핑이 없으면 원문 유지\n",
    "        return f\"{var_friendly}: {level}\"\n",
    "\n",
    "    # 원-핫이 아닌 일반 변수(BL코드 포함)\n",
    "    return mapping.get(base, FRIENDLY_OVERRIDES.get(base, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_psych_scale_aliases(mapping: dict[str, str]) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    L, Pd, Vr, mf 등 심리검사 약어 → 사람이 읽기 좋은 라벨로 보강.\n",
    "    - 대소문자 무시\n",
    "    - 접미어(_t, _raw 등)가 있어도 동작\n",
    "    \"\"\"\n",
    "    # 기본 약어 사전 (필요시 여기 계속 추가)\n",
    "    base_alias = {\n",
    "        # MMPI Validity & Clinical\n",
    "        \"L\": \"MMPI Validity: Lie (L)\",\n",
    "        \"Pd\": \"MMPI Clinical: Psychopathic Deviate (Pd)\",\n",
    "        \"Mf\": \"MMPI Clinical: Masculinity–Femininity (Mf)\",\n",
    "        \"mf\": \"MMPI Clinical: Masculinity–Femininity (Mf)\",\n",
    "        \"TR\": \"MMPI Validity: TRIN\",  # (True response inconsistency)\n",
    "        \"Vr\": \"MMPI Validity: VRIN\",  # (Variable Response Inconsistency)\n",
    "        \"F\": \"MMPI Validity: Infrequency (F)\",\n",
    "        \"K\": \"MMPI Validity: Defensiveness (K)\",\n",
    "        \"Hs\": \"MMPI Clinical: Hypochondriasis (Hs)\",\n",
    "        \"D\": \"MMPI Clinical: Depression (D)\",\n",
    "        \"Hy\": \"MMPI Clinical: Hysteria (Hy)\",\n",
    "        \"Pa\": \"MMPI Clinical: Paranoia (Pa)\",\n",
    "        \"Sc\": \"MMPI Clinical: Schizophrenia (Sc)\",\n",
    "        \"Ma\": \"MMPI Clinical: Hypomania (Ma)\",\n",
    "        \"Si\": \"MMPI Clinical: Social Introversion (Si)\",\n",
    "    }\n",
    "\n",
    "    # 대소문자 무시 매칭을 위해 보조 룰 추가:\n",
    "    # - 'pd', 'PD', 'Pd_t', 'mf_raw'처럼 변형된 컬럼명에도 적용\n",
    "    def add_case_insensitive(alias_key: str, pretty: str):\n",
    "        # 정확히 일치하는 키도 덮어씀\n",
    "        mapping[alias_key] = pretty\n",
    "        # 자주 보이는 변형 패턴 → 정규식 룰\n",
    "        # 1) 소문자/대문자 변형\n",
    "        mapping[alias_key.lower()] = pretty\n",
    "        mapping[alias_key.upper()] = pretty\n",
    "        # 2) 접미어(_t, _raw, _score, _z 등)\n",
    "        #   → 정규식으로 처리할 수 있도록, 나중 단계 prettify에서 사용\n",
    "        #   여기선 원키만 심어두고, 정규식은 prettify 단계에서 적용\n",
    "        #   (아래 prettify_psych_name에서 처리)\n",
    "        return\n",
    "\n",
    "    for k, v in base_alias.items():\n",
    "        add_case_insensitive(k, v)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def prettify_psych_name(clean_name: str, mapping: dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    clean_name(접두사 제거된 원 컬럼명)에 심리검사 스케일 약어 매핑 적용.\n",
    "    - 대소문자 무시\n",
    "    - 접미어(_t/_raw 등)는 유지하되, 앞의 약어만 사람이 읽는 표기로 교체\n",
    "    \"\"\"\n",
    "    # 원본 보존\n",
    "    name = clean_name\n",
    "\n",
    "    # 약어 + 선택적 접미어 패턴: 예) 'Pd', 'pd_t', 'MF_raw', 'Vr_score'\n",
    "    m = re.match(r\"^([A-Za-z]{1,3})(?:[_\\-](.+))?$\", name)\n",
    "    if m:\n",
    "        key = m.group(1)  # Pd / pd / MF / Vr ...\n",
    "        suffix = m.group(2)  # t / raw / z 등 (없을 수도)\n",
    "        # 매핑에 있으면 교체\n",
    "        pretty_core = mapping.get(\n",
    "            key, mapping.get(key.lower(), mapping.get(key.upper(), None))\n",
    "        )\n",
    "        if pretty_core:\n",
    "            if suffix:\n",
    "                # 접미어는 괄호 뒤에 그대로 덧붙여서 정보 보존\n",
    "                return f\"{pretty_core} [{suffix}]\"\n",
    "            else:\n",
    "                return pretty_core\n",
    "    # 매칭 안 되면 원래 이름 반환\n",
    "    return name\n",
    "\n",
    "\n",
    "def prettify_names(raw_names: list[str], mapping: dict[str, str]) -> list[str]:\n",
    "    # (1) ColumnTransformer 접두사 제거\n",
    "    cleaned = [clean_ct_feature_name(n) for n in raw_names]\n",
    "\n",
    "    # ★ (2) 심리검사 약어 보정 먼저 처리\n",
    "    mapping = inject_psych_scale_aliases(mapping)\n",
    "    cleaned2 = [prettify_psych_name(n, mapping) for n in cleaned]\n",
    "\n",
    "    # (3) BL코드 / LLM 등 일반 매핑 적용\n",
    "    pretty = [mapping.get(n, n) for n in cleaned2]\n",
    "    return pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f584ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_MAP_CSV = str(PROJECT_ROOT / \"results/new_analysis/260106/Feature Selection/simple_20/feature_summary.csv\")\n",
    "NAME_MAP = load_feature_name_map(FEATURE_MAP_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c81532",
   "metadata": {},
   "source": [
    "# 시각화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로\n",
    "OUT_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "FIG_DIR = OUT_DIR / \"figures/clinic_interpretation\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "\n",
    "# 모델/데이터 로드 + SHAP 계산\n",
    "def load_best_pipeline(label: str):\n",
    "    \"\"\"run_for_label에서 저장한 통합 파이프라인(.pkl) 로드\"\"\"\n",
    "    path = OUT_DIR / f\"{label}__best_model.pkl\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return joblib.load(path)\n",
    "\n",
    "\n",
    "def load_feature_names_transformed(label: str):\n",
    "    \"\"\"전처리 후 (원핫 포함) 최종 입력 피처명 (예쁘게 정리된)\"\"\"\n",
    "    p = OUT_DIR / f\"{label}__transformed_feature_names.json\"\n",
    "    if p.exists():\n",
    "        return json.load(open(p, \"r\"))\n",
    "    # 없으면 파이프라인에서 추출\n",
    "    pipe = load_best_pipeline(label)\n",
    "    preproc = pipe.named_steps[\"preprocessor\"]\n",
    "    try:\n",
    "        names = list(preproc.get_feature_names_out())\n",
    "    except Exception:\n",
    "        names = [\n",
    "            f\"f{i}\"\n",
    "            for i in range(\n",
    "                preproc.transform(\n",
    "                    pd.DataFrame([0], columns=pipe.feature_names_in_)\n",
    "                ).shape[1]\n",
    "            )\n",
    "        ]\n",
    "    return names\n",
    "\n",
    "\n",
    "def load_train_test(label: str):\n",
    "    \"\"\"최종 코드에서 저장한 test_predictions.csv(확률/정답) + 모델로부터 X_te 변환\"\"\"\n",
    "    # (1) 확률/정답\n",
    "    pred_csv = OUT_DIR / f\"{label}__test_predictions.csv\"\n",
    "    df_pred = pd.read_csv(pred_csv)  # columns: [label, y_true, y_prob, y_pred]\n",
    "    # (2) 파이프라인과 함께 Test 원본 X를 얻으려면, 모델링 단계에서 X_te를 저장해두는 게 베스트.\n",
    "    # 여기서는 파이프라인에 들어가기 전 원본 X_te를 아직 안 저장했다고 가정 → 아래 방식 사용:\n",
    "    #   - best_model에서 preprocessor 입력 칼럼 순서를 알아내고\n",
    "    #   - 같은 순서로 test csv에서 읽어서 transform\n",
    "    # 원본 test csv 경로는 기존 dict와 동일 규칙을 그대로 활용하면 가장 안전.\n",
    "    # (필요시 알맞게 수정)\n",
    "    IMP_DIR = Path(\n",
    "        str(PROJECT_ROOT / \"data/processed_imp/260106_split_corr_LLM_ADER/imputation/simple_imput\")\n",
    "    )\n",
    "    test_path = IMP_DIR / f\"simple_{label}_test.csv\"\n",
    "    raw = pd.read_csv(test_path)\n",
    "\n",
    "    # 타겟 컬럼 제거\n",
    "    y = raw[label].astype(int)\n",
    "    # run_for_label에서 사용된 “원본 피처 리스트” 저장 파일을 불러옴\n",
    "    feats_json = OUT_DIR / f\"{label}__features_final.json\"\n",
    "    feats = (\n",
    "        json.load(open(feats_json, \"r\"))\n",
    "        if feats_json.exists()\n",
    "        else [c for c in raw.columns if c != label]\n",
    "    )\n",
    "    X = raw[[c for c in feats if c in raw.columns]].copy()\n",
    "\n",
    "    return X, y, df_pred\n",
    "\n",
    "\n",
    "def shap_on_test(label: str, max_bg=200, force_tree_interventional=True):\n",
    "    \"\"\"Test셋 SHAP (확률기준) 계산해서 (Explanation, 변환후X, 피처명) 반환\"\"\"\n",
    "    pipe = load_best_pipeline(label)\n",
    "    preproc = pipe.named_steps[\"preprocessor\"]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "\n",
    "    X_raw, y_true, df_pred = load_train_test(label)\n",
    "    # transform\n",
    "    X_te_t = preproc.transform(X_raw)\n",
    "    feat_names = load_feature_names_transformed(label)\n",
    "\n",
    "    # background (train 일부 저장 안했으면 test 일부 사용 – OK)\n",
    "    rng = np.random.RandomState(42)\n",
    "    bg_idx = rng.choice(\n",
    "        X_te_t.shape[0], size=min(max_bg, X_te_t.shape[0]), replace=False\n",
    "    )\n",
    "    X_bg_t = X_te_t[bg_idx]\n",
    "\n",
    "    # explainer\n",
    "    try:\n",
    "        if force_tree_interventional:\n",
    "            explainer = shap.TreeExplainer(\n",
    "                clf,\n",
    "                data=X_bg_t,\n",
    "                feature_perturbation=\"interventional\",\n",
    "                model_output=\"probability\",\n",
    "            )\n",
    "        else:\n",
    "            explainer = shap.Explainer(clf, X_bg_t)\n",
    "        expl = explainer(X_te_t)\n",
    "    except Exception:\n",
    "        expl = shap.Explainer(clf, X_bg_t)(X_te_t)\n",
    "\n",
    "    # 다중출력 (n,p,2)인 경우 양성클래스만 취함\n",
    "    vals = np.array(expl.values)\n",
    "    if vals.ndim == 3 and vals.shape[2] == 2:\n",
    "        expl = expl[:, :, 1]\n",
    "\n",
    "    # feature names 보정\n",
    "    try:\n",
    "        expl.feature_names = feat_names\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return expl, X_te_t, y_true, df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beed123",
   "metadata": {},
   "source": [
    "# Feature Upset Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UpSet Plot (윈도우별 Top-K SHAP 교집합/합집합)\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    from upsetplot import UpSet, from_contents\n",
    "\n",
    "    HAS_UPSETPLOT = True\n",
    "except Exception:\n",
    "    HAS_UPSETPLOT = False\n",
    "    print(\"upsetplot 미설치. pip install upsetplot 권장합니다.\")\n",
    "\n",
    "\n",
    "def topk_features_by_label(label: str, K=10):\n",
    "    expl, *_ = shap_on_test(label)\n",
    "    mean_abs = np.abs(np.array(expl.values)).mean(axis=0)\n",
    "    feat = np.array(expl.feature_names)\n",
    "    order = np.argsort(mean_abs)[::-1][:K]\n",
    "    return list(feat[order])\n",
    "\n",
    "\n",
    "def collect_topk_across_labels(labels, K=10):\n",
    "    topk_map = {}\n",
    "    for lb in labels:\n",
    "        try:\n",
    "            topk_map[lb] = set(topk_features_by_label(lb, K=K))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {lb}: {e}\")\n",
    "    return topk_map  # {label: set(features)}\n",
    "\n",
    "\n",
    "def plot_upset_across_labels(labels, K=10, out_png=None):\n",
    "    if not HAS_UPSETPLOT:\n",
    "        print(\"upsetplot이 없어서 스킵합니다.\")\n",
    "        return\n",
    "    contents = collect_topk_across_labels(labels, K=K)\n",
    "    upset_data = from_contents(contents)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    UpSet(upset_data, subset_size=\"count\").plot()\n",
    "    plt.suptitle(f\"Top-{K} features overlap across outcomes\")\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사용\n",
    "# plot_upset_across_labels(\n",
    "#     LABELS,\n",
    "#     K=10,\n",
    "#     out_png=str(PROJECT_ROOT / \"results/step2_modeling/Final_modeling/figures/Feature/UpSet_top10_across_windows.png\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00914d0f",
   "metadata": {},
   "source": [
    "# SHAP t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e98283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP t-SNE (피처 중요도 공간 → 환자별 패턴 시각화)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def shap_tsne(label: str, perplexity=30, random_state=42, out_png=None):\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "    vals = np.array(expl.values)  # (n, p)\n",
    "    # 스케일링은 보통 불필요. 값의 스케일이 크게 다르면 정규화 추가 고려.\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        random_state=random_state,\n",
    "        perplexity=min(perplexity, max(5, vals.shape[0] // 4)),\n",
    "    )\n",
    "    Z = tsne.fit_transform(vals)\n",
    "\n",
    "    dfz = pd.DataFrame(\n",
    "        {\n",
    "            \"tsne1\": Z[:, 0],\n",
    "            \"tsne2\": Z[:, 1],\n",
    "            \"y_true\": y_true.values if hasattr(y_true, \"values\") else y_true,\n",
    "            \"y_prob\": df_pred[\"y_prob\"].values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc = plt.scatter(dfz.tsne1, dfz.tsne2, c=dfz.y_prob, s=18, alpha=0.8)\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    sns.kdeplot(\n",
    "        x=dfz.tsne1, y=dfz.tsne2, levels=3, color=\"k\", linewidths=0.6, alpha=0.3\n",
    "    )\n",
    "    plt.title(f\"SHAP t-SNE (patient-level) – {label}\")\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사용\n",
    "# for lb in LABELS:\n",
    "#     shap_tsne(lb, perplexity=25, out_png=FIG_DIR / f\"{lb}__SHAP_tSNE.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39bd11",
   "metadata": {},
   "source": [
    "# individual Force plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d30617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Individual Force Plot (개별 환자)\n",
    "def save_force_plots(label: str, indices=\"topk\", k=5, out_dir=None):\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "    base = np.mean(getattr(expl, \"base_values\", 0.0))\n",
    "    # Force plot은 1개 샘플 단위로 쓰는게 안정적\n",
    "    # 인덱스 선택\n",
    "    if indices == \"topk\":\n",
    "        order = np.argsort(-df_pred[\"y_prob\"].values)[:k]\n",
    "        idx_list = list(order)\n",
    "    elif isinstance(indices, (list, tuple, np.ndarray)):\n",
    "        idx_list = list(indices)\n",
    "    else:\n",
    "        idx_list = [indices]  # 단일 int\n",
    "\n",
    "    out_dir = Path(out_dir or FIG_DIR / f\"{label}__forceplots\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 간단한 small summary bar(전체 개요)도 함께 저장 (선택)\n",
    "    plt.figure()\n",
    "    shap.plots.bar(expl, max_display=20, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / f\"{label}__SHAP_bar_top20.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    for i in idx_list:\n",
    "        # 한 샘플짜리 Explanation 만들기\n",
    "        single = expl[i : i + 1]\n",
    "        # plotly 기반 force plot (HTML 저장 권장)\n",
    "        fp = shap.plots.force(\n",
    "            single.base_values,\n",
    "            single.values,\n",
    "            feature_names=single.feature_names,\n",
    "            matplotlib=False,\n",
    "        )\n",
    "        # 파일 저장\n",
    "        html_path = out_dir / f\"force_patient_{i}.html\"\n",
    "        shap.save_html(str(html_path), fp)\n",
    "        print(f\"[saved] {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbeafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사용: 각 아웃컴 별 상위 위험 5명 force plot 저장\n",
    "# for lb in LABELS:\n",
    "#     save_force_plots(lb, indices=\"topk\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_force_plots_static(\n",
    "#     label: str,\n",
    "#     indices=\"topk\",\n",
    "#     k=5,\n",
    "#     out_dir=None,\n",
    "#     figsize=(10, 5),\n",
    "#     dpi=300,\n",
    "#     as_pdf=False,\n",
    "# ):\n",
    "#     # expl: shap.Explanation (이미 2D, 양성클래스 슬라이싱은 shap_on_test에서 처리된 상태라고 가정)\n",
    "#     expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "#     # 저장 폴더\n",
    "#     out_dir = Path(out_dir or FIG_DIR / f\"{label}__forceplots\")\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # 인덱스 선택\n",
    "#     if indices == \"topk\":\n",
    "#         order = np.argsort(-df_pred[\"y_prob\"].values)[:k]\n",
    "#         idx_list = list(order)\n",
    "#     elif isinstance(indices, (list, tuple, np.ndarray)):\n",
    "#         idx_list = list(indices)\n",
    "#     else:\n",
    "#         idx_list = [int(indices)]\n",
    "\n",
    "#     # 각 환자 Force plot 저장 (Matplotlib backend → PNG/PDF)\n",
    "#     for i in idx_list:\n",
    "#         single = expl[i]  # Explanation의 i번째 샘플\n",
    "\n",
    "#         plt.figure(figsize=figsize)\n",
    "#         try:\n",
    "#             shap.plots.force(\n",
    "#                 single, matplotlib=True\n",
    "#             )  # SHAP가 현재 figure/axes에 그립니다\n",
    "#         except TypeError:\n",
    "#             # 일부 버전은 show 인자를 요구할 수 있음\n",
    "#             shap.plots.force(single, matplotlib=True, show=True)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] force plot failed at index {i}: {e}\")\n",
    "#             plt.close()\n",
    "#             continue\n",
    "\n",
    "#         stem = f\"force_patient_{i}\"\n",
    "#         if as_pdf:\n",
    "#             plt.savefig(out_dir / f\"{stem}.pdf\", bbox_inches=\"tight\")\n",
    "#         else:\n",
    "#             plt.savefig(out_dir / f\"{stem}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "#         plt.close()\n",
    "\n",
    "#     print(f\"[OK] Force plots saved to: {out_dir}\")\n",
    "\n",
    "\n",
    "# for lb in LABELS:\n",
    "#     save_force_plots_static(lb, indices=\"topk\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499cb45",
   "metadata": {},
   "source": [
    "# Umap clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f91f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient-level SHAP Clustering (t-SNE / UMAP on SHAP values)\n",
    "def shap_umap(label: str, n_neighbors=15, min_dist=0.1, random_state=42, out_png=None):\n",
    "    try:\n",
    "        import umap.umap_ as UMAP\n",
    "    except Exception as e:\n",
    "        print(\"UMAP 미설치 (pip install umap-learn). t-SNE 결과를 사용하세요.\")\n",
    "        return\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "    vals = np.array(expl.values)\n",
    "    reducer = UMAP.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    Z = reducer.fit_transform(vals)\n",
    "\n",
    "    dfz = pd.DataFrame(\n",
    "        {\n",
    "            \"u1\": Z[:, 0],\n",
    "            \"u2\": Z[:, 1],\n",
    "            \"y_true\": y_true.values if hasattr(y_true, \"values\") else y_true,\n",
    "            \"y_prob\": df_pred[\"y_prob\"].values,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc = plt.scatter(dfz.u1, dfz.u2, c=dfz.y_prob, s=18, alpha=0.8)\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"SHAP UMAP (patient-level) – {label}\")\n",
    "    plt.xlabel(\"UMAP 1\")\n",
    "    plt.ylabel(\"UMAP 2\")\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f041edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 사용\n",
    "# for lb in LABELS:\n",
    "#     shap_umap(lb, out_png=FIG_DIR / f\"{lb}__SHAP_UMAP.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f00fa7",
   "metadata": {},
   "source": [
    "# 개별 WaterFall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_waterfall_plots(\n",
    "    label: str,\n",
    "    indices=\"topk\",\n",
    "    k=5,\n",
    "    out_dir=None,\n",
    "    max_display=20,\n",
    "    figsize=(8, 6),\n",
    "    dpi=300,\n",
    "    as_pdf=False,\n",
    "):\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    out_dir = Path(out_dir or FIG_DIR / f\"{label}__waterfalls\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 인덱스 선택\n",
    "    if indices == \"topk\":\n",
    "        order = np.argsort(-df_pred[\"y_prob\"].values)[:k]\n",
    "        idx_list = list(order)\n",
    "    elif isinstance(indices, (list, tuple, np.ndarray)):\n",
    "        idx_list = list(indices)\n",
    "    else:\n",
    "        idx_list = [int(indices)]\n",
    "\n",
    "    for i in idx_list:\n",
    "        single = expl[i]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        try:\n",
    "            shap.plots.waterfall(single, max_display=max_display, show=False)\n",
    "        except TypeError:\n",
    "            shap.plots.waterfall(single, max_display=max_display)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] waterfall plot failed at index {i}: {e}\")\n",
    "            plt.close()\n",
    "            continue\n",
    "\n",
    "        stem = f\"waterfall_patient_{i}\"\n",
    "        if as_pdf:\n",
    "            plt.savefig(out_dir / f\"{stem}.pdf\", bbox_inches=\"tight\")\n",
    "        else:\n",
    "            plt.savefig(out_dir / f\"{stem}.png\", dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"[OK] Waterfall plots saved to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfae461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lb in LABELS:\n",
    "#     save_waterfall_plots(lb, indices=\"topk\", k=5, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91161a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_waterfall_plots(\n",
    "    label: str,\n",
    "    indices=\"topk_prob\",  # topk_prob / bottomk_prob / topk_shap / bottomk_shap / fp / fn / list(int)\n",
    "    k=5,\n",
    "    out_dir=None,\n",
    "    max_display=20,\n",
    "    figsize=(8, 6),\n",
    "    dpi=300,\n",
    "    as_pdf=False,\n",
    "):\n",
    "    # expl: shap.Explanation (n x p), df_pred: [\"y_prob\",\"y_true\",\"y_pred\"] 등\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    out_dir = Path(out_dir or FIG_DIR / f\"{label}__waterfalls\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 인덱스 선택 로직 ---\n",
    "    if isinstance(indices, (list, tuple, np.ndarray)):  # 직접 인덱스 리스트 제공\n",
    "        idx_list = list(map(int, indices))\n",
    "\n",
    "    else:\n",
    "        mode = str(indices).lower()\n",
    "\n",
    "        if mode == \"topk_prob\":  # 예측확률 상위 k\n",
    "            idx_list = np.argsort(-df_pred[\"y_prob\"].values)[:k].tolist()\n",
    "\n",
    "        elif mode == \"bottomk_prob\":  # 예측확률 하위 k\n",
    "            idx_list = np.argsort(df_pred[\"y_prob\"].values)[:k].tolist()\n",
    "\n",
    "        elif mode in {\"topk_shap\", \"bottomk_shap\"}:  # |SHAP| 합 기준 상/하위 k\n",
    "            # 각 환자별 총 영향력(절댓값 합)\n",
    "            shap_mag = np.abs(np.array(expl.values)).sum(axis=1)\n",
    "            order = (\n",
    "                np.argsort(-shap_mag) if mode == \"topk_shap\" else np.argsort(shap_mag)\n",
    "            )\n",
    "            idx_list = order[:k].tolist()\n",
    "\n",
    "        elif mode == \"fp\":  # False Positive 상위 k (확률 기준)\n",
    "            cand = df_pred[(df_pred[\"y_true\"] == 0) & (df_pred[\"y_pred\"] == 1)].copy()\n",
    "            cand = cand.sort_values(\"y_prob\", ascending=False).head(k)\n",
    "            idx_list = cand.index.tolist()\n",
    "\n",
    "        elif mode == \"fn\":  # False Negative 하위 k (확률 기준)\n",
    "            cand = df_pred[(df_pred[\"y_true\"] == 1) & (df_pred[\"y_pred\"] == 0)].copy()\n",
    "            cand = cand.sort_values(\"y_prob\", ascending=True).head(k)\n",
    "            idx_list = cand.index.tolist()\n",
    "\n",
    "        elif mode.isdigit():  # 단일 인덱스 문자열\n",
    "            idx_list = [int(mode)]\n",
    "\n",
    "        else:\n",
    "            # 기존 호환: \"topk\" -> topk_prob\n",
    "            idx_list = np.argsort(-df_pred[\"y_prob\"].values)[:k].tolist()\n",
    "\n",
    "    # --- 저장 ---\n",
    "    for i in idx_list:\n",
    "        single = expl[i]\n",
    "        plt.figure(figsize=figsize)\n",
    "        try:\n",
    "            shap.plots.waterfall(single, max_display=max_display, show=False)\n",
    "        except TypeError:\n",
    "            shap.plots.waterfall(single, max_display=max_display)\n",
    "        stem = f\"waterfall_patient_{i}\"\n",
    "        path = out_dir / (f\"{stem}.pdf\" if as_pdf else f\"{stem}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, dpi=None if as_pdf else dpi, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"[OK] Waterfall plots saved to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in LABELS:\n",
    "    save_waterfall_plots(lb, indices=\"topk_prob\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39263f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in LABELS:\n",
    "    save_waterfall_plots(lb, indices=\"bottomk_prob\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d36e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_waterfall_plots(lb, indices=\"topk_prob\", k=5)\n",
    "# save_waterfall_plots(lb, indices=\"bottomk_prob\", k=5)\n",
    "# save_waterfall_plots(lb, indices=\"topk_shap\", k=5)\n",
    "# save_waterfall_plots(lb, indices=\"bottomk_shap\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45cfe9",
   "metadata": {},
   "source": [
    "## ⭐️version 2⭐️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cb927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Paths (EDIT if needed) ======\n",
    "BASE_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "FIG_DIR = BASE_DIR / \"figures/clinic_interpretation/personal_waterfall_figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Helpers ======\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(\n",
    "    label: str, base_dir: Path = BASE_DIR\n",
    ") -> Optional[float]:\n",
    "    # support both __ and ___ prior to 'youden'\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "def _top_abs_shap_indices(\n",
    "    expl: shap.Explanation, k: int, reverse: bool = True\n",
    ") -> List[int]:\n",
    "    vals = np.array(expl.values)\n",
    "    mag = np.abs(vals).sum(axis=1)\n",
    "    order = np.argsort(-mag if reverse else mag)\n",
    "    return order[:k].tolist()\n",
    "\n",
    "\n",
    "def _pick_indices(\n",
    "    mode: str,\n",
    "    k: int,\n",
    "    expl: shap.Explanation,\n",
    "    df_pred: pd.DataFrame,\n",
    "    youden_thr: Optional[float],\n",
    ") -> List[int]:\n",
    "    mode = str(mode).lower()\n",
    "    if isinstance(k, (list, tuple, np.ndarray)):\n",
    "        return list(map(int, k))\n",
    "\n",
    "    if mode == \"topk_prob\":\n",
    "        return np.argsort(-df_pred[\"y_prob\"].values)[:k].tolist()\n",
    "    if mode == \"bottomk_prob\":\n",
    "        return np.argsort(df_pred[\"y_prob\"].values)[:k].tolist()\n",
    "    if mode == \"topk_shap\":\n",
    "        return _top_abs_shap_indices(expl, k, reverse=True)\n",
    "    if mode == \"bottomk_shap\":\n",
    "        return _top_abs_shap_indices(expl, k, reverse=False)\n",
    "    if mode == \"fp\":\n",
    "        cand = df_pred[(df_pred[\"y_true\"] == 0) & (df_pred[\"y_pred\"] == 1)]\n",
    "        return cand.sort_values(\"y_prob\", ascending=False).head(k).index.tolist()\n",
    "    if mode == \"fn\":\n",
    "        cand = df_pred[(df_pred[\"y_true\"] == 1) & (df_pred[\"y_pred\"] == 0)]\n",
    "        return cand.sort_values(\"y_prob\", ascending=True).head(k).index.tolist()\n",
    "    if mode == \"tp\":\n",
    "        cand = df_pred[(df_pred[\"y_true\"] == 1) & (df_pred[\"y_pred\"] == 1)]\n",
    "        return cand.sort_values(\"y_prob\", ascending=False).head(k).index.tolist()\n",
    "    if mode == \"tn\":\n",
    "        cand = df_pred[(df_pred[\"y_true\"] == 0) & (df_pred[\"y_pred\"] == 0)]\n",
    "        return cand.sort_values(\"y_prob\", ascending=True).head(k).index.tolist()\n",
    "    if mode == \"borderline_k\":\n",
    "        thr = (\n",
    "            youden_thr\n",
    "            if youden_thr is not None\n",
    "            else _compute_youden_from_preds(\n",
    "                df_pred[\"y_true\"].values, df_pred[\"y_prob\"].values\n",
    "            )\n",
    "        )\n",
    "        df_tmp = df_pred.copy()\n",
    "        df_tmp[\"dist\"] = (df_tmp[\"y_prob\"] - thr).abs()\n",
    "        # pick closest k, but keep balance across sides if possible\n",
    "        topk = df_tmp.sort_values(\"dist\").head(max(k, 2))\n",
    "        # try to split by side\n",
    "        above = topk[topk[\"y_prob\"] >= thr].head((k + 1) // 2)\n",
    "        below = topk[topk[\"y_prob\"] < thr].head(k // 2)\n",
    "        idx_list = list(pd.concat([above, below]).index.unique())\n",
    "        # if not enough both sides, just fill with nearest\n",
    "        if len(idx_list) < k:\n",
    "            idx_list = df_tmp.sort_values(\"dist\").head(k).index.tolist()\n",
    "        return idx_list\n",
    "    if mode == \"top_change\":\n",
    "        # large SHAP magnitude but mid-range probability → clear structure with non-extreme risk\n",
    "        vals = np.array(expl.values)\n",
    "        mag = np.abs(vals).sum(axis=1)\n",
    "        q1, q2 = 0.35, 0.65\n",
    "        mid = df_pred[(df_pred[\"y_prob\"] >= q1) & (df_pred[\"y_prob\"] <= q2)]\n",
    "        if len(mid) < k:\n",
    "            mid = df_pred.copy()  # fallback\n",
    "        order = mid.assign(_mag=mag[mid.index]).sort_values(\"_mag\", ascending=False)\n",
    "        return order.head(k).index.tolist()\n",
    "\n",
    "    if mode.isdigit():\n",
    "        return [int(mode)]\n",
    "\n",
    "    # default: topk_prob\n",
    "    return np.argsort(-df_pred[\"y_prob\"].values)[:k].tolist()\n",
    "\n",
    "\n",
    "def _extract_top_features_for_row(\n",
    "    expl_row: shap.Explanation, max_display: int, feature_names: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    vals = np.array(expl_row.values).reshape(-1)\n",
    "    order = np.argsort(-np.abs(vals))[:max_display]\n",
    "    feats = [feature_names[i] for i in order]\n",
    "    shap_vals = vals[order]\n",
    "    # Try to fetch original feature values if present (can be None)\n",
    "    try:\n",
    "        data_vals = np.array(expl_row.data).reshape(-1)[order]\n",
    "    except Exception:\n",
    "        data_vals = [np.nan] * len(order)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"rank\": np.arange(1, len(order) + 1),\n",
    "            \"feature\": feats,\n",
    "            \"feature_value\": data_vals,\n",
    "            \"shap_value\": shap_vals,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Main function ======\n",
    "def save_waterfall_plots_pro(\n",
    "    label: str,\n",
    "    indices: \"str|Iterable[int]\" = \"topk_prob\",\n",
    "    k: int = 6,\n",
    "    out_dir: Optional[str | Path] = None,\n",
    "    max_display: int = 20,\n",
    "    figsize: Tuple[float, float] = (8, 6),\n",
    "    dpi: int = 300,\n",
    "    as_pdf: bool = False,\n",
    "    xlim_mode: str = \"per_plot\",  # 'fixed' or 'per_plot'\n",
    "    fixed_xlim: Optional[Tuple[float, float]] = None,\n",
    "):\n",
    "    \"\"\"Save a batch of individual waterfall plots with robust case selection.\n",
    "    Returns a summary DataFrame of saved cases.\n",
    "    \"\"\"\n",
    "    # ---- fetch SHAP & predictions\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "    out_dir = Path(out_dir or (FIG_DIR / f\"{label}__waterfalls_pro\"))\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure prediction columns present\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= 0.5).astype(int))\n",
    "\n",
    "    # Youden threshold (optional for modes)\n",
    "    ythr = _try_read_youden_threshold(label)\n",
    "\n",
    "    # pick index list\n",
    "    idx_list = _pick_indices(indices, k, expl, df_pred, ythr)\n",
    "\n",
    "    # feature names\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        feat_names = list(expl.feature_names)\n",
    "    else:\n",
    "        feat_names = [f\"f{i}\" for i in range(np.array(expl.values).shape[1])]\n",
    "\n",
    "    # Determine fixed xlim if requested\n",
    "    global_xlim = None\n",
    "    if xlim_mode == \"fixed\" and fixed_xlim is None:\n",
    "        # use 95th percentile of |SHAP| sum as symmetric bound\n",
    "        mags = np.abs(np.array(expl.values)).sum(axis=1)\n",
    "        L = float(np.percentile(mags, 95))\n",
    "        global_xlim = (-L, +L)\n",
    "    elif fixed_xlim is not None:\n",
    "        global_xlim = fixed_xlim\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in idx_list:\n",
    "        row = expl[i]\n",
    "        y_t = (\n",
    "            int(df_pred.loc[i, \"y_true\"])\n",
    "            if \"y_true\" in df_pred.columns\n",
    "            else int(np.asarray(y_true)[i])\n",
    "        )\n",
    "        y_p = (\n",
    "            int(df_pred.loc[i, \"y_pred\"])\n",
    "            if \"y_pred\" in df_pred.columns\n",
    "            else int((df_pred.loc[i, \"y_prob\"]) >= 0.5)\n",
    "        )\n",
    "        y_prob = (\n",
    "            float(df_pred.loc[i, \"y_prob\"]) if \"y_prob\" in df_pred.columns else np.nan\n",
    "        )\n",
    "\n",
    "        # --- plot\n",
    "        plt.figure(figsize=figsize)\n",
    "        try:\n",
    "            ax = shap.plots.waterfall(row, max_display=max_display, show=False)\n",
    "        except Exception:\n",
    "            ax = shap.plots.waterfall(row, max_display=max_display)\n",
    "        ax = plt.gca()\n",
    "\n",
    "        # title\n",
    "        thr_txt = f\", thr={ythr:.3f}\" if ythr is not None else \"\"\n",
    "        plt.title(\n",
    "            f\"{label} | id={i} | y_true={y_t}, y_pred={y_p}, y_prob={y_prob:.3f}{thr_txt}\"\n",
    "        )\n",
    "\n",
    "        # optional fixed xlim for comparability\n",
    "        if global_xlim is not None:\n",
    "            ax.set_xlim(global_xlim)\n",
    "\n",
    "        stem = f\"waterfall_patient_{i}\"\n",
    "        fpath = out_dir / (f\"{stem}.pdf\" if as_pdf else f\"{stem}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fpath, dpi=None if as_pdf else dpi, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        # extract top contributors CSV\n",
    "        df_top = _extract_top_features_for_row(\n",
    "            row, max_display=max_display, feature_names=feat_names\n",
    "        )\n",
    "        df_top.to_csv(out_dir / f\"{stem}__top{max_display}.csv\", index=False)\n",
    "\n",
    "        # keep a summary row\n",
    "        rows.append(\n",
    "            {\n",
    "                \"idx\": i,\n",
    "                \"file\": str(fpath),\n",
    "                \"y_true\": y_t,\n",
    "                \"y_pred\": y_p,\n",
    "                \"y_prob\": y_prob,\n",
    "                \"youden_thr\": ythr,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    summary = pd.DataFrame(rows)\n",
    "    summary.to_csv(out_dir / f\"summary__{label}__{indices}.csv\", index=False)\n",
    "    print(f\"[OK] saved {len(rows)} plots → {out_dir}\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Convenience batch runner ======\n",
    "\n",
    "\n",
    "def run_personal_waterfalls_batch(\n",
    "    label: str,\n",
    "    base_outdir: str | Path,\n",
    "    youden_cases_k: int = 6,\n",
    "    tp_k: int = 5,\n",
    "    fn_k: int = 5,\n",
    "    max_display: int = 20,\n",
    "):\n",
    "    base_outdir = Path(base_outdir)\n",
    "    # 1) Youden-near borderline\n",
    "    out1 = base_outdir / label / \"borderline\"\n",
    "    save_waterfall_plots_pro(\n",
    "        label=label,\n",
    "        indices=\"borderline_k\",\n",
    "        k=youden_cases_k,\n",
    "        out_dir=out1,\n",
    "        max_display=max_display,\n",
    "        xlim_mode=\"fixed\",\n",
    "    )\n",
    "    # 2) High-risk True Positives\n",
    "    out2 = base_outdir / label / \"true_positive\"\n",
    "    save_waterfall_plots_pro(\n",
    "        label=label,\n",
    "        indices=\"tp\",\n",
    "        k=tp_k,\n",
    "        out_dir=out2,\n",
    "        max_display=max_display,\n",
    "        xlim_mode=\"per_plot\",\n",
    "    )\n",
    "    # 3) False Negatives\n",
    "    out3 = base_outdir / label / \"false_negative\"\n",
    "    save_waterfall_plots_pro(\n",
    "        label=label,\n",
    "        indices=\"fn\",\n",
    "        k=fn_k,\n",
    "        out_dir=out3,\n",
    "        max_display=max_display,\n",
    "        xlim_mode=\"per_plot\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Multi-outcome loop ======\n",
    "\n",
    "\n",
    "def run_all_labels_waterfalls(\n",
    "    labels: List[str],\n",
    "    base_outdir: str | Path,\n",
    "    youden_cases_k: int = 6,\n",
    "    tp_k: int = 5,\n",
    "    fn_k: int = 5,\n",
    "    max_display: int = 20,\n",
    "):\n",
    "    base_outdir = Path(base_outdir)\n",
    "    for lb in labels:\n",
    "        print(f\"\\n>>> Running personal waterfall for {lb}\")\n",
    "        run_personal_waterfalls_batch(\n",
    "            label=lb,\n",
    "            base_outdir=base_outdir,\n",
    "            youden_cases_k=youden_cases_k,\n",
    "            tp_k=tp_k,\n",
    "            fn_k=fn_k,\n",
    "            max_display=max_display,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7effc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_all = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "# run_all_labels_waterfalls(\n",
    "#     labels=labels_all,\n",
    "#     base_outdir=str(PROJECT_ROOT / \"results/step2_modeling/251127_Final_modeling_globalunion25/figures/clinic_interpretation/personal_waterfall_figures\"),\n",
    "#     youden_cases_k=6,\n",
    "#     tp_k=5,\n",
    "#     fn_k=5,\n",
    "#     max_display=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda691d6",
   "metadata": {},
   "source": [
    "## UMAP + DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81215058",
   "metadata": {},
   "source": [
    "## version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b263f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 추가 import ---\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f002a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 경로\n",
    "MODEL_DIR = str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    "OUT_ROOT = Path(MODEL_DIR) / \"figures/clinic_interpretation/PatientClusters\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1abc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# ★ UMAP + DBSCAN (적응형 튜닝) ★\n",
    "# --------------------------\n",
    "def plot_cluster_heatmap(\n",
    "    vals, feat_names, labels, topN, out_path, cmap: str = \"rocket\"\n",
    "):\n",
    "    from textwrap import fill\n",
    "\n",
    "    # --- 유효 클러스터만 추출 ---\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "    if not clusters:\n",
    "        return\n",
    "\n",
    "    # --- 클러스터별 평균 |SHAP| ---\n",
    "    mean_abs = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        mean_abs.append(np.abs(vals[mask]).mean(axis=0))\n",
    "    M = np.vstack(mean_abs)\n",
    "\n",
    "    # --- 전체에서 상위 topN 피처 선택 ---\n",
    "    top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    # --- 각 클러스터 행 정규화 (상대적 중요도) ---\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max\n",
    "\n",
    "    # --- plotting data 전치 (features → y축, clusters → x축) ---\n",
    "    M_plot = M_disp.T\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]  # 자동 줄바꿈\n",
    "\n",
    "    # --- figure 크기 계산 (적절히 압축) ---\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        M_plot,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # --- 스타일 조정 ---\n",
    "    ax.set_title(\n",
    "        \"Cluster × Feature importance\",  # (row-normalized)\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features by mean |SHAP|\", fontsize=10, labelpad=6)\n",
    "\n",
    "    # 🔹 x축: 글씨 겹침 방지 (회전 + 작은 폰트)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    # 🔹 y축: 글씨 크기 축소 및 패딩\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "\n",
    "    # 🔹 컬러바 폰트도 줄이기\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.15)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "    return X\n",
    "\n",
    "\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    \"\"\"UMAP latent space에서 k-distance의 pct-백분위를 eps 초기값으로 사용.\"\"\"\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    # 각 점에서 k번째 이웃까지의 거리(자기 자신 제외)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps\n",
    "\n",
    "\n",
    "def cluster_umap_dbscan_tuned(\n",
    "    vals: np.ndarray,\n",
    "    random_state: int = 42,\n",
    "    # UMAP(클러스터용) 세팅\n",
    "    umap_n_components: int = 12,\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"cosine\",\n",
    "    # DBSCAN 초기값\n",
    "    min_samples: int = 6,\n",
    "    eps: float | None = None,  # None이면 k-distance로 추정\n",
    "    # 적응형 종료 조건\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    V = _standardize(vals)\n",
    "\n",
    "    # 1) UMAP 고차원 임베딩(클러스터링용)\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # eps 초기값이 없으면 k-distance 기반으로 추정\n",
    "    if eps is None:\n",
    "        eps = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.85)\n",
    "\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "\n",
    "    # 2) 적응형 루프\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples, metric=\"euclidean\", n_jobs=-1)\n",
    "        labels = db.fit_predict(X_hi)\n",
    "\n",
    "        n = len(labels)\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=eps,\n",
    "                min_samples=min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 종료 조건\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        # 노이즈가 너무 많다 → eps ↑ (이웃 더 많이 포함), min_samples ↓\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            eps *= 1.20\n",
    "            if min_samples > 3:\n",
    "                min_samples -= 1\n",
    "        # 군집이 너무 적다(과병합) → eps ↓ (더 촘촘하게)\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            eps *= 0.88\n",
    "        else:\n",
    "            # 애매하면 소폭 완화\n",
    "            eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # 3) 시각화용 2D UMAP\n",
    "    um_2d = UMAP.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=max(umap_n_neighbors, 15),\n",
    "        min_dist=max(umap_min_dist, 0.05),\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    info = dict(\n",
    "        final_eps=eps,\n",
    "        final_min_samples=min_samples,\n",
    "        trials=trial,\n",
    "        history=info_hist,\n",
    "        n_clusters=len(set(labels) - {-1}),\n",
    "        n_noise=int(np.sum(labels == -1)),\n",
    "    )\n",
    "    return labels, Z2, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d95e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_cluster_analysis_for_label(\n",
    "#     label: str,\n",
    "#     algo: str = \"dbscan_tuned\",\n",
    "#     k=None,\n",
    "#     topN: int = 10,\n",
    "#     save_representatives: bool = True,\n",
    "# ):\n",
    "#     print(f\"\\n=== {label}: SHAP clustering & summaries (algo={algo}) ===\")\n",
    "#     out_dir = OUT_ROOT / label\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "#     # Youden threshold 확보 및 df_pred 보정\n",
    "#     ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "#     if ythr is None:\n",
    "#         ythr = _compute_youden_from_preds(\n",
    "#             np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "#         )\n",
    "#     if \"y_true\" not in df_pred.columns:\n",
    "#         df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "#     if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "#         df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "#     # --- 군집화 ---\n",
    "#     if algo == \"dbscan_tuned\":\n",
    "#         labels, Z, info = cluster_umap_dbscan_tuned(vals)\n",
    "#         print(\n",
    "#             f\"[DBSCAN tuned] clusters={info['n_clusters']}, noise={info['n_noise']}, \"\n",
    "#             f\"eps={info['final_eps']:.3f}, min_samples={info['final_min_samples']}, \"\n",
    "#             f\"trials={info['trials']}\"\n",
    "#         )\n",
    "#         # 2D 시각화 (cluster 색)\n",
    "#         uniq = sorted(np.unique(labels))\n",
    "#         cmap = plt.get_cmap(\"tab20\")\n",
    "#         color_map = {c: (\"#bbbbbb\" if c == -1 else cmap(c % 20)) for c in uniq}\n",
    "#         colors = [color_map[c] for c in labels]\n",
    "#         plt.figure(figsize=(7.2, 6.2))\n",
    "#         plt.scatter(Z[:, 0], Z[:, 1], c=colors, s=18, alpha=0.9)\n",
    "#         plt.title(\n",
    "#             f\"UMAP(2D) + DBSCAN — {info['n_clusters']} clusters, noise={info['n_noise']}\"\n",
    "#         )\n",
    "#         plt.xlabel(\"UMAP-1\")\n",
    "#         plt.ylabel(\"UMAP-2\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "#         # 위험도 색\n",
    "#         plt.figure(figsize=(7.2, 6.2))\n",
    "#         sc = plt.scatter(Z[:, 0], Z[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "#         plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "#         plt.title(f\"UMAP(2D) colored by risk — {label}\")\n",
    "#         plt.xlabel(\"UMAP-1\")\n",
    "#         plt.ylabel(\"UMAP-2\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "#         cluster_ids = labels\n",
    "\n",
    "#     elif algo == \"hdbscan_tuned\":\n",
    "#         # ← 너가 이미 만든 HDBSCAN 버전 그대로 사용\n",
    "#         labels, probs, Z, info = cluster_umap_hdbscan_tuned(vals)\n",
    "#         cluster_ids = labels\n",
    "#         # (플롯 저장은 기존 코드 그대로)\n",
    "\n",
    "#     else:  # KMeans 대안\n",
    "#         vals_std = (vals - vals.mean(0)) / (vals.std(0) + 1e-9)\n",
    "#         kk = k or pick_k_by_silhouette(vals_std, k_min=2, k_max=6, random_state=42)\n",
    "#         km = KMeans(n_clusters=kk, random_state=42, n_init=20)\n",
    "#         cluster_ids = km.fit_predict(vals_std)\n",
    "#         um = UMAP.UMAP(\n",
    "#             n_components=2,\n",
    "#             n_neighbors=30,\n",
    "#             min_dist=0.1,\n",
    "#             metric=\"cosine\",\n",
    "#             random_state=42,\n",
    "#         )\n",
    "#         Z = um.fit_transform(vals_std)\n",
    "#         plt.figure(figsize=(7.2, 6.2))\n",
    "#         sc = plt.scatter(Z[:, 0], Z[:, 1], c=cluster_ids, s=18, alpha=0.9, cmap=\"tab10\")\n",
    "#         plt.colorbar(sc, label=\"Cluster ID\")\n",
    "#         plt.title(f\"UMAP(2D) + KMeans — k={kk}\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(out_dir / f\"{label}__UMAP2D_KMeans.png\", dpi=300)\n",
    "#         plt.close()\n",
    "\n",
    "#     # --- 요약/히트맵/대표 waterfall (그대로) ---\n",
    "#     df_summary = summarize_clusters(\n",
    "#         vals, feat_names, y_prob, y_true, cluster_ids, topN=topN\n",
    "#     )\n",
    "#     df_summary.to_csv(out_dir / f\"{label}__cluster_summary_{algo}.csv\", index=False)\n",
    "#     plot_cluster_heatmap(\n",
    "#         vals,\n",
    "#         feat_names,\n",
    "#         cluster_ids,\n",
    "#         topN=topN,\n",
    "#         out_path=out_dir / f\"{label}__heatmap_top{topN}_{algo}.png\",\n",
    "#     )\n",
    "\n",
    "#     if save_representatives:\n",
    "#         reps = pick_representatives(y_prob, cluster_ids)\n",
    "#         rep_dir = out_dir / f\"representatives_{algo}\"\n",
    "#         for rep in reps:\n",
    "#             c = rep[\"cluster\"]\n",
    "#             for tag in [\"median_idx\", \"max_idx\", \"min_idx\"]:\n",
    "#                 i = rep[tag]\n",
    "#                 name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "#                 save_local_plots(\n",
    "#                     label,\n",
    "#                     expl,\n",
    "#                     i,\n",
    "#                     rep_dir / f\"cluster_{c}\",\n",
    "#                     feat_names=feat_names,\n",
    "#                     max_display=15,\n",
    "#                     df_pred=df_pred,\n",
    "#                     youden_thr=ythr,\n",
    "#                     name_tag=name_tag,\n",
    "#                 )\n",
    "\n",
    "#     print(f\"-> saved: {out_dir}\")\n",
    "#     return df_summary, cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd730a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cluster_summaries = []\n",
    "# for lb in LABELS:\n",
    "#     try:\n",
    "#         df_sum, labels = run_cluster_analysis_for_label(\n",
    "#             lb, algo=\"dbscan_tuned\", topN=10, save_representatives=True\n",
    "#         )\n",
    "#         df_sum.insert(0, \"label\", lb)\n",
    "#         all_cluster_summaries.append(df_sum)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] {lb}: {e}\")\n",
    "\n",
    "# if all_cluster_summaries:\n",
    "#     all_summ = pd.concat(all_cluster_summaries, ignore_index=True)\n",
    "#     all_summ.to_csv(\n",
    "#         OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.csv\", index=False\n",
    "#     )\n",
    "#     print(\"✅ saved:\", OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df726a3",
   "metadata": {},
   "source": [
    "## version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import re, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from scipy.stats import chi2_contingency, kruskal\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- UMAP\n",
    "try:\n",
    "    import umap.umap_ as UMAP\n",
    "\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "# =============== 경로/전역 ===============\n",
    "MODEL_DIR = str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    "OUT_ROOT = Path(MODEL_DIR) / \"figures/clinic_interpretation/PatientClusters\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 없으면 기본 라벨\n",
    "if \"LABELS\" not in globals():\n",
    "    LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "# ---- 가독명 매핑 (없을 때 fallback)\n",
    "if \"clean_name\" not in globals():\n",
    "\n",
    "    def clean_name(name: str) -> str:\n",
    "        name = re.sub(r\"^[^_]+__\", \"\", str(name))\n",
    "        name = re.sub(r\"^(num_|cat_|code_)\", \"\", name)\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 유틸 ===============\n",
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.nan_to_num(\n",
    "        X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "    )\n",
    "    return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "    plt.savefig(figpath, **savefig_kwargs)\n",
    "    try:\n",
    "        plt.savefig(\n",
    "            figpath.with_suffix(\".svg\"),\n",
    "            bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============== Youden 임계값 ===============\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# =============== SHAP 로딩 ===============\n",
    "def get_shap_matrix(label: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      expl: shap.Explanation (n, p)\n",
    "      vals: ndarray (n, p)\n",
    "      feat_names: list[str] length p (cleaned)\n",
    "      y_prob: ndarray (n,)\n",
    "      y_true: ndarray (n,)\n",
    "      X_te_t: ndarray (n, p_after_preproc)\n",
    "      df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "    \"\"\"\n",
    "    # ⚠️ 기존 프로젝트의 함수 가정\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    vals = np.asarray(expl.values, dtype=float)\n",
    "    vals = np.nan_to_num(\n",
    "        vals,\n",
    "        nan=0.0,\n",
    "        posinf=np.finfo(float).max / 1e6,\n",
    "        neginf=-np.finfo(float).max / 1e6,\n",
    "    )\n",
    "\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        raw_names = list(expl.feature_names)\n",
    "    else:\n",
    "        raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "    feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "    y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "    y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "    return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# =============== k-distance & eps 추정 ===============\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps, kth\n",
    "\n",
    "\n",
    "def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "    plt.xlabel(\"Points (sorted)\")\n",
    "    plt.ylabel(f\"{k}-NN distance\")\n",
    "    plt.title(f\"k-distance curve (k={k})\")\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 히트맵 ===============\n",
    "def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "    return (\n",
    "        np.abs(values).mean(axis=0)\n",
    "        if how == \"mean\"\n",
    "        else np.mean(np.abs(values), axis=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cluster_heatmap(\n",
    "    vals,\n",
    "    feat_names,\n",
    "    labels,\n",
    "    topN,\n",
    "    out_path: Path,\n",
    "    cmap: str = \"rocket\",\n",
    "    agg: str = \"mean\",\n",
    "):\n",
    "    from textwrap import fill\n",
    "\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]  # 노이즈 제외\n",
    "    if not clusters:\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        rows.append(_agg_abs(vals[mask], how=agg))\n",
    "    M = np.vstack(rows)\n",
    "\n",
    "    # top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    # M_top = M[:, top_idx]\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    global_imp = _agg_abs(vals, how=agg)  # shape (p,)  # median(|SHAP|)\n",
    "    top_idx = np.argsort(global_imp)[::-1][:topN]\n",
    "\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max\n",
    "\n",
    "    M_plot = M_disp.T\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]\n",
    "\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        M_plot,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",  # row-normalized\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 대표 케이스 ===============\n",
    "def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        probs = y_prob[idx]\n",
    "        med = np.mean(probs)\n",
    "        med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "        max_idx = idx[np.argmax(probs)]\n",
    "        min_idx = idx[np.argmin(probs)]\n",
    "        reps.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"mean\": int(med_idx),\n",
    "                \"max_idx\": int(max_idx),\n",
    "                \"min_idx\": int(min_idx),\n",
    "            }\n",
    "        )\n",
    "    return reps\n",
    "\n",
    "\n",
    "def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        d = df_pred.iloc[idx]\n",
    "        rec = {\"cluster\": int(c)}\n",
    "        fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "        fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "        if len(fp):\n",
    "            rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "        if len(fn):\n",
    "            rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "        reps.append(rec)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def save_local_plots(\n",
    "    label: str,\n",
    "    expl: shap.Explanation,\n",
    "    idx: int,\n",
    "    out_dir: Path,\n",
    "    feat_names=None,\n",
    "    max_display=15,\n",
    "    df_pred: pd.DataFrame | None = None,\n",
    "    youden_thr: float | None = None,\n",
    "    name_tag: str | None = None,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    y_t = y_p = None\n",
    "    y_prob = None\n",
    "    thr_txt = \"\"\n",
    "    if df_pred is not None:\n",
    "        if \"y_true\" in df_pred.columns:\n",
    "            y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "        if \"y_prob\" in df_pred.columns:\n",
    "            y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "        if \"y_pred\" in df_pred.columns:\n",
    "            y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "        else:\n",
    "            thr = youden_thr if youden_thr is not None else 0.5\n",
    "            y_p = (\n",
    "                int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "                if \"y_prob\" in df_pred.columns\n",
    "                else None\n",
    "            )\n",
    "        if youden_thr is not None:\n",
    "            thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "    # waterfall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "    except Exception:\n",
    "        row = expl[idx]\n",
    "        shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "    title_bits = [f\"{label} | id={idx}\"]\n",
    "    if y_t is not None:\n",
    "        title_bits.append(f\"y_true={y_t}\")\n",
    "    if y_p is not None:\n",
    "        title_bits.append(f\"y_pred={y_p}\")\n",
    "    if y_prob is not None:\n",
    "        title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "    if name_tag:\n",
    "        title_bits.append(f\"[{name_tag}]\")\n",
    "    ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "    plt.title(ttl)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(\n",
    "        out_dir\n",
    "        / (\n",
    "            f\"{label}__waterfall_id{idx}\"\n",
    "            + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "            + \".png\"\n",
    "        ),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # force (옵션)\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 2.8))\n",
    "        shap.plots.force(\n",
    "            expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_png_svg(\n",
    "            out_dir\n",
    "            / (\n",
    "                f\"{label}__force_id{idx}\"\n",
    "                + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "                + \".png\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "    except Exception:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============== DBSCAN 통계/요약 ===============\n",
    "def add_dbscan_stats(\n",
    "    out_dir: Path,\n",
    "    label: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "):\n",
    "    lab = np.asarray(labels)\n",
    "    clusters = [c for c in np.unique(lab) if c != -1]\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        m = lab == c\n",
    "        rows.append(\n",
    "            dict(\n",
    "                cluster=int(c),\n",
    "                n=int(m.sum()),\n",
    "                event_rate=float(np.mean(y_true[m] == 1)),\n",
    "                mean_y_prob=float(np.mean(y_prob[m])),\n",
    "            )\n",
    "        )\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    chi_p = np.nan\n",
    "    kw_p = np.nan\n",
    "    if len(clusters) >= 2:\n",
    "        table = [\n",
    "            [\n",
    "                int(np.sum((lab == c) & (y_true == 1))),\n",
    "                int(np.sum((lab == c) & (y_true == 0))),\n",
    "            ]\n",
    "            for c in clusters\n",
    "        ]\n",
    "        chi_p = chi2_contingency(table)[1]\n",
    "        groups = [y_prob[lab == c] for c in clusters]\n",
    "        kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "    df[\"p_event_rate(chi2)\"] = chi_p\n",
    "    df[\"p_yprob(kruskal)\"] = kw_p\n",
    "    df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 메인 루틴 ===============\n",
    "def run_cluster_analysis_for_label(\n",
    "    label: str,\n",
    "    umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"cosine\",\n",
    "    min_samples: int = 6,  # DBSCAN\n",
    "    eps: float | None = None,\n",
    "    dbscan_metric: str = \"euclidean\",\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "    topN: int = 10,\n",
    "    agg_for_heatmap: str = \"mean\",\n",
    "    save_representatives: bool = True,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "    out_dir = OUT_ROOT / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 0) 데이터 로드\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # Youden 임계값 & df_pred 보정\n",
    "    ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "    if ythr is None:\n",
    "        ythr = _compute_youden_from_preds(\n",
    "            np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "        )\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "    # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "    V = _standardize(vals)\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "    if eps is None:\n",
    "        eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.85)\n",
    "        eps = eps0\n",
    "        save_kdist_plot(\n",
    "            kth,\n",
    "            k=min_samples,\n",
    "            out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "        )\n",
    "\n",
    "    # 3) DBSCAN 적응형 루프\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "    cur_min_samples = int(min_samples)\n",
    "    cur_eps = float(eps)\n",
    "\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            cur_eps *= 1.20\n",
    "            if cur_min_samples > 3:\n",
    "                cur_min_samples -= 1\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            cur_eps *= 0.88\n",
    "        else:\n",
    "            cur_eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # all-noise 대비: 마지막 완화 시도 1회\n",
    "    if (labels is None) or np.all(labels == -1):\n",
    "        cur_eps *= 1.30\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=\"final_relax\",\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "    um_2d = UMAP.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=max(umap_n_neighbors, 15),\n",
    "        min_dist=max(umap_min_dist, 0.05),\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "    m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # 6) 2D 플롯 (클러스터 색)\n",
    "    uniq = sorted(np.unique(labels))\n",
    "    cmap = plt.get_cmap(\"tab20\")  # tab20\n",
    "    color_map = {c: (\"#bbbbbb\" if c == -1 else cmap(c % 20)) for c in uniq}\n",
    "    colors = [color_map[c] for c in labels]\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    plt.scatter(Z2[:, 0], Z2[:, 1], c=colors, s=18, alpha=0.9)\n",
    "    title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "    plt.title(f\"{title} — {len(set(labels)-{-1})} clusters, noise={(labels==-1).sum()}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 6-2) 2D 플롯 (위험도 색)\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 7) 클러스터 통계/요약 + 히트맵\n",
    "    df_stats = add_dbscan_stats(\n",
    "        out_dir,\n",
    "        label,\n",
    "        y_true=np.asarray(y_true),\n",
    "        y_prob=np.asarray(y_prob),\n",
    "        labels=np.asarray(labels),\n",
    "    )\n",
    "    plot_cluster_heatmap(\n",
    "        vals,\n",
    "        feat_names,\n",
    "        labels,\n",
    "        topN=topN,\n",
    "        out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "        cmap=\"rocket\",\n",
    "        agg=agg_for_heatmap,\n",
    "    )\n",
    "\n",
    "    # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "    if save_representatives:\n",
    "        rep_dir = out_dir / \"representatives_dbscan\"\n",
    "        # mean/max/min\n",
    "        reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "        for rep in reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"mean\", \"max_idx\", \"min_idx\"]:\n",
    "                i = rep[tag]\n",
    "                name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                save_local_plots(\n",
    "                    label,\n",
    "                    expl,\n",
    "                    i,\n",
    "                    rep_dir / f\"cluster_{c}\",\n",
    "                    feat_names=feat_names,\n",
    "                    max_display=15,\n",
    "                    df_pred=df_pred,\n",
    "                    youden_thr=ythr,\n",
    "                    name_tag=name_tag,\n",
    "                )\n",
    "        # FP/FN\n",
    "        err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "        for rep in err_reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "                if tag in rep:\n",
    "                    i = rep[tag]\n",
    "                    name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                    save_local_plots(\n",
    "                        label,\n",
    "                        expl,\n",
    "                        i,\n",
    "                        rep_dir / f\"cluster_{c}\",\n",
    "                        feat_names=feat_names,\n",
    "                        max_display=15,\n",
    "                        df_pred=df_pred,\n",
    "                        youden_thr=ythr,\n",
    "                        name_tag=name_tag,\n",
    "                    )\n",
    "\n",
    "    # 9) 메타(JSON)\n",
    "    meta = dict(\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        algo=\"dbscan_tuned\",\n",
    "        random_state=random_state,\n",
    "        umap=dict(\n",
    "            n_components=umap_n_components,\n",
    "            n_neighbors=umap_n_neighbors,\n",
    "            min_dist=umap_min_dist,\n",
    "            metric=umap_metric,\n",
    "        ),\n",
    "        final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "        trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "        n_clusters=int(len(set(labels) - {-1})),\n",
    "        noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "        history=info_hist,\n",
    "    )\n",
    "    with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"-> saved: {out_dir}\")\n",
    "    return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82081588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============== 전체 라벨 반복 실행 ===============\n",
    "# all_cluster_summaries = []\n",
    "# for lb in LABELS:\n",
    "#     try:\n",
    "#         df_sum, labels = run_cluster_analysis_for_label(\n",
    "#             lb,\n",
    "#             umap_n_components=12,\n",
    "#             umap_n_neighbors=12,\n",
    "#             umap_min_dist=0.01,\n",
    "#             umap_metric=\"cosine\",\n",
    "#             min_samples=6,\n",
    "#             eps=None,\n",
    "#             dbscan_metric=\"euclidean\",\n",
    "#             target_min_clusters=2,\n",
    "#             max_noise_ratio=0.45,\n",
    "#             max_trials=5,\n",
    "#             topN=10,\n",
    "#             agg_for_heatmap=\"mean\",\n",
    "#             save_representatives=True,\n",
    "#             random_state=42,\n",
    "#         )\n",
    "#         df_sum.insert(0, \"label\", lb)\n",
    "#         all_cluster_summaries.append(df_sum)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] {lb}: {e}\")\n",
    "\n",
    "# if all_cluster_summaries:\n",
    "#     all_summ = pd.concat(all_cluster_summaries, ignore_index=True)\n",
    "#     out_csv = OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.csv\"\n",
    "#     all_summ.to_csv(out_csv, index=False)\n",
    "#     print(\"✅ saved:\", out_csv)\n",
    "#     # Excel(라벨별 시트) 옵션\n",
    "#     try:\n",
    "#         with pd.ExcelWriter(\n",
    "#             OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\"\n",
    "#         ) as xw:\n",
    "#             for lb, df_ in all_summ.groupby(\"label\"):\n",
    "#                 df_.to_excel(xw, sheet_name=lb, index=False)\n",
    "#         print(\"✅ saved:\", OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\")\n",
    "#     except Exception as e:\n",
    "#         print(\"[WARN] Excel save:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2589c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 경로 설정 ----------\n",
    "csv_path = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters/ALL_labels__cluster_summary_dbscan_tuned.csv\")\n",
    ")\n",
    "out_path = csv_path.with_name(\"Table3_Cluster_Summary.xlsx\")\n",
    "\n",
    "# ---------- 데이터 로드 ----------\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ---------- 전처리 ----------\n",
    "# (1) Δ 계산: 예측확률 - 실제 event rate 차이\n",
    "df[\"Delta(abs)\"] = (df[\"mean_y_prob\"] - df[\"event_rate\"]).abs().round(3)\n",
    "df[\"Direction\"] = df.apply(\n",
    "    lambda x: (\n",
    "        \"Over (Pred>Event)\"\n",
    "        if x[\"mean_y_prob\"] > x[\"event_rate\"]\n",
    "        else \"Under (Pred<Event)\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# (2) 도메인/해석 컬럼 자리 만들기 (수동 입력용)\n",
    "df[\"Dominant domain\"] = \"\"\n",
    "df[\"Key top features\"] = \"\"\n",
    "df[\"Interpretation summary\"] = \"\"\n",
    "\n",
    "# (3) 정렬 및 보기 좋은 라벨링\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"label\": \"Outcome (label)\",\n",
    "        \"cluster\": \"Cluster\",\n",
    "        \"n\": \"N\",\n",
    "        \"event_rate\": \"Event rate\",\n",
    "        \"mean_y_prob\": \"Mean predicted prob\",\n",
    "        \"p_event_rate(chi2)\": \"p (event rate, χ²)\",\n",
    "        \"p_yprob(kruskal)\": \"p (pred prob, KW)\",\n",
    "    }\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Outcome (label)\",\n",
    "        \"Cluster\",\n",
    "        \"N\",\n",
    "        \"Event rate\",\n",
    "        \"Mean predicted prob\",\n",
    "        \"Delta(abs)\",\n",
    "        \"Direction\",\n",
    "        \"Dominant domain\",\n",
    "        \"Key top features\",\n",
    "        \"Interpretation summary\",\n",
    "        \"p (event rate, χ²)\",\n",
    "        \"p (pred prob, KW)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# ---------- Excel 저장 ----------\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    for lb, dsub in df.groupby(\"Outcome (label)\"):\n",
    "        dsub.to_excel(writer, sheet_name=lb, index=False)\n",
    "    # 전체 통합 시트\n",
    "    df.to_excel(writer, sheet_name=\"All_labels_combined\", index=False)\n",
    "\n",
    "print(f\"✅ Saved Table 3 summary Excel → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125ad28",
   "metadata": {},
   "source": [
    "## ⭐️⭐️⭐️ version 3 ⭐️⭐️⭐️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8945a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import chi2_contingency, kruskal\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "from adjustText import adjust_text\n",
    "\n",
    "try:\n",
    "    import umap.umap_ as UMAP\n",
    "\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 경로/전역 ===============\n",
    "MODEL_DIR = str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    "OUT_ROOT = Path(MODEL_DIR) / \"figures/clinic_interpretation/PatientClusters\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ALIAS = {\n",
    "    \"Impaired_Social_Function\": \"Social Function Impairment (LLM)\",\n",
    "    \"Religious_Affiliation\": \"Religious Affiliation (LLM)\",\n",
    "    \"Violence_and_Impulsivity\": \"Aggression/Impulsivity (LLM)\",\n",
    "    \"Domestic_Violence\": \"Domestic Violence (LLM)\",\n",
    "    \"Physical_Abuse\": \"Physical Abuse (LLM)\",\n",
    "    \"Divorce\": \"Divorce Experience (LLM)\",\n",
    "    \"Death_of_Family_Member\": \"Family Loss (LLM)\",\n",
    "    \"Emotional_Abuse\": \"Emotional Abuse (LLM)\",\n",
    "    \"Lack_of_Family_Support\": \"Lack of Family Support (LLM)\",\n",
    "    \"Social_Isolation_and_Lack_of_Support\": \"Social Isolation (LLM)\",\n",
    "    \"Psychotic_Symptoms\": \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "    \"Interpersonal_Conflict\": \"Interpersonal Conflict (LLM)\",\n",
    "    \"Exposure_to_Suicide\": \"Suicide Exposure (LLM)\",\n",
    "    \"Alcohol_Use_Problems\": \"Alcohol Use Issues (LLM)\",\n",
    "    \"Sexual_Abuse\": \"Sexual Victimization (LLM)\",\n",
    "    \"Physical_and_Emotional_Neglect\": \"Neglect (LLM)\",\n",
    "}\n",
    "\n",
    "def harmonize_columns_for_pipeline(X, pipe):\n",
    "    \"\"\"\n",
    "    파이프라인의 전처리기가 기대하는 컬럼 구성과 순서로 데이터프레임을 재정렬하는 함수\n",
    "    \"\"\"\n",
    "    # 보통 파이프라인의 첫 번째 스텝(전처리기)에서 피처 이름을 가져옵니다.\n",
    "    try:\n",
    "        expected_cols = pipe.named_steps[\"preprocessor\"].feature_names_in_\n",
    "    except AttributeError:\n",
    "        # 피처 이름이 저장되지 않은 경우 등에 대한 예외 처리\n",
    "        return X\n",
    "\n",
    "    # 누락된 컬럼은 0으로 채우고, 순서를 맞춤\n",
    "    for col in expected_cols:\n",
    "        if col not in X.columns:\n",
    "            X[col] = 0\n",
    "\n",
    "    return X[expected_cols]\n",
    "\n",
    "def harmonize_with_alias(X, pipe, alias_map=LLM_ALIAS, fill_value=0):\n",
    "    X = X.copy()\n",
    "\n",
    "    # 1) alias 컬럼이 있으면, 파이프라인이 기대하는 \"원본키\" 컬럼로 복사 생성\n",
    "    for raw_key, pretty_name in alias_map.items():\n",
    "        if (raw_key not in X.columns) and (pretty_name in X.columns):\n",
    "            X[raw_key] = X[pretty_name]\n",
    "\n",
    "    # 2) 기존 harmonize (없는 컬럼 생성/정렬 등)\n",
    "    X = harmonize_columns_for_pipeline(X, pipe)\n",
    "\n",
    "    return X\n",
    "\n",
    "def shap_on_test(label: str, max_bg=200, force_tree_interventional=True):\n",
    "    pipe = load_best_pipeline(label)\n",
    "    preproc = pipe.named_steps[\"preprocessor\"]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "\n",
    "    # ✅ 이거 반드시 필요\n",
    "    X_raw, y_true, df_pred = load_train_test(label)\n",
    "\n",
    "    # ✅ alias -> raw_key 복구 후 harmonize\n",
    "    X_raw_h = harmonize_with_alias(X_raw, pipe)\n",
    "\n",
    "    X_te_t = preproc.transform(X_raw_h)\n",
    "    feat_names = load_feature_names_transformed(label)\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    bg_idx = rng.choice(X_te_t.shape[0], size=min(max_bg, X_te_t.shape[0]), replace=False)\n",
    "    X_bg_t = X_te_t[bg_idx]\n",
    "\n",
    "    try:\n",
    "        if force_tree_interventional:\n",
    "            explainer = shap.TreeExplainer(\n",
    "                clf,\n",
    "                data=X_bg_t,\n",
    "                feature_perturbation=\"interventional\",\n",
    "                model_output=\"probability\",\n",
    "            )\n",
    "        else:\n",
    "            explainer = shap.Explainer(clf, X_bg_t)\n",
    "        expl = explainer(X_te_t)\n",
    "    except Exception:\n",
    "        expl = shap.Explainer(clf, X_bg_t)(X_te_t)\n",
    "\n",
    "    vals = np.array(expl.values)\n",
    "    if vals.ndim == 3 and vals.shape[2] == 2:\n",
    "        expl = expl[:, :, 1]\n",
    "\n",
    "    try:\n",
    "        expl.feature_names = feat_names\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return expl, X_te_t, y_true, df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.nan_to_num(\n",
    "        X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "    )\n",
    "    return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "    plt.savefig(figpath, **savefig_kwargs)\n",
    "    try:\n",
    "        plt.savefig(\n",
    "            figpath.with_suffix(\".svg\"),\n",
    "            bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def reorder_labels_by_risk(labels: np.ndarray, y_true: np.ndarray):\n",
    "    \"\"\"\n",
    "    클러스터 라벨을 '실제 발생률(Event Rate)' 오름차순으로 재정렬합니다.\n",
    "    - Cluster 0: 발생률이 가장 낮은 그룹\n",
    "    - Cluster N: 발생률이 가장 높은 그룹\n",
    "    - Cluster -1: 노이즈 (변경 없음)\n",
    "    \"\"\"\n",
    "    # 노이즈 제외한 유니크 라벨\n",
    "    unique_labels = [c for c in np.unique(labels) if c != -1]\n",
    "\n",
    "    if not unique_labels:\n",
    "        return labels\n",
    "\n",
    "    # (구 라벨, Event Rate) 리스트 생성\n",
    "    risk_scores = []\n",
    "    print(\"\\n[Event Rate Reordering Check]\")\n",
    "    for c in unique_labels:\n",
    "        # 해당 클러스터의 실제 발생률 계산 (Mean of y_true)\n",
    "        # y_true가 0/1 이진값이라 가정하면 평균이 곧 발생률입니다.\n",
    "        event_rate = np.mean(y_true[labels == c])\n",
    "        risk_scores.append((c, event_rate))\n",
    "        print(f\" - Original Cluster {c}: Event Rate = {event_rate:.4f}\")\n",
    "\n",
    "    # Event Rate 기준 오름차순 정렬 (낮음 -> 높음)\n",
    "    risk_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 매핑 딕셔너리 생성\n",
    "    print(\"-> Mapping Result:\")\n",
    "    mapping = {-1: -1}\n",
    "    for new_idx, (old_label, rate) in enumerate(risk_scores):\n",
    "        mapping[old_label] = new_idx\n",
    "        print(f\"   Old {old_label} (Rate {rate:.4f}) -> New {new_idx} (Low to High)\")\n",
    "\n",
    "    # 새로운 라벨 적용\n",
    "    new_labels = np.array([mapping[x] for x in labels])\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# =============== Youden 임계값 ===============\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# =============== SHAP 로딩 ===============\n",
    "def get_shap_matrix(label: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      expl: shap.Explanation (n, p)\n",
    "      vals: ndarray (n, p)\n",
    "      feat_names: list[str] length p (cleaned)\n",
    "      y_prob: ndarray (n,)\n",
    "      y_true: ndarray (n,)\n",
    "      X_te_t: ndarray (n, p_after_preproc)\n",
    "      df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "    \"\"\"\n",
    "    # ⚠️ 기존 프로젝트의 함수 가정\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    vals = np.asarray(expl.values, dtype=float)\n",
    "    vals = np.nan_to_num(\n",
    "        vals,\n",
    "        nan=0.0,\n",
    "        posinf=np.finfo(float).max / 1e6,\n",
    "        neginf=-np.finfo(float).max / 1e6,\n",
    "    )\n",
    "\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        raw_names = list(expl.feature_names)\n",
    "    else:\n",
    "        raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "    feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "    y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "    y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "    return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# =============== k-distance & eps 추정 ===============\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps, kth\n",
    "\n",
    "\n",
    "def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "    plt.xlabel(\"Points (sorted)\")\n",
    "    plt.ylabel(f\"{k}-NN distance\")\n",
    "    plt.title(f\"k-distance curve (k={k})\")\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 히트맵 ===============\n",
    "def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "    return (\n",
    "        np.abs(values).mean(axis=0)\n",
    "        if how == \"mean\"\n",
    "        else np.mean(np.abs(values), axis=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cluster_heatmap(\n",
    "    vals,\n",
    "    feat_names,\n",
    "    labels,\n",
    "    topN,\n",
    "    out_path: Path,\n",
    "    cmap: str = \"rocket\",\n",
    "    agg: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      {\n",
    "        \"raw_all\":  DataFrame (clusters × all_features) : 클러스터별 원시 집계 |SHAP|\n",
    "        \"raw_top\":  DataFrame (clusters × topN)         : 히트맵에 사용된 TopN의 원시 |SHAP|\n",
    "        \"norm_top\": DataFrame (clusters × topN)         : TopN의 행 정규화(상대값)\n",
    "        \"feat_top\": List[str]                           : TopN 피처명\n",
    "        \"clusters\": np.ndarray                          : 사용된 클러스터 ID (노이즈 -1 제외)\n",
    "        \"fig_raw_path\": Path                            : raw heatmap PNG 경로\n",
    "        \"fig_norm_path\": Path                           : normalized heatmap PNG 경로(= out_path)\n",
    "      }\n",
    "    \"\"\"\n",
    "    from textwrap import fill\n",
    "\n",
    "    # 노이즈(-1) 제외\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "    if not clusters:\n",
    "        return None\n",
    "\n",
    "    # --- (A) 클러스터별 원시 집계 |SHAP|\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        rows.append(_agg_abs(vals[mask], how=agg))  # shape: (p,)\n",
    "    M = np.vstack(rows)  # (n_clusters, n_features)\n",
    "\n",
    "    # --- (B) 히트맵에서 쓸 TopN 선택 (클러스터 max 기준)\n",
    "    global_imp = _agg_abs(vals, how=agg)  # 전반적 중요도 (median|SHAP| 또는 mean|SHAP|)\n",
    "    hetero = M.max(axis=0) - M.min(axis=0)  # 간단한 이질성 점수 (max - min)\n",
    "\n",
    "    topK_imp = max(topN * 2, 30)  # 후보 풀(전반적 중요도 상위 K)\n",
    "    cand = np.argsort(global_imp)[::-1][:topK_imp]\n",
    "    top_idx = cand[\n",
    "        np.argsort(hetero[cand])[::-1][:topN]\n",
    "    ]  # 후보 안에서 이질성 큰 순 TopN\n",
    "\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    ###########################################\n",
    "    # global_imp = _agg_abs(vals, how=agg)  # shape (p,)  # median(|SHAP|)\n",
    "    # top_idx = np.argsort(global_imp)[::-1][:topN]\n",
    "\n",
    "    # M_top = M[:, top_idx]\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "    ###########################################\n",
    "    # top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    # M_top = M[:, top_idx]  # raw topN\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    # --- (C) 행 정규화(클러스터 내부 상대값)\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max  # normalized topN\n",
    "\n",
    "    # --- (D) DataFrame 구성\n",
    "    idx_names = [f\"cluster_{c}\" for c in clusters]\n",
    "    df_raw_all = pd.DataFrame(M, index=idx_names, columns=feat_names)\n",
    "    df_raw_top = pd.DataFrame(M_top, index=idx_names, columns=feat_top)\n",
    "    df_norm_top = pd.DataFrame(M_disp, index=idx_names, columns=feat_top)\n",
    "\n",
    "    # ---------- (E) Normalized Heatmap (기존 파일명 유지) ----------\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_norm_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",  #  – Row-normalized\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # ---------- (F) Raw Heatmap (새 파일) ----------\n",
    "    out_path_raw = out_path.with_name(out_path.stem + \"__RAW.png\")\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_raw_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": f\"{agg} |SHAP| (raw)\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|) – Raw\",\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path_raw, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return {\n",
    "        \"raw_all\": df_raw_all,\n",
    "        \"raw_top\": df_raw_top,\n",
    "        \"norm_top\": df_norm_top,\n",
    "        \"feat_top\": feat_top,\n",
    "        \"clusters\": np.array(clusters),\n",
    "        \"fig_raw_path\": out_path_raw,\n",
    "        \"fig_norm_path\": out_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============== 대표 케이스 ===============\n",
    "def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        probs = y_prob[idx]\n",
    "        med = np.mean(probs)\n",
    "        med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "        max_idx = idx[np.argmax(probs)]\n",
    "        min_idx = idx[np.argmin(probs)]\n",
    "        reps.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"mean_idx\": int(med_idx),\n",
    "                \"max_idx\": int(max_idx),\n",
    "                \"min_idx\": int(min_idx),\n",
    "            }\n",
    "        )\n",
    "    return reps\n",
    "\n",
    "\n",
    "def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        d = df_pred.iloc[idx]\n",
    "        rec = {\"cluster\": int(c)}\n",
    "        fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "        fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "        if len(fp):\n",
    "            rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "        if len(fn):\n",
    "            rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "        reps.append(rec)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def save_local_plots(\n",
    "    label: str,\n",
    "    expl: shap.Explanation,\n",
    "    idx: int,\n",
    "    out_dir: Path,\n",
    "    feat_names=None,\n",
    "    max_display=15,\n",
    "    df_pred: pd.DataFrame | None = None,\n",
    "    youden_thr: float | None = None,\n",
    "    name_tag: str | None = None,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    y_t = y_p = None\n",
    "    y_prob = None\n",
    "    thr_txt = \"\"\n",
    "    if df_pred is not None:\n",
    "        if \"y_true\" in df_pred.columns:\n",
    "            y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "        if \"y_prob\" in df_pred.columns:\n",
    "            y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "        if \"y_pred\" in df_pred.columns:\n",
    "            y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "        else:\n",
    "            thr = youden_thr if youden_thr is not None else 0.5\n",
    "            y_p = (\n",
    "                int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "                if \"y_prob\" in df_pred.columns\n",
    "                else None\n",
    "            )\n",
    "        if youden_thr is not None:\n",
    "            thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "    # waterfall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "    except Exception:\n",
    "        row = expl[idx]\n",
    "        shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "    title_bits = [f\"{label} | id={idx}\"]\n",
    "    if y_t is not None:\n",
    "        title_bits.append(f\"y_true={y_t}\")\n",
    "    if y_p is not None:\n",
    "        title_bits.append(f\"y_pred={y_p}\")\n",
    "    if y_prob is not None:\n",
    "        title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "    if name_tag:\n",
    "        title_bits.append(f\"[{name_tag}]\")\n",
    "    ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "    plt.title(ttl)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(\n",
    "        out_dir\n",
    "        / (\n",
    "            f\"{label}__waterfall_id{idx}\"\n",
    "            + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "            + \".png\"\n",
    "        ),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # force (옵션)\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 2.8))\n",
    "        shap.plots.force(\n",
    "            expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_png_svg(\n",
    "            out_dir\n",
    "            / (\n",
    "                f\"{label}__force_id{idx}\"\n",
    "                + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "                + \".png\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "    except Exception:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============== DBSCAN 통계/요약 ===============\n",
    "def add_dbscan_stats(\n",
    "    out_dir: Path,\n",
    "    label: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "):\n",
    "    lab = np.asarray(labels)\n",
    "    clusters = [c for c in np.unique(lab) if c != -1]\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        m = lab == c\n",
    "        rows.append(\n",
    "            dict(\n",
    "                cluster=int(c),\n",
    "                n=int(m.sum()),\n",
    "                event_rate=float(np.mean(y_true[m] == 1)),\n",
    "                mean_y_prob=float(np.mean(y_prob[m])),\n",
    "            )\n",
    "        )\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    chi_p = np.nan\n",
    "    kw_p = np.nan\n",
    "    if len(clusters) >= 2:\n",
    "        table = [\n",
    "            [\n",
    "                int(np.sum((lab == c) & (y_true == 1))),\n",
    "                int(np.sum((lab == c) & (y_true == 0))),\n",
    "            ]\n",
    "            for c in clusters\n",
    "        ]\n",
    "        chi_p = chi2_contingency(table)[1]\n",
    "        groups = [y_prob[lab == c] for c in clusters]\n",
    "        kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "    df[\"p_event_rate(chi2)\"] = chi_p\n",
    "    df[\"p_yprob(kruskal)\"] = kw_p\n",
    "    df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 메인 루틴 ===============\n",
    "def run_cluster_analysis_for_label(\n",
    "    label: str,\n",
    "    umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"euclidean\",  # cosine\n",
    "    min_samples: int = 6,  # DBSCAN\n",
    "    eps: float | None = None,\n",
    "    dbscan_metric: str = \"euclidean\",\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "    topN: int = 10,\n",
    "    agg_for_heatmap: str = \"mean\",\n",
    "    save_representatives: bool = True,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "    out_dir = OUT_ROOT / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 0) 데이터 로드\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # Youden 임계값 & df_pred 보정\n",
    "    ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "    if ythr is None:\n",
    "        ythr = _compute_youden_from_preds(\n",
    "            np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "        )\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "    # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "    # V = _standardize(vals) # 표준화\n",
    "    V = vals\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "    if eps is None:\n",
    "        eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.9)  # 0.85\n",
    "        eps = eps0\n",
    "        save_kdist_plot(\n",
    "            kth,\n",
    "            k=min_samples,\n",
    "            out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "        )\n",
    "\n",
    "    # 3) DBSCAN 적응형 루프\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "    cur_min_samples = int(min_samples)\n",
    "    cur_eps = float(eps)\n",
    "\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            cur_eps *= 1.20\n",
    "            if cur_min_samples > 3:\n",
    "                cur_min_samples -= 1\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            cur_eps *= 0.88\n",
    "        else:\n",
    "            cur_eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # all-noise 대비: 마지막 완화 시도 1회\n",
    "    if (labels is None) or np.all(labels == -1):\n",
    "        cur_eps *= 1.30\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=\"final_relax\",\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "    if umap_n_components == 2:\n",
    "        Z2 = X_hi\n",
    "    else:\n",
    "        um_2d = UMAP.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=max(umap_n_neighbors, 15),\n",
    "            min_dist=max(umap_min_dist, 0.05),\n",
    "            metric=umap_metric,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "    m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # 6) 2D 플롯 (클러스터 색)\n",
    "    uniq = sorted(np.unique(labels))\n",
    "    cmap = plt.get_cmap(\"tab10\")  # tab20\n",
    "    color_map = {c: (\"#bbbbbb\" if c == -1 else cmap(c % 20)) for c in uniq}\n",
    "    colors = [color_map[c] for c in labels]\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    plt.scatter(Z2[:, 0], Z2[:, 1], c=colors, s=18, alpha=0.9)\n",
    "    title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "    plt.title(f\"{title} — {len(set(labels)-{-1})} clusters, noise={(labels==-1).sum()}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 6-2) 2D 플롯 (위험도 색)\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 7) 클러스터 통계/요약 + 히트맵\n",
    "    df_stats = add_dbscan_stats(\n",
    "        out_dir,\n",
    "        label,\n",
    "        y_true=np.asarray(y_true),\n",
    "        y_prob=np.asarray(y_prob),\n",
    "        labels=np.asarray(labels),\n",
    "    )\n",
    "    ret = plot_cluster_heatmap(\n",
    "        vals,\n",
    "        feat_names,\n",
    "        labels,\n",
    "        topN=topN,\n",
    "        out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "        cmap=\"magma_r\",  # rocket_r, viridis_r\n",
    "        agg=agg_for_heatmap,\n",
    "    )\n",
    "\n",
    "    # === 수치 저장(요청 파일명 규칙) ===\n",
    "    if ret is not None:\n",
    "        # 모든 피처의 원시 |SHAP| (클러스터 × 전체 피처)\n",
    "        ret[\"raw_all\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__ALL_FEATURES.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 원시 |SHAP|\n",
    "        ret[\"raw_top\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_RAW.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 행 정규화(상대값)\n",
    "        ret[\"norm_top\"].to_csv(\n",
    "            out_dir\n",
    "            / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_ROWNORM.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "    # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "    if save_representatives:\n",
    "        rep_dir = out_dir / \"representatives_dbscan\"\n",
    "        # mean/max/min\n",
    "        reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "        for rep in reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"mean_idx\", \"max_idx\", \"min_idx\"]:\n",
    "                i = rep[tag]\n",
    "                name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                save_local_plots(\n",
    "                    label,\n",
    "                    expl,\n",
    "                    i,\n",
    "                    rep_dir / f\"cluster_{c}\",\n",
    "                    feat_names=feat_names,\n",
    "                    max_display=15,\n",
    "                    df_pred=df_pred,\n",
    "                    youden_thr=ythr,\n",
    "                    name_tag=name_tag,\n",
    "                )\n",
    "        # FP/FN\n",
    "        err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "        for rep in err_reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "                if tag in rep:\n",
    "                    i = rep[tag]\n",
    "                    name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                    save_local_plots(\n",
    "                        label,\n",
    "                        expl,\n",
    "                        i,\n",
    "                        rep_dir / f\"cluster_{c}\",\n",
    "                        feat_names=feat_names,\n",
    "                        max_display=15,\n",
    "                        df_pred=df_pred,\n",
    "                        youden_thr=ythr,\n",
    "                        name_tag=name_tag,\n",
    "                    )\n",
    "\n",
    "    # 9) 메타(JSON)\n",
    "    meta = dict(\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        algo=\"dbscan_tuned\",\n",
    "        random_state=random_state,\n",
    "        umap=dict(\n",
    "            n_components=umap_n_components,\n",
    "            n_neighbors=umap_n_neighbors,\n",
    "            min_dist=umap_min_dist,\n",
    "            metric=umap_metric,\n",
    "        ),\n",
    "        final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "        trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "        n_clusters=int(len(set(labels) - {-1})),\n",
    "        noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "        history=info_hist,\n",
    "    )\n",
    "    with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"-> saved: {out_dir}\")\n",
    "    return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_for_ppt(\n",
    "    Z2: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    y_true: np.ndarray,  # [NEW] 실제 정답 레이블 추가\n",
    "    out_path: Path,\n",
    "    title: str,\n",
    "):\n",
    "\n",
    "    # plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    # --- [설정] Risk 판단 기준 (Event Rate 기준) ---\n",
    "    def get_risk_style(event_rate):\n",
    "        # Event Rate는 보통 확률보다 낮으므로 기준을 조금 조정할 수도 있음 (현행 유지)\n",
    "        if event_rate < 0.1:\n",
    "            return \"Low\", \"#4575b4\"\n",
    "        elif event_rate < 0.3:\n",
    "            return \"Moderate\", \"#7b3294\"\n",
    "        elif event_rate < 0.5:\n",
    "            return \"High\", \"#f1a340\"\n",
    "        else:\n",
    "            return \"Very High\", \"#d73027\"\n",
    "\n",
    "    uniq = sorted(np.unique(labels))\n",
    "    clusters = [c for c in uniq if c != -1]\n",
    "    n_clusters = len(clusters)\n",
    "\n",
    "    global_center = np.mean(Z2, axis=0)\n",
    "    cluster_cmap = plt.get_cmap(\"tab20\" if n_clusters > 10 else \"tab10\")\n",
    "\n",
    "    centroids = {}\n",
    "    for c in clusters:\n",
    "        centroids[c] = np.mean(Z2[labels == c], axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.scatter(\n",
    "        Z2[:, 0], Z2[:, 1], c=\"#e0e0e0\", s=20, alpha=0.4, edgecolors=\"none\", zorder=0\n",
    "    )\n",
    "\n",
    "    label_items = []\n",
    "    legend_handles = []\n",
    "\n",
    "    for i, c in enumerate(clusters):\n",
    "        mask = labels == c\n",
    "        points = Z2[mask]\n",
    "\n",
    "        # [변경] 실제 Event Rate 계산\n",
    "        # y_true가 0/1로 되어 있다고 가정 (1=Event)\n",
    "        true_vals = y_true[mask]\n",
    "        event_rate = np.mean(true_vals == 1)\n",
    "\n",
    "        risk_name, risk_color = get_risk_style(event_rate)  # Event Rate 기준 스타일\n",
    "        cluster_color = cluster_cmap(i % 20)\n",
    "\n",
    "        handle = mpatches.Patch(color=cluster_color, label=f\"Cluster {c}\")\n",
    "        legend_handles.append(handle)\n",
    "\n",
    "        if len(points) < 3:\n",
    "            ax.scatter(\n",
    "                points[:, 0], points[:, 1], c=cluster_color, s=20, alpha=0.8, zorder=2\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        ax.scatter(\n",
    "            points[:, 0],\n",
    "            points[:, 1],\n",
    "            c=[cluster_color],\n",
    "            s=20,\n",
    "            alpha=0.8,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.2,\n",
    "            zorder=2,\n",
    "        )\n",
    "        hull = ConvexHull(points)\n",
    "        hull_coords = points[np.append(hull.vertices, hull.vertices[0])]\n",
    "        ax.plot(\n",
    "            hull_coords[:, 0],\n",
    "            hull_coords[:, 1],\n",
    "            color=cluster_color,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.2,\n",
    "            alpha=0.8,\n",
    "            zorder=3,\n",
    "        )\n",
    "        ax.add_patch(\n",
    "            mpatches.Polygon(\n",
    "                hull_coords,\n",
    "                closed=True,\n",
    "                fc=cluster_color,\n",
    "                ec=None,\n",
    "                alpha=0.08,\n",
    "                zorder=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- 위치 계산 ---\n",
    "        current_centroid = centroids[c]\n",
    "        vec_radial = current_centroid - global_center\n",
    "        vec_repulsion = np.array([0.0, 0.0])\n",
    "        crowding = 0\n",
    "\n",
    "        for other_c, other_cent in centroids.items():\n",
    "            if c == other_c:\n",
    "                continue\n",
    "            diff = current_centroid - other_cent\n",
    "            dist = np.linalg.norm(diff)\n",
    "            if dist > 1e-4:\n",
    "                w = 1.0 / (dist**2.0)\n",
    "                vec_repulsion += (diff / dist) * w\n",
    "                crowding += w\n",
    "\n",
    "        final_vec = vec_radial + (vec_repulsion * 4.0)\n",
    "        if crowding > 0.5 or np.linalg.norm(vec_radial) < 1.0:\n",
    "            final_vec[1] += (1.0 if i % 2 == 0 else -1.0) * 3.0\n",
    "\n",
    "        norm = np.linalg.norm(final_vec)\n",
    "        unit_vec = final_vec / norm if norm > 1e-4 else np.array([0, 1])\n",
    "\n",
    "        hull_points = points[hull.vertices]\n",
    "        anchor_point = hull_points[np.argmax(np.dot(hull_points, unit_vec))]\n",
    "\n",
    "        offset = 2.5 + min(crowding, 3.5)\n",
    "        text_pos = anchor_point + (unit_vec * offset)\n",
    "\n",
    "        label_items.append(\n",
    "            {\n",
    "                \"text_pos\": text_pos,\n",
    "                \"anchor_point\": anchor_point,\n",
    "                # [변경] 라벨 텍스트에 Event Rate 표시\n",
    "                \"text_str\": f\"{risk_name}\\n(Event:{event_rate:.2f})\",\n",
    "                \"color\": risk_color,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --- 물리 엔진 ---\n",
    "    iterations = 50\n",
    "    min_dist = 3.0\n",
    "    for _ in range(iterations):\n",
    "        for i in range(len(label_items)):\n",
    "            for j in range(i + 1, len(label_items)):\n",
    "                p1 = label_items[i][\"text_pos\"]\n",
    "                p2 = label_items[j][\"text_pos\"]\n",
    "                diff = p1 - p2\n",
    "                dist = np.linalg.norm(diff)\n",
    "                if dist < min_dist:\n",
    "                    if dist < 1e-4:\n",
    "                        force = np.array([0.1, 0.1])\n",
    "                    else:\n",
    "                        force = (diff / dist) * (min_dist - dist) * 0.5\n",
    "                    label_items[i][\"text_pos\"] += force\n",
    "                    label_items[j][\"text_pos\"] -= force\n",
    "\n",
    "    # --- 축 확장 ---\n",
    "    all_x = list(Z2[:, 0]) + [item[\"text_pos\"][0] for item in label_items]\n",
    "    all_y = list(Z2[:, 1]) + [item[\"text_pos\"][1] for item in label_items]\n",
    "    margin_x = (max(all_x) - min(all_x)) * 0.1\n",
    "    margin_y = (max(all_y) - min(all_y)) * 0.1\n",
    "    ax.set_xlim(min(all_x) - margin_x, max(all_x) + margin_x)\n",
    "    ax.set_ylim(min(all_y) - margin_y, max(all_y) + margin_y)\n",
    "\n",
    "    # # --- Annotation 그리기 ---\n",
    "    # for item in label_items:\n",
    "    #     bbox_props = dict(\n",
    "    #         boxstyle=\"round,pad=0.2\", fc=\"white\", ec=item[\"color\"], lw=1.5, alpha=0.95\n",
    "    #     )\n",
    "\n",
    "    #     ax.annotate(\n",
    "    #         text=item[\"text_str\"],\n",
    "    #         xy=(item[\"anchor_point\"][0], item[\"anchor_point\"][1]),\n",
    "    #         xytext=(item[\"text_pos\"][0], item[\"text_pos\"][1]),\n",
    "    #         ha=\"center\",\n",
    "    #         va=\"center\",\n",
    "    #         fontsize=14,\n",
    "    #         weight=\"bold\",\n",
    "    #         color=item[\"color\"],\n",
    "    #         bbox=bbox_props,\n",
    "    #         arrowprops=dict(\n",
    "    #             arrowstyle=\"-\",\n",
    "    #             color=\"black\",\n",
    "    #             linewidth=0.8,\n",
    "    #             shrinkB=4,\n",
    "    #             connectionstyle=\"arc3,rad=0.1\",\n",
    "    #         ),\n",
    "    #         zorder=10,\n",
    "    #     )\n",
    "\n",
    "    # 범례 및 타이틀\n",
    "    if -1 in uniq:\n",
    "        legend_handles.insert(0, mpatches.Patch(color=\"#e0e0e0\", label=\"Noise\"))\n",
    "\n",
    "    ax.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Clusters\",\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=\"upper left\",\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    noise_pct = (labels == -1).sum() / len(labels) if len(labels) > 0 else 0\n",
    "    full_title = f\"{title}\\n(Noise ratio={noise_pct:.1%})\"\n",
    "\n",
    "    ax.set_title(full_title, fontsize=14, weight=\"bold\", pad=20)\n",
    "    ax.set_xlabel(\"UMAP-1\", fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel(\"UMAP-2\", fontsize=12, labelpad=10)\n",
    "    # ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 유틸 ===============\n",
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.nan_to_num(\n",
    "        X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "    )\n",
    "    return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "    plt.savefig(figpath, **savefig_kwargs)\n",
    "    try:\n",
    "        plt.savefig(\n",
    "            figpath.with_suffix(\".svg\"),\n",
    "            bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============== Youden 임계값 ===============\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# =============== SHAP 로딩 ===============\n",
    "def get_shap_matrix(label: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      expl: shap.Explanation (n, p)\n",
    "      vals: ndarray (n, p)\n",
    "      feat_names: list[str] length p (cleaned)\n",
    "      y_prob: ndarray (n,)\n",
    "      y_true: ndarray (n,)\n",
    "      X_te_t: ndarray (n, p_after_preproc)\n",
    "      df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "    \"\"\"\n",
    "    # ⚠️ 기존 프로젝트의 함수 가정\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    vals = np.asarray(expl.values, dtype=float)\n",
    "    vals = np.nan_to_num(\n",
    "        vals,\n",
    "        nan=0.0,\n",
    "        posinf=np.finfo(float).max / 1e6,\n",
    "        neginf=-np.finfo(float).max / 1e6,\n",
    "    )\n",
    "\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        raw_names = list(expl.feature_names)\n",
    "    else:\n",
    "        raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "    feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "    y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "    y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "    return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# =============== k-distance & eps 추정 ===============\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps, kth\n",
    "\n",
    "\n",
    "def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "    plt.xlabel(\"Points (sorted)\")\n",
    "    plt.ylabel(f\"{k}-NN distance\")\n",
    "    plt.title(f\"k-distance curve (k={k})\")\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 히트맵 ===============\n",
    "def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "    return (\n",
    "        np.abs(values).mean(axis=0)\n",
    "        if how == \"mean\"\n",
    "        else np.mean(np.abs(values), axis=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cluster_heatmap(\n",
    "    vals,\n",
    "    feat_names,\n",
    "    labels,\n",
    "    topN,\n",
    "    out_path: Path,\n",
    "    cmap: str = \"rocket\",\n",
    "    agg: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      {\n",
    "        \"raw_all\":  DataFrame (clusters × all_features) : 클러스터별 원시 집계 |SHAP|\n",
    "        \"raw_top\":  DataFrame (clusters × topN)         : 히트맵에 사용된 TopN의 원시 |SHAP|\n",
    "        \"norm_top\": DataFrame (clusters × topN)         : TopN의 행 정규화(상대값)\n",
    "        \"feat_top\": List[str]                           : TopN 피처명\n",
    "        \"clusters\": np.ndarray                          : 사용된 클러스터 ID (노이즈 -1 제외)\n",
    "        \"fig_raw_path\": Path                            : raw heatmap PNG 경로\n",
    "        \"fig_norm_path\": Path                           : normalized heatmap PNG 경로(= out_path)\n",
    "      }\n",
    "    \"\"\"\n",
    "    from textwrap import fill\n",
    "\n",
    "    # 노이즈(-1) 제외\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "    if not clusters:\n",
    "        return None\n",
    "\n",
    "    # --- (A) 클러스터별 원시 집계 |SHAP|\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        rows.append(_agg_abs(vals[mask], how=agg))  # shape: (p,)\n",
    "    M = np.vstack(rows)  # (n_clusters, n_features)\n",
    "\n",
    "    # --- (B) 히트맵에서 쓸 TopN 선택 (클러스터 max 기준)\n",
    "    global_imp = _agg_abs(vals, how=agg)  # 전반적 중요도 (median|SHAP| 또는 mean|SHAP|)\n",
    "    hetero = M.max(axis=0) - M.min(axis=0)  # 간단한 이질성 점수 (max - min)\n",
    "\n",
    "    topK_imp = max(topN * 2, 30)  # 후보 풀(전반적 중요도 상위 K)\n",
    "    cand = np.argsort(global_imp)[::-1][:topK_imp]\n",
    "    top_idx = cand[\n",
    "        np.argsort(hetero[cand])[::-1][:topN]\n",
    "    ]  # 후보 안에서 이질성 큰 순 TopN\n",
    "\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    ###########################################\n",
    "    # global_imp = _agg_abs(vals, how=agg)  # shape (p,)  # median(|SHAP|)\n",
    "    # top_idx = np.argsort(global_imp)[::-1][:topN]\n",
    "\n",
    "    # M_top = M[:, top_idx]\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "    ###########################################\n",
    "    # top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    # M_top = M[:, top_idx]  # raw topN\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    # --- (C) 행 정규화(클러스터 내부 상대값)\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max  # normalized topN\n",
    "\n",
    "    # --- (D) DataFrame 구성\n",
    "    idx_names = [f\"cluster_{c}\" for c in clusters]\n",
    "    df_raw_all = pd.DataFrame(M, index=idx_names, columns=feat_names)\n",
    "    df_raw_top = pd.DataFrame(M_top, index=idx_names, columns=feat_top)\n",
    "    df_norm_top = pd.DataFrame(M_disp, index=idx_names, columns=feat_top)\n",
    "\n",
    "    # ---------- (E) Normalized Heatmap (기존 파일명 유지) ----------\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_norm_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",  #  – Row-normalized\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # ---------- (F) Raw Heatmap (새 파일) ----------\n",
    "    out_path_raw = out_path.with_name(out_path.stem + \"__RAW.png\")\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_raw_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": f\"{agg} |SHAP|\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path_raw, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return {\n",
    "        \"raw_all\": df_raw_all,\n",
    "        \"raw_top\": df_raw_top,\n",
    "        \"norm_top\": df_norm_top,\n",
    "        \"feat_top\": feat_top,\n",
    "        \"clusters\": np.array(clusters),\n",
    "        \"fig_raw_path\": out_path_raw,\n",
    "        \"fig_norm_path\": out_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============== 대표 케이스 ===============\n",
    "def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        probs = y_prob[idx]\n",
    "        med = np.mean(probs)\n",
    "        med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "        max_idx = idx[np.argmax(probs)]\n",
    "        min_idx = idx[np.argmin(probs)]\n",
    "        reps.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"mean_idx\": int(med_idx),\n",
    "                \"max_idx\": int(max_idx),\n",
    "                \"min_idx\": int(min_idx),\n",
    "            }\n",
    "        )\n",
    "    return reps\n",
    "\n",
    "\n",
    "def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        d = df_pred.iloc[idx]\n",
    "        rec = {\"cluster\": int(c)}\n",
    "        fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "        fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "        if len(fp):\n",
    "            rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "        if len(fn):\n",
    "            rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "        reps.append(rec)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def save_local_plots(\n",
    "    label: str,\n",
    "    expl: shap.Explanation,\n",
    "    idx: int,\n",
    "    out_dir: Path,\n",
    "    feat_names=None,\n",
    "    max_display=15,\n",
    "    df_pred: pd.DataFrame | None = None,\n",
    "    youden_thr: float | None = None,\n",
    "    name_tag: str | None = None,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    y_t = y_p = None\n",
    "    y_prob = None\n",
    "    thr_txt = \"\"\n",
    "    if df_pred is not None:\n",
    "        if \"y_true\" in df_pred.columns:\n",
    "            y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "        if \"y_prob\" in df_pred.columns:\n",
    "            y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "        if \"y_pred\" in df_pred.columns:\n",
    "            y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "        else:\n",
    "            thr = youden_thr if youden_thr is not None else 0.5\n",
    "            y_p = (\n",
    "                int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "                if \"y_prob\" in df_pred.columns\n",
    "                else None\n",
    "            )\n",
    "        if youden_thr is not None:\n",
    "            thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "    # waterfall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "    except Exception:\n",
    "        row = expl[idx]\n",
    "        shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "    title_bits = [f\"{label} | id={idx}\"]\n",
    "    if y_t is not None:\n",
    "        title_bits.append(f\"y_true={y_t}\")\n",
    "    if y_p is not None:\n",
    "        title_bits.append(f\"y_pred={y_p}\")\n",
    "    if y_prob is not None:\n",
    "        title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "    if name_tag:\n",
    "        title_bits.append(f\"[{name_tag}]\")\n",
    "    ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "    plt.title(ttl)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(\n",
    "        out_dir\n",
    "        / (\n",
    "            f\"{label}__waterfall_id{idx}\"\n",
    "            + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "            + \".png\"\n",
    "        ),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # force (옵션)\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 2.8))\n",
    "        shap.plots.force(\n",
    "            expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_png_svg(\n",
    "            out_dir\n",
    "            / (\n",
    "                f\"{label}__force_id{idx}\"\n",
    "                + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "                + \".png\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "    except Exception:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============== DBSCAN 통계/요약 ===============\n",
    "def add_dbscan_stats(\n",
    "    out_dir: Path,\n",
    "    label: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "):\n",
    "    lab = np.asarray(labels)\n",
    "    clusters = [c for c in np.unique(lab) if c != -1]\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        m = lab == c\n",
    "        rows.append(\n",
    "            dict(\n",
    "                cluster=int(c),\n",
    "                n=int(m.sum()),\n",
    "                event_rate=float(np.mean(y_true[m] == 1)),\n",
    "                mean_y_prob=float(np.mean(y_prob[m])),\n",
    "            )\n",
    "        )\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    chi_p = np.nan\n",
    "    kw_p = np.nan\n",
    "    if len(clusters) >= 2:\n",
    "        table = [\n",
    "            [\n",
    "                int(np.sum((lab == c) & (y_true == 1))),\n",
    "                int(np.sum((lab == c) & (y_true == 0))),\n",
    "            ]\n",
    "            for c in clusters\n",
    "        ]\n",
    "        chi_p = chi2_contingency(table)[1]\n",
    "        groups = [y_prob[lab == c] for c in clusters]\n",
    "        kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "    df[\"p_event_rate(chi2)\"] = chi_p\n",
    "    df[\"p_yprob(kruskal)\"] = kw_p\n",
    "    df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 메인 루틴 ===============\n",
    "\n",
    "\n",
    "def run_cluster_analysis_for_label(\n",
    "    label: str,\n",
    "    umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"euclidean\",  # cosine\n",
    "    min_samples: int = 6,  # DBSCAN\n",
    "    eps: float | None = None,\n",
    "    dbscan_metric: str = \"euclidean\",\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "    topN: int = 10,\n",
    "    agg_for_heatmap: str = \"mean\",\n",
    "    save_representatives: bool = True,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "    out_dir = OUT_ROOT / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 0) 데이터 로드\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # Youden 임계값 & df_pred 보정\n",
    "    ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "    if ythr is None:\n",
    "        ythr = _compute_youden_from_preds(\n",
    "            np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "        )\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "    # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "    # V = _standardize(vals) # 표준화\n",
    "    V = vals\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "    if eps is None:\n",
    "        eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.9)  # 0.85\n",
    "        eps = eps0\n",
    "        save_kdist_plot(\n",
    "            kth,\n",
    "            k=min_samples,\n",
    "            out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "        )\n",
    "\n",
    "    # 3) DBSCAN 적응형 루프\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "    cur_min_samples = int(min_samples)\n",
    "    cur_eps = float(eps)\n",
    "\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            cur_eps *= 1.20\n",
    "            if cur_min_samples > 3:\n",
    "                cur_min_samples -= 1\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            cur_eps *= 0.88\n",
    "        else:\n",
    "            cur_eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # all-noise 대비: 마지막 완화 시도 1회\n",
    "    if (labels is None) or np.all(labels == -1):\n",
    "        cur_eps *= 1.30\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=\"final_relax\",\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "    if umap_n_components == 2:\n",
    "        Z2 = X_hi\n",
    "    else:\n",
    "        um_2d = UMAP.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=max(umap_n_neighbors, 15),\n",
    "            min_dist=max(umap_min_dist, 0.05),\n",
    "            metric=umap_metric,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    # [NEW] 위험도 순으로 라벨 재정렬 (이 부분 추가!)\n",
    "    # Cluster 0 = Low Risk, Cluster N = High Risk\n",
    "    if labels is not None:\n",
    "        print(\"-> Reordering clusters by risk (0: Low -> N: High)...\")\n",
    "        labels = reorder_labels_by_risk(labels, np.asarray(y_true))\n",
    "\n",
    "    # 5) 라벨 매핑 저장 (이제 정렬된 labels가 저장됨)\n",
    "    m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "    # m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    # m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # # ⭐️⭐️⭐️⭐️기존 코드 6) 2D 플롯 (클러스터 색) ⭐️⭐️⭐️⭐️\n",
    "    # uniq = sorted(np.unique(labels))\n",
    "    # # 클러스터 개수가 많을 수 있으므로 tab10 사용 권장\n",
    "    # cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "    # # 범례 공간 확보를 위해 가로 사이즈를 약간 늘림\n",
    "    # plt.figure(figsize=(8.5, 6.0))\n",
    "\n",
    "    # # 1. 노이즈(-1) 먼저 그리기 (배경으로 깔리게 처리)\n",
    "    # if -1 in uniq:\n",
    "    #     mask = labels == -1\n",
    "    #     plt.scatter(\n",
    "    #         Z2[mask, 0],\n",
    "    #         Z2[mask, 1],\n",
    "    #         c=\"#e0e0e0\",  # 아주 연한 회색\n",
    "    #         edgecolor=\"#bbbbbb\",  # 테두리는 조금 진하게\n",
    "    #         linewidth=0.1,\n",
    "    #         s=15,  # 크기는 작게\n",
    "    #         alpha=0.4,  # 투명하게\n",
    "    #         label=\"Noise\",  # 범례 이름\n",
    "    #         zorder=1,  # 가장 뒤쪽에 배치\n",
    "    #     )\n",
    "\n",
    "    # # 2. 실제 클러스터 그리기 Loop\n",
    "    # for c in uniq:\n",
    "    #     if c == -1:\n",
    "    #         continue\n",
    "    #     mask = labels == c\n",
    "    #     color = cmap(c % 20)  # 색상 순환 적용\n",
    "\n",
    "    #     plt.scatter(\n",
    "    #         Z2[mask, 0],\n",
    "    #         Z2[mask, 1],\n",
    "    #         c=[color],  # 단일 색상 적용\n",
    "    #         s=22,  # 클러스터 포인트는 조금 더 크게\n",
    "    #         alpha=0.9,  # 진하게\n",
    "    #         label=f\"Cluster {c}\",\n",
    "    #         edgecolor=\"white\",  # 포인트 구분감\n",
    "    #         linewidth=0.3,\n",
    "    #         zorder=2,  # 노이즈보다 앞쪽에 배치\n",
    "    #     )\n",
    "\n",
    "    # title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "    # # 제목에 전체 대비 노이즈 비율 표기\n",
    "    # n_noise = np.sum(labels == -1)\n",
    "    # n_total = len(labels)\n",
    "    # plt.title(\n",
    "    #     f\"{title}\\nTotal: {n_total}, Clusters: {len(set(labels)-{-1})}, Noise: {n_noise} ({n_noise/n_total:.1%})\",\n",
    "    #     fontsize=11,\n",
    "    # )\n",
    "    # plt.xlabel(\"UMAP-1\")\n",
    "    # plt.ylabel(\"UMAP-2\")\n",
    "\n",
    "    # # 범례 설정 (그래프 영역 밖 우측 상단에 배치)\n",
    "    # plt.legend(\n",
    "    #     bbox_to_anchor=(1.02, 1),\n",
    "    #     loc=\"upper left\",\n",
    "    #     borderaxespad=0,\n",
    "    #     frameon=False,\n",
    "    #     fontsize=9,\n",
    "    #     markerscale=1.5,  # 범례의 점 크기 키우기\n",
    "    # )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # save_png_svg(\n",
    "    #     out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    # )\n",
    "\n",
    "    # 6) 2D 플롯 (Hull + Annotation 적용 버전)\n",
    "    # print(\"-> Plotting UMAP with Threshold-based Risk Labels...\")\n",
    "    # plot_umap_with_hulls_and_labels(\n",
    "    #     Z2=Z2,\n",
    "    #     labels=labels,\n",
    "    #     y_prob=y_prob,  # [중요] 여기에 y_prob를 반드시 넘겨주세요!\n",
    "    #     out_path=out_dir / f\"{label}__UMAP2D_DBSCAN_annotated.png\",\n",
    "    #     title=f\"Risk Patterns ({label})\",\n",
    "    # )\n",
    "\n",
    "    print(\"-> Plotting UMAP for PPT (Event Rate based)...\")\n",
    "    plot_umap_for_ppt(\n",
    "        Z2=Z2,\n",
    "        labels=labels,\n",
    "        y_prob=y_prob,  # 기존 인자\n",
    "        y_true=y_true,  # [NEW] 실제 정답값 전달\n",
    "        out_path=out_dir / f\"{label}__UMAP2D_DBSCAN_.png\", # annotated_ppt\n",
    "        title=f\"Risk Patterns ({label})\",\n",
    "    )\n",
    "\n",
    "    # 6-2) 2D 플롯 (위험도 색)\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 7) 클러스터 통계/요약 + 히트맵\n",
    "    df_stats = add_dbscan_stats(\n",
    "        out_dir,\n",
    "        label,\n",
    "        y_true=np.asarray(y_true),\n",
    "        y_prob=np.asarray(y_prob),\n",
    "        labels=np.asarray(labels),\n",
    "    )\n",
    "    ret = plot_cluster_heatmap(\n",
    "        vals,\n",
    "        feat_names,\n",
    "        labels,\n",
    "        topN=topN,\n",
    "        out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "        cmap=\"magma_r\",  # rocket_r, viridis_r\n",
    "        agg=agg_for_heatmap,\n",
    "    )\n",
    "\n",
    "    # === 수치 저장(요청 파일명 규칙) ===\n",
    "    if ret is not None:\n",
    "        # 모든 피처의 원시 |SHAP| (클러스터 × 전체 피처)\n",
    "        ret[\"raw_all\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__ALL_FEATURES.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 원시 |SHAP|\n",
    "        ret[\"raw_top\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_RAW.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 행 정규화(상대값)\n",
    "        ret[\"norm_top\"].to_csv(\n",
    "            out_dir\n",
    "            / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_ROWNORM.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "    # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "    if save_representatives:\n",
    "        rep_dir = out_dir / \"representatives_dbscan\"\n",
    "        # mean/max/min\n",
    "        reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "        for rep in reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"mean_idx\", \"max_idx\", \"min_idx\"]:\n",
    "                i = rep[tag]\n",
    "                name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                save_local_plots(\n",
    "                    label,\n",
    "                    expl,\n",
    "                    i,\n",
    "                    rep_dir / f\"cluster_{c}\",\n",
    "                    feat_names=feat_names,\n",
    "                    max_display=15,\n",
    "                    df_pred=df_pred,\n",
    "                    youden_thr=ythr,\n",
    "                    name_tag=name_tag,\n",
    "                )\n",
    "        # FP/FN\n",
    "        err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "        for rep in err_reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "                if tag in rep:\n",
    "                    i = rep[tag]\n",
    "                    name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                    save_local_plots(\n",
    "                        label,\n",
    "                        expl,\n",
    "                        i,\n",
    "                        rep_dir / f\"cluster_{c}\",\n",
    "                        feat_names=feat_names,\n",
    "                        max_display=15,\n",
    "                        df_pred=df_pred,\n",
    "                        youden_thr=ythr,\n",
    "                        name_tag=name_tag,\n",
    "                    )\n",
    "\n",
    "    # 9) 메타(JSON)\n",
    "    meta = dict(\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        algo=\"dbscan_tuned\",\n",
    "        random_state=random_state,\n",
    "        umap=dict(\n",
    "            n_components=umap_n_components,\n",
    "            n_neighbors=umap_n_neighbors,\n",
    "            min_dist=umap_min_dist,\n",
    "            metric=umap_metric,\n",
    "        ),\n",
    "        final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "        trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "        n_clusters=int(len(set(labels) - {-1})),\n",
    "        noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "        history=info_hist,\n",
    "    )\n",
    "    with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"-> saved: {out_dir}\")\n",
    "    return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 전체 라벨 반복 실행 ===============\n",
    "all_cluster_summaries = []\n",
    "for lb in LABELS:\n",
    "    try:\n",
    "        df_sum, labels = run_cluster_analysis_for_label(\n",
    "            lb,\n",
    "            umap_n_components=2,\n",
    "            umap_n_neighbors=45,  # 40\n",
    "            umap_min_dist=0.0,  # 0.05, 0.01\n",
    "            umap_metric=\"euclidean\",  # cosine\n",
    "            min_samples=17,  # 20\n",
    "            eps=None,\n",
    "            dbscan_metric=\"euclidean\",\n",
    "            target_min_clusters=3,\n",
    "            max_noise_ratio=0.45,  # 0.45\n",
    "            max_trials=5,\n",
    "            topN=10,\n",
    "            agg_for_heatmap=\"mean\",\n",
    "            save_representatives=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "        df_sum.insert(0, \"label\", lb)\n",
    "        all_cluster_summaries.append(df_sum)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {lb}: {e}\")\n",
    "\n",
    "if all_cluster_summaries:\n",
    "    all_summ = pd.concat(all_cluster_summaries, ignore_index=True)\n",
    "    out_csv = OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.csv\"\n",
    "    all_summ.to_csv(out_csv, index=False)\n",
    "    print(\"✅ saved:\", out_csv)\n",
    "    # Excel(라벨별 시트) 옵션\n",
    "    try:\n",
    "        with pd.ExcelWriter(\n",
    "            OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\"\n",
    "        ) as xw:\n",
    "            for lb, df_ in all_summ.groupby(\"label\"):\n",
    "                df_.to_excel(xw, sheet_name=lb, index=False)\n",
    "        print(\"✅ saved:\", OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Excel save:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 경로 설정 ----------\n",
    "csv_path = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters/ALL_labels__cluster_summary_dbscan_tuned.csv\")\n",
    ")\n",
    "out_path = csv_path.with_name(\"Table3_Cluster_Summary.xlsx\")\n",
    "\n",
    "# ---------- 데이터 로드 ----------\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ---------- 전처리 ----------\n",
    "# (1) Δ 계산: 예측확률 - 실제 event rate 차이\n",
    "df[\"Delta(abs)\"] = (df[\"mean_y_prob\"] - df[\"event_rate\"]).abs().round(3)\n",
    "df[\"Direction\"] = df.apply(\n",
    "    lambda x: (\n",
    "        \"Over (Pred>Event)\"\n",
    "        if x[\"mean_y_prob\"] > x[\"event_rate\"]\n",
    "        else \"Under (Pred<Event)\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# (2) 도메인/해석 컬럼 자리 만들기 (수동 입력용)\n",
    "df[\"Dominant domain\"] = \"\"\n",
    "df[\"Key top features\"] = \"\"\n",
    "df[\"Interpretation summary\"] = \"\"\n",
    "\n",
    "# (3) 정렬 및 보기 좋은 라벨링\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"label\": \"Outcome (label)\",\n",
    "        \"cluster\": \"Cluster\",\n",
    "        \"n\": \"N\",\n",
    "        \"event_rate\": \"Event rate\",\n",
    "        \"mean_y_prob\": \"Mean predicted prob\",\n",
    "        \"p_event_rate(chi2)\": \"p (event rate, χ²)\",\n",
    "        \"p_yprob(kruskal)\": \"p (pred prob, KW)\",\n",
    "    }\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Outcome (label)\",\n",
    "        \"Cluster\",\n",
    "        \"N\",\n",
    "        \"Event rate\",\n",
    "        \"Mean predicted prob\",\n",
    "        \"Delta(abs)\",\n",
    "        \"Direction\",\n",
    "        \"Dominant domain\",\n",
    "        \"Key top features\",\n",
    "        \"Interpretation summary\",\n",
    "        \"p (event rate, χ²)\",\n",
    "        \"p (pred prob, KW)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# ---------- Excel 저장 ----------\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    for lb, dsub in df.groupby(\"Outcome (label)\"):\n",
    "        dsub.to_excel(writer, sheet_name=lb, index=False)\n",
    "    # 전체 통합 시트\n",
    "    df.to_excel(writer, sheet_name=\"All_labels_combined\", index=False)\n",
    "\n",
    "print(f\"✅ Saved Table 3 summary Excel → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e281791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 경로 설정 =====\n",
    "base = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "\n",
    "\n",
    "# ===== Domain 분류 함수 =====\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = feat.lower()\n",
    "    if any(k in t for k in [\"aggression\", \"support\", \"social\", \"interpersonal\", \"llm\"]):\n",
    "        return \"SDoMH\"\n",
    "    if any(k in t for k in [\"er visit\", \"≥2 er\", \"≥3 er\", \"admission\", \"admit\"]):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"k(\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"alc\",\n",
    "            \"lymphocyte\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"hcl\",\n",
    "            \"asrs\",\n",
    "            \"ham\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"treatment\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# ===== 초기화 =====\n",
    "labels = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "records_summary = []  # Table 4\n",
    "etable_dict = {}  # eTable S4 outcome별 sheet 저장\n",
    "\n",
    "# ===== outcome별 반복 =====\n",
    "for lb in labels:\n",
    "    fpath = base / lb / f\"{lb}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    print(\"using:\", fpath)\n",
    "    df = pd.read_csv(fpath, index_col=0)\n",
    "\n",
    "    # --- eTable S4용 wide-format 저장\n",
    "    df_wide = df.copy()\n",
    "    df_wide[\"Feature\"] = df_wide.index\n",
    "    etable_dict[lb] = df_wide.T  # transpose해서 Feature가 column으로 가도록 저장\n",
    "\n",
    "    # --- Table 4용 계산\n",
    "    for col in df.columns:\n",
    "        vals = df[col].values\n",
    "        # Kruskal–Wallis 검정 (클러스터 간 이질성 유의성)\n",
    "        try:\n",
    "            stat, pval = kruskal(*[df[col][df.index == c] for c in df.index])\n",
    "        except Exception:\n",
    "            pval = None\n",
    "\n",
    "        records_summary.append(\n",
    "            {\n",
    "                \"Outcome\": lb,\n",
    "                \"Feature\": col,\n",
    "                \"Mean(|SHAP|)\": vals.mean(),\n",
    "                \"SD\": vals.std(),\n",
    "                \"Range\": vals.max() - vals.min(),\n",
    "                \"Δ(Top−Bottom)\": vals.max() - vals.min(),\n",
    "                \"Max cluster\": df.index[vals.argmax()],\n",
    "                \"Min cluster\": df.index[vals.argmin()],\n",
    "                \"Top vs Bottom\": f\"{df.index[vals.argmax()]} > {df.index[vals.argmin()]}\",\n",
    "                \"p(KW)\": pval,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ===== Table 4 (요약) =====\n",
    "df_table4 = pd.DataFrame(records_summary)\n",
    "df_table4[\"Domain\"] = df_table4[\"Feature\"].apply(infer_domain)\n",
    "df_table4 = df_table4.sort_values([\"Outcome\", \"SD\"], ascending=[True, False])\n",
    "\n",
    "# ===== 저장 =====\n",
    "out_summary = base / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "out_wide = base / \"eTableS4_SHAP_ClusterMatrix.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(out_summary) as writer:\n",
    "    df_table4.to_excel(writer, sheet_name=\"All_outcomes\", index=False)\n",
    "    for lb, sub in df_table4.groupby(\"Outcome\"):\n",
    "        sub.to_excel(writer, sheet_name=lb, index=False)\n",
    "\n",
    "with pd.ExcelWriter(out_wide) as writer:\n",
    "    for lb, dfw in etable_dict.items():\n",
    "        dfw.to_excel(writer, sheet_name=lb)\n",
    "\n",
    "print(f\"✅ Saved Table 4 summary → {out_summary}\")\n",
    "print(f\"✅ Saved eTable S4 matrix → {out_wide}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 경로 =====\n",
    "base = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "\n",
    "# Table4 요약 엑셀\n",
    "f_table4 = base / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "df = pd.read_excel(f_table4, sheet_name=\"All_outcomes\")\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# ===== (A) Feature별 Boxplot (x=cluster, y=mean |SHAP|) =====\n",
    "for outcome in [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]:\n",
    "    target_outcome = outcome\n",
    "    fpath = (\n",
    "        base / target_outcome / f\"{target_outcome}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    )\n",
    "    df_feat = pd.read_csv(fpath, index_col=0)\n",
    "    df_feat.index = [\n",
    "        c.replace(\"cluster_\", \"C\") for c in df_feat.index\n",
    "    ]  # cluster_0 → C0\n",
    "\n",
    "    # ===== (1) Dotplot (by Feature 색상) =====\n",
    "    df_melt = df_feat.T.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Cluster\", value_name=\"Mean |SHAP|\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.stripplot(\n",
    "        x=\"Cluster\",\n",
    "        y=\"Mean |SHAP|\",\n",
    "        hue=\"index\",\n",
    "        data=df_melt,\n",
    "        dodge=True,\n",
    "        size=6,\n",
    "        jitter=False,\n",
    "        palette=\"tab10\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    # ➊ 카테고리 순서(좌→우) 얻기\n",
    "    cats = [t.get_text() for t in ax.get_xticklabels()]\n",
    "\n",
    "    # ➋ 각 카테고리 경계(정수 + 0.5 위치)에 점선 추가\n",
    "    for i in range(len(cats) - 1):\n",
    "        ax.axvline(i + 0.5, ls=\"--\", lw=0.8, color=\"gray\", alpha=0.5, zorder=0)\n",
    "\n",
    "    # ➌ 레이아웃/레이블\n",
    "    ax.set_title(\n",
    "        f\"{target_outcome} — Cluster mean |SHAP| per Feature\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Mean |SHAP|\")\n",
    "    plt.xticks()  # rotation=30\n",
    "    plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base / f\"Fig5A_{target_outcome}_dotplot_byFeature_guides.png\", dpi=300)\n",
    "\n",
    "    # ===== (2) Top N feature subplot 버전 =====\n",
    "    top_feats = df_feat.columns[:6]  # 상위 6개 feature 예시\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
    "\n",
    "    for ax, feat in zip(axes.flat, top_feats):\n",
    "        sns.lineplot(x=df_feat.index, y=df_feat[feat], marker=\"o\", color=\"black\", ax=ax)\n",
    "        ax.set_title(feat, fontsize=10)\n",
    "        ax.set_xlabel(\"Cluster\")\n",
    "        ax.set_ylabel(\"Mean |SHAP|\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")  # rotation=30\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{target_outcome} — Cluster mean |SHAP| (Top 6 features)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(base / f\"Fig5A_{target_outcome}_TopFeatures_clean.png\", dpi=300)\n",
    "\n",
    "# ===== (B) Δ(Top−Bottom) Barplot (feature별 이질성 순위) =====\n",
    "topN = 20  # 상위 20개 표시 (필요시 조절)\n",
    "df_rank = df.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "\n",
    "plt.figure(figsize=(7.5, 6.5))\n",
    "sns.barplot(\n",
    "    y=\"Feature\",\n",
    "    x=\"Δ(Top−Bottom)\",\n",
    "    hue=\"Domain\",\n",
    "    data=df_rank,\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    ")\n",
    "plt.title(\n",
    "    \"Top features by SHAP heterogeneity (Δ Top−Bottom)\", fontsize=12, weight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Δ (max−min mean |SHAP| across clusters)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Domain\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base / \"Fig5B_SHAP_Heterogeneity_Rank.png\", dpi=300)\n",
    "\n",
    "# ===== (C) Domain별 평균 SD 막대그래프 =====\n",
    "df_dom = (\n",
    "    df.groupby(\"Domain\", as_index=False)[\"SD\"].mean().sort_values(\"SD\", ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6.5, 4.2))\n",
    "sns.barplot(y=\"Domain\", x=\"SD\", data=df_dom, palette=palette)\n",
    "plt.title(\"Average SHAP heterogeneity (SD) by Domain\", fontsize=12, weight=\"bold\")\n",
    "plt.xlabel(\"Mean SD of mean |SHAP| across clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base / \"Fig5C_Domain_SHAP_Heterogeneity.png\", dpi=300)\n",
    "\n",
    "print(\"✅ Figures saved:\")\n",
    "print(\" -\", base / f\"Fig5A_{target_outcome}_boxplot.png\")\n",
    "print(\" -\", base / \"Fig5B_SHAP_Heterogeneity_Rank.png\")\n",
    "\n",
    "print(\" -\", base / \"Fig5C_Domain_SHAP_Heterogeneity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55690ed",
   "metadata": {},
   "source": [
    "# Raw SHAP 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 샘플별 SHAP 추출 & 저장 ===\n",
    "OUT_ROOT = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters/SHAP\")\n",
    ")\n",
    "\n",
    "\n",
    "def export_sample_shap(label: str, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) SHAP/예측 로드 (이미 있는 함수들 활용)\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # 2) 메타(정답/확률/예측) 정리\n",
    "    meta = pd.DataFrame(\n",
    "        {\n",
    "            \"sample_id\": np.arange(vals.shape[0], dtype=int),\n",
    "            \"y_true\": np.asarray(y_true, dtype=int),\n",
    "            \"y_prob\": np.asarray(y_prob, dtype=float),\n",
    "        }\n",
    "    )\n",
    "    if \"y_pred\" in df_pred.columns:\n",
    "        meta[\"y_pred\"] = df_pred[\"y_pred\"].astype(int)\n",
    "    else:\n",
    "        # 필요 시 Youden threshold로 예측치 생성\n",
    "        thr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "        if thr is None:\n",
    "            thr = _compute_youden_from_preds(\n",
    "                meta[\"y_true\"].values, meta[\"y_prob\"].values\n",
    "            )\n",
    "        meta[\"y_pred\"] = (meta[\"y_prob\"] >= thr).astype(int)\n",
    "\n",
    "    # 3) 샘플×피처 SHAP (wide)\n",
    "    df_shap_wide = pd.DataFrame(vals, columns=feat_names)\n",
    "    df_wide = pd.concat([meta, df_shap_wide], axis=1)\n",
    "\n",
    "    # 4) (선택) 클러스터 라벨 붙이기: 이미 DBSCAN 돌렸다면 라벨 파일을 자동 병합\n",
    "    lab_path = OUT_ROOT / label / f\"{label}__dbscan_labels.csv\"\n",
    "    if lab_path.exists():\n",
    "        lab = pd.read_csv(lab_path)  # columns: idx, cluster\n",
    "        lab = lab.rename(columns={\"idx\": \"sample_id\"})\n",
    "        df_wide = df_wide.merge(lab, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "    # 5) 저장 (wide & long)\n",
    "    out_wide_csv = OUT_ROOT / label / f\"{label}__sample_SHAP_wide.csv\"\n",
    "    out_wide_parq = OUT_ROOT / label / f\"{label}__sample_SHAP_wide.parquet\"\n",
    "    df_wide.to_csv(out_wide_csv, index=False)\n",
    "    try:\n",
    "        df_wide.to_parquet(out_wide_parq, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # long 포맷 (원하면 주석 해제; 용량 커질 수 있음)\n",
    "    # df_long = df_wide.melt(\n",
    "    #     id_vars=[c for c in [\"sample_id\",\"y_true\",\"y_prob\",\"y_pred\",\"cluster\"] if c in df_wide.columns],\n",
    "    #     var_name=\"feature\",\n",
    "    #     value_name=\"shap_value\"\n",
    "    # )\n",
    "    # df_long.to_parquet(OUT_ROOT / label / f\"{label}__sample_SHAP_long.parquet\", index=False)\n",
    "\n",
    "    print(f\"✅ saved: {out_wide_csv}\")\n",
    "    return df_wide\n",
    "\n",
    "\n",
    "for label in [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]:\n",
    "    export_sample_shap(label, OUT_ROOT / label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b06451",
   "metadata": {},
   "source": [
    "# Cluster 유의성 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 경로\n",
    "# --------------------------\n",
    "ROOT = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "SHAP_ROOT = ROOT / \"SHAP\"\n",
    "DBSCAN_ROOT = ROOT\n",
    "SIG_ROOT = DBSCAN_ROOT / \"cluster_sig\"\n",
    "SIG_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff82d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 유틸\n",
    "# --------------------------\n",
    "META_COLS = {\"sample_id\", \"y_true\", \"y_prob\", \"y_pred\", \"cluster\"}\n",
    "\n",
    "\n",
    "def _feature_columns(df: pd.DataFrame):\n",
    "    return [c for c in df.columns if c not in META_COLS]\n",
    "\n",
    "\n",
    "def _all_identical(groups):\n",
    "    cat = np.concatenate([g.ravel() for g in groups if len(g) > 0])\n",
    "    return cat.size == 0 or np.nanstd(cat) == 0.0\n",
    "\n",
    "\n",
    "def _safe_mw(a, b):\n",
    "    if len(a) < 2 or len(b) < 2:\n",
    "        return np.nan, np.nan\n",
    "    if _all_identical([a, b]):\n",
    "        return 1.0, 0.0\n",
    "    p = mannwhitneyu(a, b, alternative=\"two-sided\").pvalue\n",
    "    # Cliff's delta\n",
    "    gt = sum(x > y for x in a for y in b)\n",
    "    lt = sum(x < y for x in a for y in b)\n",
    "    d = (gt - lt) / (len(a) * len(b))\n",
    "    return p, d\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 1) SHAP × Cluster 병합\n",
    "# --------------------------\n",
    "def attach_cluster_fast(label: str) -> Path:\n",
    "    \"\"\"\n",
    "    sample_id == idx 가정 하에 DBSCAN 라벨을 병합.\n",
    "    병합된 CSV를 SHAP_ROOT/<label>/에 저장하고 그 경로를 반환.\n",
    "    \"\"\"\n",
    "    shap_dir = SHAP_ROOT / label\n",
    "    dbscan_dir = DBSCAN_ROOT / label\n",
    "    shap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    f_shap = shap_dir / f\"{label}__sample_SHAP_wide.csv\"\n",
    "    f_lab = dbscan_dir / f\"{label}__dbscan_labels.csv\"\n",
    "    assert f_shap.exists(), f\"Not found: {f_shap}\"\n",
    "    assert f_lab.exists(), f\"Not found: {f_lab}\"\n",
    "\n",
    "    df = pd.read_csv(f_shap)\n",
    "    lab = pd.read_csv(f_lab)  # columns: idx, cluster\n",
    "\n",
    "    merged = df.merge(\n",
    "        lab.rename(columns={\"idx\": \"sample_id\"}), on=\"sample_id\", how=\"left\"\n",
    "    )\n",
    "    cov = merged[\"cluster\"].notna().mean()\n",
    "    print(f\"[{label}] coverage by sample_id==idx: {cov*100:.1f}%\")\n",
    "\n",
    "    out = shap_dir / f\"{label}__sample_SHAP_wide_with_cluster.csv\"\n",
    "    merged.to_csv(out, index=False)\n",
    "    print(f\"✅ saved (merged): {out}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) 유의성 검정 (Kruskal + 모든 pairwise)\n",
    "#    - noise(-1) 제외 기본값\n",
    "#    - pairwise FDR은 '피처별'로 보정\n",
    "# --------------------------\n",
    "def run_significance_full(\n",
    "    label: str, use_abs: bool = True, include_noise: bool = False\n",
    ") -> Path:\n",
    "    f_merged = SHAP_ROOT / label / f\"{label}__sample_SHAP_wide_with_cluster.csv\"\n",
    "    df = pd.read_csv(f_merged)\n",
    "    if not include_noise:\n",
    "        df = df[df[\"cluster\"] != -1].copy()\n",
    "    print(f\"[{label}] Used {len(df)} samples (exclude noise={not include_noise})\")\n",
    "\n",
    "    feat_cols = _feature_columns(df)\n",
    "    clusters = sorted(df[\"cluster\"].dropna().unique())\n",
    "\n",
    "    # ---- (A) Kruskal ----\n",
    "    rows_kw = []\n",
    "    for feat in feat_cols:\n",
    "        vals = np.abs(df[feat].values) if use_abs else df[feat].values\n",
    "        groups = [vals[df[\"cluster\"].values == c] for c in clusters]\n",
    "        if len(groups) < 2 or any(len(g) < 2 for g in groups) or _all_identical(groups):\n",
    "            p_kw = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                p_kw = kruskal(*groups).pvalue\n",
    "            except Exception:\n",
    "                p_kw = np.nan\n",
    "        rows_kw.append({\"Feature\": feat, \"p_kw\": p_kw})\n",
    "    kw = pd.DataFrame(rows_kw)\n",
    "    kw[\"q_kw\"] = multipletests(kw[\"p_kw\"].fillna(1.0), method=\"fdr_bh\")[1]\n",
    "\n",
    "    # 보조 요약(중앙값 기반 Top/Bottom 및 범위)\n",
    "    if len(clusters) >= 1:\n",
    "        med = df.groupby(\"cluster\")[feat_cols].mean()\n",
    "        rng = med.max(0) - med.min(0)\n",
    "        top_c = med.idxmax()\n",
    "        bot_c = med.idxmin()\n",
    "        kw[\"Δ(Top−Bottom)\"] = kw[\"Feature\"].map(rng.to_dict())\n",
    "        kw[\"Top cluster\"] = kw[\"Feature\"].map(top_c.to_dict())\n",
    "        kw[\"Bottom cluster\"] = kw[\"Feature\"].map(bot_c.to_dict())\n",
    "        kw[\"Top vs Bottom\"] = (\n",
    "            kw[\"Top cluster\"].astype(str) + \" > \" + kw[\"Bottom cluster\"].astype(str)\n",
    "        )\n",
    "\n",
    "    # ---- (B) Pairwise 모든 클러스터 쌍 (피처별 FDR) ----\n",
    "    pair_tables = []\n",
    "    for feat in feat_cols:\n",
    "        vals = np.abs(df[feat].values) if use_abs else df[feat].values\n",
    "        rows = []\n",
    "        for c1, c2 in combinations(clusters, 2):\n",
    "            a, b = vals[df[\"cluster\"] == c1], vals[df[\"cluster\"] == c2]\n",
    "            p, d = _safe_mw(a, b)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Feature\": feat,\n",
    "                    \"Cluster1\": c1,\n",
    "                    \"Cluster2\": c2,\n",
    "                    \"p_raw\": p,\n",
    "                    \"cliffs_delta\": d,\n",
    "                    \"n1\": len(a),\n",
    "                    \"n2\": len(b),\n",
    "                }\n",
    "            )\n",
    "        pw_feat = pd.DataFrame(rows)\n",
    "        if len(pw_feat) > 0:\n",
    "            pw_feat[\"q_fdr\"] = multipletests(\n",
    "                pw_feat[\"p_raw\"].fillna(1.0), method=\"fdr_bh\"\n",
    "            )[1]\n",
    "        pair_tables.append(pw_feat)\n",
    "    pw = (\n",
    "        pd.concat(pair_tables, ignore_index=True)\n",
    "        if pair_tables\n",
    "        else pd.DataFrame(\n",
    "            columns=[\n",
    "                \"Feature\",\n",
    "                \"Cluster1\",\n",
    "                \"Cluster2\",\n",
    "                \"p_raw\",\n",
    "                \"cliffs_delta\",\n",
    "                \"n1\",\n",
    "                \"n2\",\n",
    "                \"q_fdr\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---- (C) 저장 ----\n",
    "    out_xlsx = SIG_ROOT / f\"{label}__SHAP_cluster_significance_full.xlsx\"\n",
    "    with pd.ExcelWriter(out_xlsx) as xw:\n",
    "        kw.to_excel(xw, sheet_name=\"Kruskal\", index=False)\n",
    "        pw.to_excel(xw, sheet_name=\"Pairwise_All\", index=False)\n",
    "    print(f\"✅ saved full pairwise significance: {out_xlsx}\")\n",
    "    return out_xlsx\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) 전체 outcome 일괄 실행\n",
    "# --------------------------\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "\n",
    "def run_all(labels=LABELS):\n",
    "    for lb in labels:\n",
    "        try:\n",
    "            attach_cluster_fast(lb)\n",
    "            run_significance_full(\n",
    "                lb, use_abs=True, include_noise=False\n",
    "            )  # |SHAP| 기준, 노이즈 제외\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {lb}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72d716",
   "metadata": {},
   "source": [
    "## Table 및 시각화 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kruskal  # 기존 코드에서 사용\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 경로 설정 -----\n",
    "RAWNORM_BASE = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "SIG_ROOT = RAWNORM_BASE / \"cluster_sig\"\n",
    "FIGTAB_ROOT = RAWNORM_BASE / \"cluster_figure_table\"\n",
    "FIGTAB_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 유틸 -----\n",
    "def stars_from_q(q):\n",
    "    if pd.isna(q):\n",
    "        return \"\"\n",
    "    if q < 0.001:\n",
    "        return \"***\"\n",
    "    if q < 0.01:\n",
    "        return \"**\"\n",
    "    if q < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_cluster_str(s):\n",
    "    # \"cluster_3\" -> 3 / \"C3\" -> 3\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    m = re.search(r\"(-?\\d+)\", str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "# ===== Domain 분류 함수 (기존과 동일) =====\n",
    "# ===== Domain 분류 함수 =====\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = feat.lower()\n",
    "    if any(k in t for k in [\"aggression\", \"support\", \"social\", \"interpersonal\", \"llm\"]):\n",
    "        return \"SDoMH\"\n",
    "    if any(k in t for k in [\"er visit\", \"≥2 er\", \"≥3 er\", \"admission\", \"admit\"]):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"k(\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"alc\",\n",
    "            \"lymphocyte\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"hcl\",\n",
    "            \"asrs\",\n",
    "            \"ham\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"treatment\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# ===== 1) 기존 Table4 요약 읽기 =====\n",
    "f_table4 = RAWNORM_BASE / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "df_all = pd.read_excel(f_table4, sheet_name=\"All_outcomes\")\n",
    "# 안전하게 Domain 재계산(혹시 비어있을 경우)\n",
    "if \"Domain\" not in df_all.columns:\n",
    "    df_all[\"Domain\"] = df_all[\"Feature\"].apply(infer_domain)\n",
    "\n",
    "# ===== 2) 유의성 결과(각 label)와 결합 → 메인 테이블 확장 =====\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "# 결과 컬럼 초기화\n",
    "for col in [\"p_kw\", \"q_kw\", \"p_tb\", \"q_tb\", \"cliffs_delta\", \"sig_kw\", \"sig_tb\"]:\n",
    "    if col not in df_all.columns:\n",
    "        df_all[col] = np.nan\n",
    "\n",
    "# label별로 KW + Pairwise에서 Top-Bottom p를 주입\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if not sig_path.exists():\n",
    "        print(f\"[WARN] missing significance file: {sig_path}\")\n",
    "        continue\n",
    "    df_kw = pd.read_excel(sig_path, sheet_name=\"Kruskal\")\n",
    "    df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "\n",
    "    # 매핑(dict) 준비\n",
    "    qkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"q_kw\"]))\n",
    "    pkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"p_kw\"]))\n",
    "\n",
    "    # df_all 중 해당 outcome만 추림\n",
    "    m_out = df_all[\"Outcome\"] == lb\n",
    "    sub = df_all.loc[m_out].copy()\n",
    "\n",
    "    # 각 row에서 Top/Bottom 클러스터 번호 파싱 후 pairwise에서 해당 쌍의 p/q/효과크기 찾기\n",
    "    p_tb_list, q_tb_list, d_tb_list, sig_tb_list = [], [], [], []\n",
    "    p_kw_list, q_kw_list, sig_kw_list = [], [], []\n",
    "    for _, row in sub.iterrows():\n",
    "        feat = row[\"Feature\"]\n",
    "        # KW\n",
    "        pkw = pkw_map.get(feat, np.nan)\n",
    "        qkw = qkw_map.get(feat, np.nan)\n",
    "        sig_kw = stars_from_q(qkw)\n",
    "\n",
    "        # Top/Bottom 클러스터\n",
    "        topc = parse_cluster_str(row.get(\"Max cluster\") or row.get(\"Top cluster\") or \"\")\n",
    "        botc = parse_cluster_str(\n",
    "            row.get(\"Min cluster\") or row.get(\"Bottom cluster\") or \"\"\n",
    "        )\n",
    "\n",
    "        # pairwise에서 두 방향(Cluster1,Cluster2) 모두 확인\n",
    "        df_feat = df_pw[df_pw[\"Feature\"] == feat]\n",
    "        hit = df_feat[\n",
    "            ((df_feat[\"Cluster1\"] == topc) & (df_feat[\"Cluster2\"] == botc))\n",
    "            | ((df_feat[\"Cluster1\"] == botc) & (df_feat[\"Cluster2\"] == topc))\n",
    "        ]\n",
    "        if len(hit):\n",
    "            p_tb = float(hit[\"p_raw\"].iloc[0])\n",
    "            q_tb = float(hit[\"q_fdr\"].iloc[0])\n",
    "            d_tb = float(hit[\"cliffs_delta\"].iloc[0])\n",
    "            sig_tb = stars_from_q(q_tb)\n",
    "        else:\n",
    "            p_tb = q_tb = d_tb = np.nan\n",
    "            sig_tb = \"\"\n",
    "\n",
    "        p_kw_list.append(pkw)\n",
    "        q_kw_list.append(qkw)\n",
    "        sig_kw_list.append(sig_kw)\n",
    "        p_tb_list.append(p_tb)\n",
    "        q_tb_list.append(q_tb)\n",
    "        d_tb_list.append(d_tb)\n",
    "        sig_tb_list.append(sig_tb)\n",
    "\n",
    "    # 주입\n",
    "    df_all.loc[m_out, \"p_kw\"] = p_kw_list\n",
    "    df_all.loc[m_out, \"q_kw\"] = q_kw_list\n",
    "    df_all.loc[m_out, \"sig_kw\"] = sig_kw_list\n",
    "    df_all.loc[m_out, \"p_tb\"] = p_tb_list\n",
    "    df_all.loc[m_out, \"q_tb\"] = q_tb_list\n",
    "    df_all.loc[m_out, \"cliffs_delta\"] = d_tb_list\n",
    "    df_all.loc[m_out, \"sig_tb\"] = sig_tb_list\n",
    "\n",
    "# 저장(메인 테이블)\n",
    "out_table_main = FIGTAB_ROOT / \"Table4_with_significance.xlsx\"\n",
    "with pd.ExcelWriter(out_table_main) as xw:\n",
    "    df_all.to_excel(xw, sheet_name=\"All_outcomes\", index=False)\n",
    "    for lb, sub in df_all.groupby(\"Outcome\"):\n",
    "        sub.to_excel(xw, sheet_name=lb, index=False)\n",
    "print(\"✅ Saved main table with significance →\", out_table_main)\n",
    "\n",
    "# ===== 3) 서플: 모든 쌍 pairwise를 그대로 outcome별로 CSV 저장 =====\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if sig_path.exists():\n",
    "        df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "        df_pw.to_csv(FIGTAB_ROOT / f\"Supp_Pairwise_All_{lb}.csv\", index=False)\n",
    "\n",
    "# ===== 4) 그림에 별표 추가 (KW 기준) =====\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# (A) Feature별 Dotplot: 범례(Feature 이름)에 별표 붙이기 (q_kw 기준)\n",
    "for outcome in LABELS:\n",
    "    # 히트맵 수치 파일 로드\n",
    "    fpath = RAWNORM_BASE / outcome / f\"{outcome}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    if not fpath.exists():\n",
    "        print(f\"[WARN] missing heatmap CSV: {fpath}\")\n",
    "        continue\n",
    "    df_feat = pd.read_csv(fpath, index_col=0)\n",
    "    # C 라벨로 보기 좋게\n",
    "    df_feat.index = [c.replace(\"cluster_\", \"C\") for c in df_feat.index]\n",
    "\n",
    "    # 해당 outcome의 q_kw 맵(Feature→별표)\n",
    "    sub_main = df_all[df_all[\"Outcome\"] == outcome]\n",
    "    qkw_map = dict(zip(sub_main[\"Feature\"], sub_main[\"q_kw\"]))\n",
    "    star_map = {f: stars_from_q(q) for f, q in qkw_map.items()}\n",
    "\n",
    "    # legend용 라벨 교체를 위해 컬럼명을 \"원래+별\"로 임시 재라벨링\n",
    "    feat_renamed = {\n",
    "        col: (f\"{col}{star_map.get(col,'')}\" if col in star_map else col)\n",
    "        for col in df_feat.columns\n",
    "    }\n",
    "    df_feat_renamed = df_feat.rename(columns=feat_renamed)\n",
    "\n",
    "    # Dotplot\n",
    "    df_melt = df_feat_renamed.T.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Cluster\", value_name=\"Mean |SHAP|\"\n",
    "    )\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.stripplot(\n",
    "        x=\"Cluster\",\n",
    "        y=\"Mean |SHAP|\",\n",
    "        hue=\"index\",\n",
    "        data=df_melt,\n",
    "        dodge=True,\n",
    "        size=6,\n",
    "        jitter=False,\n",
    "        palette=\"tab10\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    # 경계선 가이드\n",
    "    cats = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    for i in range(len(cats) - 1):\n",
    "        ax.axvline(i + 0.5, ls=\"--\", lw=0.8, color=\"gray\", alpha=0.5, zorder=0)\n",
    "    ax.set_title(\n",
    "        f\"{outcome} — Cluster mean |SHAP| per Feature\", fontsize=11, weight=\"bold\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Mean |SHAP|\")\n",
    "    plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_dotplot_byFeature_guides_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # (B) Top N feature subplot: 제목에 별표 붙이기 (KW 기준)\n",
    "    top_feats = df_feat.columns[:6]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
    "    for ax, feat in zip(axes.flat, top_feats):\n",
    "        star = star_map.get(feat, \"\")\n",
    "        sns.lineplot(x=df_feat.index, y=df_feat[feat], marker=\"o\", color=\"black\", ax=ax)\n",
    "        ax.set_title(f\"{feat}{star}\", fontsize=10)\n",
    "        ax.set_xlabel(\"Cluster\")\n",
    "        ax.set_ylabel(\"Mean |SHAP|\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")\n",
    "    plt.suptitle(\n",
    "        f\"{outcome} — Cluster mean |SHAP| (Top 6 features)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_TopFeatures_clean_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "# (B) Δ(Top−Bottom) barplot & (C) Domain barplot은 기존 그림 유지\n",
    "#    (원하면 bar label에 별표 추가 가능: df_all에서 원하는 조건으로 star를 조합)\n",
    "# 그대로 복제해서 저장 위치만 figure_table로 변경:\n",
    "\n",
    "# Δ(Top−Bottom) TopN\n",
    "topN = 20\n",
    "df_rank = df_all.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "plt.figure(figsize=(7.5, 6.5))\n",
    "sns.barplot(\n",
    "    y=\"Feature\",\n",
    "    x=\"Δ(Top−Bottom)\",\n",
    "    hue=\"Domain\",\n",
    "    data=df_rank,\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    ")\n",
    "plt.title(\n",
    "    \"Top features by SHAP heterogeneity (Δ Top−Bottom)\", fontsize=12, weight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Δ (max−min mean |SHAP| across clusters)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Domain\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5B_SHAP_Heterogeneity_Rank.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Domain 별 평균 SD\n",
    "df_dom = (\n",
    "    df_all.groupby(\"Domain\", as_index=False)[\"SD\"]\n",
    "    .mean()\n",
    "    .sort_values(\"SD\", ascending=False)\n",
    ")\n",
    "plt.figure(figsize=(6.5, 4.2))\n",
    "sns.barplot(y=\"Domain\", x=\"SD\", data=df_dom, palette=palette)\n",
    "plt.title(\"Average SHAP heterogeneity (SD) by Domain\", fontsize=12, weight=\"bold\")\n",
    "plt.xlabel(\"Mean SD of mean |SHAP| across clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5C_Domain_SHAP_Heterogeneity.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Outputs saved to:\", FIGTAB_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b31bd1",
   "metadata": {},
   "source": [
    "- ER, AD 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGTAB_ROOT = RAWNORM_BASE / \"cluster_figure_table/no_util\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 유틸 -----\n",
    "def stars_from_q(q):\n",
    "    if pd.isna(q):\n",
    "        return \"\"\n",
    "    if q < 0.001:\n",
    "        return \"***\"\n",
    "    if q < 0.01:\n",
    "        return \"**\"\n",
    "    if q < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_cluster_str(s):\n",
    "    # \"cluster_3\" -> 3 / \"C3\" -> 3\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    m = re.search(r\"(-?\\d+)\", str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "# ===== Domain 분류 함수 (기존과 동일) =====\n",
    "# ===== Domain 분류 함수 =====\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = feat.lower()\n",
    "    if any(k in t for k in [\"aggression\", \"support\", \"social\", \"interpersonal\", \"llm\"]):\n",
    "        return \"Aggression / Social function (LLM)\"\n",
    "    if any(k in t for k in [\"er visit\", \"≥2 er\", \"≥3 er\", \"admission\", \"admit\"]):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"k(\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"alc\",\n",
    "            \"lymphocyte\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"hcl\",\n",
    "            \"asrs\",\n",
    "            \"ham\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"treatment\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# ===== 1) 기존 Table4 요약 읽기 =====\n",
    "f_table4 = RAWNORM_BASE / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "df_all = pd.read_excel(f_table4, sheet_name=\"All_outcomes\")\n",
    "# 안전하게 Domain 재계산(혹시 비어있을 경우)\n",
    "if \"Domain\" not in df_all.columns:\n",
    "    df_all[\"Domain\"] = df_all[\"Feature\"].apply(infer_domain)\n",
    "\n",
    "# ===== 2) 유의성 결과(각 label)와 결합 → 메인 테이블 확장 =====\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "# 결과 컬럼 초기화\n",
    "for col in [\"p_kw\", \"q_kw\", \"p_tb\", \"q_tb\", \"cliffs_delta\", \"sig_kw\", \"sig_tb\"]:\n",
    "    if col not in df_all.columns:\n",
    "        df_all[col] = np.nan\n",
    "\n",
    "# label별로 KW + Pairwise에서 Top-Bottom p를 주입\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if not sig_path.exists():\n",
    "        print(f\"[WARN] missing significance file: {sig_path}\")\n",
    "        continue\n",
    "    df_kw = pd.read_excel(sig_path, sheet_name=\"Kruskal\")\n",
    "    df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "\n",
    "    # 매핑(dict) 준비\n",
    "    qkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"q_kw\"]))\n",
    "    pkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"p_kw\"]))\n",
    "\n",
    "    # df_all 중 해당 outcome만 추림\n",
    "    m_out = df_all[\"Outcome\"] == lb\n",
    "    sub = df_all.loc[m_out].copy()\n",
    "\n",
    "    # 각 row에서 Top/Bottom 클러스터 번호 파싱 후 pairwise에서 해당 쌍의 p/q/효과크기 찾기\n",
    "    p_tb_list, q_tb_list, d_tb_list, sig_tb_list = [], [], [], []\n",
    "    p_kw_list, q_kw_list, sig_kw_list = [], [], []\n",
    "    for _, row in sub.iterrows():\n",
    "        feat = row[\"Feature\"]\n",
    "        # KW\n",
    "        pkw = pkw_map.get(feat, np.nan)\n",
    "        qkw = qkw_map.get(feat, np.nan)\n",
    "        sig_kw = stars_from_q(qkw)\n",
    "\n",
    "        # Top/Bottom 클러스터\n",
    "        topc = parse_cluster_str(row.get(\"Max cluster\") or row.get(\"Top cluster\") or \"\")\n",
    "        botc = parse_cluster_str(\n",
    "            row.get(\"Min cluster\") or row.get(\"Bottom cluster\") or \"\"\n",
    "        )\n",
    "\n",
    "        # pairwise에서 두 방향(Cluster1,Cluster2) 모두 확인\n",
    "        df_feat = df_pw[df_pw[\"Feature\"] == feat]\n",
    "        hit = df_feat[\n",
    "            ((df_feat[\"Cluster1\"] == topc) & (df_feat[\"Cluster2\"] == botc))\n",
    "            | ((df_feat[\"Cluster1\"] == botc) & (df_feat[\"Cluster2\"] == topc))\n",
    "        ]\n",
    "        if len(hit):\n",
    "            p_tb = float(hit[\"p_raw\"].iloc[0])\n",
    "            q_tb = float(hit[\"q_fdr\"].iloc[0])\n",
    "            d_tb = float(hit[\"cliffs_delta\"].iloc[0])\n",
    "            sig_tb = stars_from_q(q_tb)\n",
    "        else:\n",
    "            p_tb = q_tb = d_tb = np.nan\n",
    "            sig_tb = \"\"\n",
    "\n",
    "        p_kw_list.append(pkw)\n",
    "        q_kw_list.append(qkw)\n",
    "        sig_kw_list.append(sig_kw)\n",
    "        p_tb_list.append(p_tb)\n",
    "        q_tb_list.append(q_tb)\n",
    "        d_tb_list.append(d_tb)\n",
    "        sig_tb_list.append(sig_tb)\n",
    "\n",
    "    # 주입\n",
    "    df_all.loc[m_out, \"p_kw\"] = p_kw_list\n",
    "    df_all.loc[m_out, \"q_kw\"] = q_kw_list\n",
    "    df_all.loc[m_out, \"sig_kw\"] = sig_kw_list\n",
    "    df_all.loc[m_out, \"p_tb\"] = p_tb_list\n",
    "    df_all.loc[m_out, \"q_tb\"] = q_tb_list\n",
    "    df_all.loc[m_out, \"cliffs_delta\"] = d_tb_list\n",
    "    df_all.loc[m_out, \"sig_tb\"] = sig_tb_list\n",
    "\n",
    "# 저장(메인 테이블)\n",
    "out_table_main = FIGTAB_ROOT / \"Table4_with_significance.xlsx\"\n",
    "with pd.ExcelWriter(out_table_main) as xw:\n",
    "    df_all.to_excel(xw, sheet_name=\"All_outcomes\", index=False)\n",
    "    for lb, sub in df_all.groupby(\"Outcome\"):\n",
    "        sub.to_excel(xw, sheet_name=lb, index=False)\n",
    "print(\"✅ Saved main table with significance →\", out_table_main)\n",
    "\n",
    "# ===== 3) 서플: 모든 쌍 pairwise를 그대로 outcome별로 CSV 저장 =====\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if sig_path.exists():\n",
    "        df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "        df_pw.to_csv(FIGTAB_ROOT / f\"Supp_Pairwise_All_{lb}.csv\", index=False)\n",
    "\n",
    "# ===== 4) 그림에 별표 추가 (KW 기준) =====\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# (A) Feature별 Dotplot: 범례(Feature 이름)에 별표 붙이기 (q_kw 기준)\n",
    "for outcome in LABELS:\n",
    "    # 히트맵 수치 파일 로드\n",
    "    fpath = RAWNORM_BASE / outcome / f\"{outcome}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    if not fpath.exists():\n",
    "        print(f\"[WARN] missing heatmap CSV: {fpath}\")\n",
    "        continue\n",
    "    df_feat = pd.read_csv(fpath, index_col=0)\n",
    "\n",
    "    # 🌟🌟🌟 이 위치에 필터링 코드 추가 🌟🌟🌟\n",
    "    EXCLUDE_VARS = [\"≥2 ER Visits\", \"≥3 Admissions\"]\n",
    "    cols_to_keep = [col for col in df_feat.columns if col not in EXCLUDE_VARS]\n",
    "    df_feat = df_feat[cols_to_keep]\n",
    "    # 🌟🌟🌟 필터링 코드 끝 🌟🌟🌟\n",
    "\n",
    "    # C 라벨로 보기 좋게\n",
    "    df_feat.index = [c.replace(\"cluster_\", \"C\") for c in df_feat.index]\n",
    "\n",
    "    # 해당 outcome의 q_kw 맵(Feature→별표)\n",
    "    sub_main = df_all[df_all[\"Outcome\"] == outcome]\n",
    "    qkw_map = dict(zip(sub_main[\"Feature\"], sub_main[\"q_kw\"]))\n",
    "    star_map = {f: stars_from_q(q) for f, q in qkw_map.items()}\n",
    "\n",
    "    # legend용 라벨 교체를 위해 컬럼명을 \"원래+별\"로 임시 재라벨링\n",
    "    feat_renamed = {\n",
    "        col: (f\"{col}{star_map.get(col,'')}\" if col in star_map else col)\n",
    "        for col in df_feat.columns\n",
    "    }\n",
    "    df_feat_renamed = df_feat.rename(columns=feat_renamed)\n",
    "\n",
    "    # Dotplot\n",
    "    df_melt = df_feat_renamed.T.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Cluster\", value_name=\"Mean |SHAP|\"\n",
    "    )\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.stripplot(\n",
    "        x=\"Cluster\",\n",
    "        y=\"Mean |SHAP|\",\n",
    "        hue=\"index\",\n",
    "        data=df_melt,\n",
    "        dodge=True,\n",
    "        size=6,\n",
    "        jitter=False,\n",
    "        palette=\"tab10\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    # 경계선 가이드\n",
    "    cats = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    for i in range(len(cats) - 1):\n",
    "        ax.axvline(i + 0.5, ls=\"--\", lw=0.8, color=\"gray\", alpha=0.5, zorder=0)\n",
    "    ax.set_title(\n",
    "        f\"{outcome} — Cluster mean |SHAP| per Feature\", fontsize=11, weight=\"bold\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Mean |SHAP|\")\n",
    "    plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_dotplot_byFeature_guides_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # (B) Top N feature subplot: 제목에 별표 붙이기 (KW 기준)\n",
    "    top_feats = df_feat.columns[:6]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
    "    for ax, feat in zip(axes.flat, top_feats):\n",
    "        star = star_map.get(feat, \"\")\n",
    "        sns.lineplot(x=df_feat.index, y=df_feat[feat], marker=\"o\", color=\"black\", ax=ax)\n",
    "        ax.set_title(f\"{feat}{star}\", fontsize=10)\n",
    "        ax.set_xlabel(\"Cluster\")\n",
    "        ax.set_ylabel(\"Mean |SHAP|\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")\n",
    "    plt.suptitle(\n",
    "        f\"{outcome} — Cluster mean |SHAP| (Top 6 features)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_TopFeatures_clean_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "# (B) Δ(Top−Bottom) barplot & (C) Domain barplot은 기존 그림 유지\n",
    "#    (원하면 bar label에 별표 추가 가능: df_all에서 원하는 조건으로 star를 조합)\n",
    "# 그대로 복제해서 저장 위치만 figure_table로 변경:\n",
    "\n",
    "# Δ(Top−Bottom) TopN\n",
    "\n",
    "# 🌟🌟🌟 이 위치에 필터링 코드 추가 🌟🌟🌟\n",
    "EXCLUDE_VARS = [\"AD_more_three\", \"ER_more_two\"]\n",
    "df_all_filtered = df_all[~df_all[\"Feature\"].isin(EXCLUDE_VARS)].copy()\n",
    "# 🌟🌟🌟 필터링 코드 끝 🌟🌟🌟\n",
    "\n",
    "topN = 20\n",
    "# df_rank = df_all.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "df_rank = df_all_filtered.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "plt.figure(figsize=(7.5, 6.5))\n",
    "sns.barplot(\n",
    "    y=\"Feature\",\n",
    "    x=\"Δ(Top−Bottom)\",\n",
    "    hue=\"Domain\",\n",
    "    data=df_rank,\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    ")\n",
    "plt.title(\n",
    "    \"Top features by SHAP heterogeneity (Δ Top−Bottom)\", fontsize=12, weight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Δ (max−min mean |SHAP| across clusters)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Domain\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5B_SHAP_Heterogeneity_Rank.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Domain 별 평균 SD\n",
    "# df_dom = (\n",
    "#     df_all.groupby(\"Domain\", as_index=False)[\"SD\"]\n",
    "#     .mean()\n",
    "#     .sort_values(\"SD\", ascending=False)\n",
    "# )\n",
    "df_all_viz = df_all[~df_all[\"Feature\"].isin(EXCLUDE_VARS)]  # 🌟 필터링된 데이터 사용\n",
    "\n",
    "df_dom = (\n",
    "    df_all_viz.groupby(\"Domain\", as_index=False)[\"SD\"]  # 🌟 df_all_viz 사용\n",
    "    .mean()\n",
    "    .sort_values(\"SD\", ascending=False)\n",
    ")\n",
    "plt.figure(figsize=(6.5, 4.2))\n",
    "sns.barplot(y=\"Domain\", x=\"SD\", data=df_dom, palette=palette)\n",
    "plt.title(\"Average SHAP heterogeneity (SD) by Domain\", fontsize=12, weight=\"bold\")\n",
    "plt.xlabel(\"Mean SD of mean |SHAP| across clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5C_Domain_SHAP_Heterogeneity.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Outputs saved to:\", FIGTAB_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5900b",
   "metadata": {},
   "source": [
    "# Waterfall plot 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f39d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, glob, textwrap\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========= 경로 설정 (당신 환경에 맞게 확인) =========\n",
    "ROOT = Path(\n",
    "    str(PROJECT_ROOT / \"results/step2_modeling/251127_Final_modeling_globalunion25/figures/clinic_interpretation\")\n",
    ")\n",
    "PWF = ROOT / \"personal_waterfall_figures\"  # 개인 워터폴\n",
    "CLSR = ROOT / \"PatientClusters\"  # 클러스터 산출물\n",
    "OUT = ROOT / \"personal_cluster_water_sum\"  # 출력 폴더 (이미 사용 중이면 그대로 씀)\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ========= 공통 유틸 =========\n",
    "def _font(size=18):\n",
    "    try:\n",
    "        # mac 기본 폰트 후보\n",
    "        return ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", size)\n",
    "    except Exception:\n",
    "        return ImageFont.load_default()\n",
    "\n",
    "\n",
    "def read_topk_txt_if_any(png_path: Path, fallback_csv_top: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    representatives_dbscan 쪽은 __top*.txt가 없을 수 있으니\n",
    "    옆의 __top20.csv에서 상위 k개로 간단히 카드 문자열 구성.\n",
    "    \"\"\"\n",
    "    txt_path = (\n",
    "        png_path.with_suffix(\"\").with_suffix(\"\").with_name(png_path.stem + \"__top3.txt\")\n",
    "    )\n",
    "    if txt_path.exists():\n",
    "        try:\n",
    "            return txt_path.read_text().strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback: 같은 이름의 __top20.csv 이용\n",
    "    csv_guess = png_path.with_name(png_path.stem + \"__top20.csv\")\n",
    "    if csv_guess.exists():\n",
    "        try:\n",
    "            d = pd.read_csv(csv_guess)\n",
    "            d = d.sort_values(\"rank\").head(fallback_csv_top)\n",
    "            bits = [\n",
    "                f\"{r.feature}({'+' if r.shap_value>=0 else '−'})\"\n",
    "                for r in d.itertuples(index=False)\n",
    "            ]\n",
    "            return \"; \".join(bits)\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def montage(\n",
    "    images: List[Image.Image],\n",
    "    captions: List[str],\n",
    "    ncols=3,\n",
    "    pad=28,\n",
    "    cap_h=42,\n",
    "    bgcolor=(255, 255, 255),\n",
    "    font_size=18,\n",
    ") -> Image.Image:\n",
    "    \"\"\"이미지들을 grid로 배치하고 각 패널 하단에 한 줄 캡션\"\"\"\n",
    "    if not images:\n",
    "        raise ValueError(\"No images to montage.\")\n",
    "    W = max(im.width for im in images)\n",
    "    H = max(im.height for im in images)\n",
    "    font = _font(font_size)\n",
    "    n = len(images)\n",
    "    ncols = max(1, ncols)\n",
    "    nrows = math.ceil(n / ncols)\n",
    "    canvas_w = pad + ncols * (W + pad)\n",
    "    canvas_h = pad + nrows * (H + cap_h + pad)\n",
    "    canvas = Image.new(\"RGB\", (canvas_w, canvas_h), bgcolor)\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    for i, (im, cap) in enumerate(zip(images, captions)):\n",
    "        r, c = divmod(i, ncols)\n",
    "        x = pad + c * (W + pad)\n",
    "        y = pad + r * (H + cap_h + pad)\n",
    "        canvas.paste(im, (x, y))\n",
    "        # 한 줄로 너무 길면 줄임표\n",
    "        cap = cap.strip().replace(\"\\n\", \" \")\n",
    "        if len(cap) > 120:\n",
    "            cap = cap[:117] + \"...\"\n",
    "        draw.text((x, y + H + 4), cap, fill=(25, 25, 25), font=font)\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def save_multipage_pdf(png_paths: List[Path], out_pdf: Path):\n",
    "    \"\"\"여러 PNG를 여러 페이지 PDF로 저장\"\"\"\n",
    "    imgs = [Image.open(p).convert(\"RGB\") for p in png_paths]\n",
    "    if not imgs:\n",
    "        return\n",
    "    first, rest = imgs[0], imgs[1:]\n",
    "    first.save(out_pdf, save_all=True, append_images=rest)\n",
    "\n",
    "\n",
    "def parse_cluster_from_name(name: str) -> str:\n",
    "    m = re.search(r\"cluster[_\\-]?(\\d+)\", name)\n",
    "    return m.group(1) if m else \"?\"\n",
    "\n",
    "\n",
    "def caption_from_filename(p: Path, extra: str = \"\") -> str:\n",
    "    \"\"\"파일명에서 핵심 메타 추출 (id, tag 등) + 보조 텍스트\"\"\"\n",
    "    stem = p.stem\n",
    "    # label_xxx__waterfall_id123__cluster2__median\n",
    "    m_id = re.search(r\"id(\\d+)\", stem)\n",
    "    m_tag = re.search(r\"__(mean|max|min|fp|fn)\", stem)\n",
    "    s_id = m_id.group(1) if m_id else \"\"\n",
    "    s_tag = m_tag.group(1).upper() if m_tag else \"\"\n",
    "    s_clu = parse_cluster_from_name(stem)\n",
    "    bits = []\n",
    "    if s_clu != \"?\":\n",
    "        bits.append(f\"C{s_clu}\")\n",
    "    if s_tag:\n",
    "        bits.append(s_tag)\n",
    "    if s_id:\n",
    "        bits.append(f\"id={s_id}\")\n",
    "    if extra:\n",
    "        bits.append(extra)\n",
    "    return \" · \".join(bits)\n",
    "\n",
    "\n",
    "# ========= Figure 8: Cluster-level representatives =========\n",
    "def build_figure8_for_label(\n",
    "    label: str,\n",
    "    max_clusters: int = 4,\n",
    "    per_cluster_modes: Tuple[str, ...] = (\"median\", \"max\", \"min\"),\n",
    "    select_by: str = \"largest\",  # or \"event_rate\"\n",
    "    ncols: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    representatives_dbscan/cluster_k/ 에 있는 median/max/min png를 한 장으로.\n",
    "    max_clusters 개 클러스터만 선별(본문용 축약).\n",
    "    \"\"\"\n",
    "    lab_dir = CLSR / label\n",
    "    rep_root = lab_dir / \"representatives_dbscan\"\n",
    "    stat_csv = lab_dir / f\"{label}__dbscan_cluster_stats.csv\"\n",
    "    if not rep_root.exists():\n",
    "        print(f\"[{label}] no representatives_dbscan.\")\n",
    "        return None\n",
    "\n",
    "    # 클러스터 선택: 크기순 or 사건율순\n",
    "    clusters = []\n",
    "    if stat_csv.exists():\n",
    "        d = pd.read_csv(stat_csv)\n",
    "        if select_by == \"event_rate\" and \"event_rate\" in d.columns:\n",
    "            d = d.sort_values([\"event_rate\", \"n\"], ascending=[False, False])\n",
    "        else:\n",
    "            d = d.sort_values([\"n\", \"event_rate\"], ascending=[False, False])\n",
    "        clusters = [int(x) for x in d[\"cluster\"].tolist()]\n",
    "    else:\n",
    "        # 폴더 스캔\n",
    "        clusters = sorted(\n",
    "            [int(parse_cluster_from_name(p.name)) for p in rep_root.glob(\"cluster_*\")]\n",
    "        )\n",
    "\n",
    "    clusters = clusters[:max_clusters]\n",
    "    panel_imgs, panel_caps = [], []\n",
    "    for c in clusters:\n",
    "        cdir = rep_root / f\"cluster_{c}\"\n",
    "        for mode in per_cluster_modes:\n",
    "            # label__waterfall_id###__cluster{c}__{mode}.png\n",
    "            cand = sorted(cdir.glob(f\"{label}__waterfall_id*__cluster{c}__{mode}.png\"))\n",
    "            if not cand:\n",
    "                continue\n",
    "            p = cand[0]\n",
    "            cap_extra = read_topk_txt_if_any(p)\n",
    "            cap = caption_from_filename(p) + (f\" · {cap_extra}\" if cap_extra else \"\")\n",
    "            panel_imgs.append(Image.open(p).convert(\"RGB\"))\n",
    "            panel_caps.append(cap)\n",
    "\n",
    "    if not panel_imgs:\n",
    "        print(f\"[{label}] no panels for Figure8.\")\n",
    "        return None\n",
    "\n",
    "    fig = montage(panel_imgs, panel_caps, ncols=ncols, pad=28, cap_h=46, font_size=18)\n",
    "    out_path = OUT / f\"Figure8_{label}_representatives.png\"\n",
    "    fig.save(out_path, quality=95)\n",
    "    print(f\"[OK] Figure8 saved → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def build_figure8_all_labels(labels: List[str]):\n",
    "    outs = []\n",
    "    for lb in labels:\n",
    "        p = build_figure8_for_label(\n",
    "            lb,\n",
    "            max_clusters=4,\n",
    "            per_cluster_modes=(\"mean\", \"max\", \"min\"),\n",
    "            select_by=\"largest\",\n",
    "            ncols=3,\n",
    "        )\n",
    "        if p:\n",
    "            outs.append(p)\n",
    "    # 멀티페이지 PDF로도 저장\n",
    "    if outs:\n",
    "        pdf = OUT / \"Figure8_ClusterRepresentatives_ALL.pdf\"\n",
    "        save_multipage_pdf(outs, pdf)\n",
    "        print(f\"[OK] multi-page PDF → {pdf}\")\n",
    "\n",
    "\n",
    "# ========= Figure 9: Decision boundary (borderline vs TP) =========\n",
    "def build_figure9_for_label(label: str, pairs: int = 3, ncols: int = 2):\n",
    "    \"\"\"\n",
    "    personal_waterfall_figures/label_xx/{borderline,true_positive}\n",
    "    요에서 같은 outcome의 borderline K개와 TP K개를 좌/우 비교 그리드로.\n",
    "    \"\"\"\n",
    "    base = PWF / label\n",
    "    bdir = base / \"borderline\"\n",
    "    tdir = base / \"true_positive\"\n",
    "    if not (bdir.exists() and tdir.exists()):\n",
    "        print(f\"[{label}] missing borderline/true_positive dirs.\")\n",
    "        return None\n",
    "\n",
    "    bsum = sorted(bdir.glob(f\"summary__{label}__borderline_k.csv\"))\n",
    "    tsum = sorted(tdir.glob(f\"summary__{label}__tp.csv\"))\n",
    "    if not (bsum and tsum):\n",
    "        print(f\"[{label}] missing summary csvs.\")\n",
    "        return None\n",
    "\n",
    "    db = pd.read_csv(bsum[0])\n",
    "    dt = pd.read_csv(tsum[0])\n",
    "    # 상위 probability 기준 정렬(이미 정렬되어 있을 가능성 큼)\n",
    "    db = db.sort_values(\"y_prob\", ascending=False).head(pairs)\n",
    "    dt = dt.sort_values(\"y_prob\", ascending=False).head(pairs)\n",
    "\n",
    "    imgs, caps = [], []\n",
    "    for (_, rb), (_, rt) in zip(db.iterrows(), dt.iterrows()):\n",
    "        # 아이디 기준 이미지 찾기\n",
    "        bid = int(rb[\"idx\"])\n",
    "        tid = int(rt[\"idx\"])\n",
    "        bp = bdir / f\"waterfall_patient_{bid}.png\"\n",
    "        tp = tdir / f\"waterfall_patient_{tid}.png\"\n",
    "        if not (bp.exists() and tp.exists()):\n",
    "            continue\n",
    "        for pth, meta in [(bp, rb), (tp, rt)]:\n",
    "            cap_extra = read_topk_txt_if_any(pth)\n",
    "            thr = (\n",
    "                f\", thr={meta['youden_thr']:.3f}\"\n",
    "                if not pd.isna(meta.get(\"youden_thr\", np.nan))\n",
    "                else \"\"\n",
    "            )\n",
    "            cap = f\"{'BORDER' if pth==bp else 'TP'} · id={int(meta['idx'])} · y_true={int(meta['y_true'])}, y_pred={int(meta['y_pred'])}, y_prob={meta['y_prob']:.3f}{thr}\"\n",
    "            if cap_extra:\n",
    "                cap += f\" · {cap_extra}\"\n",
    "            imgs.append(Image.open(pth).convert(\"RGB\"))\n",
    "            caps.append(cap)\n",
    "\n",
    "    if not imgs:\n",
    "        print(f\"[{label}] no panels for Figure9.\")\n",
    "        return None\n",
    "\n",
    "    # 2열(좌: Borderline, 우: TP) 고정\n",
    "    fig = montage(imgs, caps, ncols=ncols, pad=32, cap_h=48, font_size=18)\n",
    "    out_path = OUT / f\"Figure9_{label}_decision_boundary.png\"\n",
    "    fig.save(out_path, quality=95)\n",
    "    print(f\"[OK] Figure9 saved → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def build_figure9_all_labels(labels: List[str]):\n",
    "    outs = []\n",
    "    for lb in labels:\n",
    "        p = build_figure9_for_label(lb, pairs=3, ncols=2)\n",
    "        if p:\n",
    "            outs.append(p)\n",
    "    if outs:\n",
    "        pdf = OUT / \"Figure9_DecisionBoundary_ALL.pdf\"\n",
    "        save_multipage_pdf(outs, pdf)\n",
    "        print(f\"[OK] multi-page PDF → {pdf}\")\n",
    "\n",
    "\n",
    "# ========= Supplement eFigure S6: Cluster gallery (모든 클러스터) =========\n",
    "def build_supp_cluster_gallery(label: str, ncols=3):\n",
    "    lab_dir = CLSR / label / \"representatives_dbscan\"\n",
    "    if not lab_dir.exists():\n",
    "        print(f\"[{label}] no representatives_dbscan.\")\n",
    "        return None\n",
    "    pages = []\n",
    "    for cdir in sorted(\n",
    "        lab_dir.glob(\"cluster_*\"), key=lambda p: int(parse_cluster_from_name(p.name))\n",
    "    ):\n",
    "        pngs = sorted(\n",
    "            cdir.glob(f\"{label}__waterfall_id*__cluster*__(mean|max|min|fp|fn).png\")\n",
    "        )\n",
    "        if not pngs:\n",
    "            continue\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in pngs]\n",
    "        caps = [caption_from_filename(p, extra=read_topk_txt_if_any(p)) for p in pngs]\n",
    "        page = montage(imgs, caps, ncols=ncols, pad=28, cap_h=46, font_size=18)\n",
    "        tmp = OUT / f\"_tmp_{label}_cluster{parse_cluster_from_name(cdir.name)}.png\"\n",
    "        page.save(tmp, quality=95)\n",
    "        pages.append(tmp)\n",
    "    if pages:\n",
    "        out_pdf = OUT / f\"eFigureS6_{label}_ClusterGallery.pdf\"\n",
    "        save_multipage_pdf(pages, out_pdf)\n",
    "        print(f\"[OK] eFigure S6 saved → {out_pdf}\")\n",
    "        # 임시 PNG 삭제(원하면 유지 가능)\n",
    "        for p in pages:\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# ========= Supplement eFigure S7: Personal waterfalls (TP/FN galleries) =========\n",
    "def build_supp_personal_gallery(\n",
    "    label: str, ncols=3, which=(\"true_positive\", \"false_negative\", \"borderline\")\n",
    "):\n",
    "    base = PWF / label\n",
    "    pages = []\n",
    "    for mode in which:\n",
    "        mdir = base / mode\n",
    "        if not mdir.exists():\n",
    "            continue\n",
    "        pngs = sorted(mdir.glob(\"waterfall_patient_*.png\"))\n",
    "        if not pngs:\n",
    "            continue\n",
    "        # 여러 페이지로 나눔 (1페이지 당 ncols*rows = 9 가정)\n",
    "        per_page = ncols * 3\n",
    "        for i in range(0, len(pngs), per_page):\n",
    "            chunk = pngs[i : i + per_page]\n",
    "            imgs = [Image.open(p).convert(\"RGB\") for p in chunk]\n",
    "            caps = []\n",
    "            # summary csv 로 메타 읽기 (optional)\n",
    "            scsv = list(mdir.glob(\"summary__*.csv\"))\n",
    "            meta = None\n",
    "            if scsv:\n",
    "                try:\n",
    "                    meta = pd.read_csv(scsv[0]).set_index(\"idx\")\n",
    "                except Exception:\n",
    "                    meta = None\n",
    "            for p in chunk:\n",
    "                m = re.search(r\"waterfall_patient_(\\d+)\", p.stem)\n",
    "                cap_extra = read_topk_txt_if_any(p)\n",
    "                if meta is not None and m:\n",
    "                    iidx = int(m.group(1))\n",
    "                    if iidx in meta.index:\n",
    "                        r = meta.loc[iidx]\n",
    "                        thr = (\n",
    "                            f\", thr={r['youden_thr']:.3f}\"\n",
    "                            if not pd.isna(r.get(\"youden_thr\", np.nan))\n",
    "                            else \"\"\n",
    "                        )\n",
    "                        cap = f\"{mode.upper()} · id={iidx} · y_true={int(r['y_true'])}, y_pred={int(r['y_pred'])}, y_prob={r['y_prob']:.3f}{thr}\"\n",
    "                    else:\n",
    "                        cap = f\"{mode.upper()} · id={iidx}\"\n",
    "                else:\n",
    "                    cap = mode.upper()\n",
    "                if cap_extra:\n",
    "                    cap += f\" · {cap_extra}\"\n",
    "                caps.append(cap)\n",
    "            page = montage(imgs, caps, ncols=ncols, pad=28, cap_h=46, font_size=18)\n",
    "            tmp = OUT / f\"_tmp_{label}_{mode}_{i//per_page+1}.png\"\n",
    "            page.save(tmp, quality=95)\n",
    "            pages.append(tmp)\n",
    "    if pages:\n",
    "        out_pdf = OUT / f\"eFigureS7_{label}_PersonalWaterfalls.pdf\"\n",
    "        save_multipage_pdf(pages, out_pdf)\n",
    "        print(f\"[OK] eFigure S7 saved → {out_pdf}\")\n",
    "        for p in pages:\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# ========= eTable S6: Case cards (CSV/XLSX) =========\n",
    "def build_case_cards(labels: List[str]):\n",
    "    rows = []\n",
    "    # representatives_dbscan\n",
    "    for lb in labels:\n",
    "        rep_root = CLSR / lb / \"representatives_dbscan\"\n",
    "        for p in sorted(rep_root.glob(\"cluster_*/*.png\")):\n",
    "            kind = \"cluster_rep\"\n",
    "            m_id = re.search(r\"id(\\d+)\", p.stem)\n",
    "            idx = int(m_id.group(1)) if m_id else None\n",
    "            cluster = parse_cluster_from_name(p.stem)\n",
    "            tag = re.search(r\"__(mean|max|min|fp|fn)\", p.stem)\n",
    "            tag = tag.group(1) if tag else \"\"\n",
    "            top = read_topk_txt_if_any(p)\n",
    "            rows.append(\n",
    "                dict(\n",
    "                    source=kind,\n",
    "                    outcome=lb,\n",
    "                    cluster=cluster,\n",
    "                    case_id=idx,\n",
    "                    tag=tag,\n",
    "                    path=str(p),\n",
    "                    top3=top,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # personal_waterfall_figures\n",
    "    for lb in labels:\n",
    "        for mode in (\"borderline\", \"true_positive\", \"false_negative\"):\n",
    "            base = PWF / lb / mode\n",
    "            scsv = list(base.glob(\"summary__*.csv\"))\n",
    "            meta = pd.read_csv(scsv[0]).set_index(\"idx\") if scsv else None\n",
    "            for p in sorted(base.glob(\"waterfall_patient_*.png\")):\n",
    "                kind = \"personal\"\n",
    "                m_id = re.search(r\"waterfall_patient_(\\d+)\", p.stem)\n",
    "                idx = int(m_id.group(1)) if m_id else None\n",
    "                top = read_topk_txt_if_any(p)\n",
    "                y_true = y_pred = y_prob = thr = None\n",
    "                if meta is not None and idx in meta.index:\n",
    "                    r = meta.loc[idx]\n",
    "                    y_true = int(r[\"y_true\"])\n",
    "                    y_pred = int(r[\"y_pred\"])\n",
    "                    y_prob = float(r[\"y_prob\"])\n",
    "                    thr = (\n",
    "                        float(r[\"youden_thr\"])\n",
    "                        if \"youden_thr\" in r and not pd.isna(r[\"youden_thr\"])\n",
    "                        else None\n",
    "                    )\n",
    "                rows.append(\n",
    "                    dict(\n",
    "                        source=kind,\n",
    "                        outcome=lb,\n",
    "                        cluster=\"\",\n",
    "                        case_id=idx,\n",
    "                        tag=mode,\n",
    "                        path=str(p),\n",
    "                        y_true=y_true,\n",
    "                        y_pred=y_pred,\n",
    "                        y_prob=y_prob,\n",
    "                        youden_thr=thr,\n",
    "                        top3=top,\n",
    "                    )\n",
    "                )\n",
    "    df = pd.DataFrame(rows)\n",
    "    out_csv = OUT / \"eTableS6_CaseCards.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    try:\n",
    "        xlsx = OUT / \"eTableS6_CaseCards.xlsx\"\n",
    "        with pd.ExcelWriter(xlsx) as w:\n",
    "            for g, sub in df.groupby(\"outcome\"):\n",
    "                sub.to_excel(w, sheet_name=g, index=False)\n",
    "            df.to_excel(w, sheet_name=\"All\", index=False)\n",
    "        print(f\"[OK] eTable S6 → {xlsx}\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Excel write:\", e)\n",
    "    print(f\"[OK] eTable S6 → {out_csv}\")\n",
    "\n",
    "\n",
    "# ========= 실행 =========\n",
    "if __name__ == \"__main__\":\n",
    "    # 프로젝트에서 쓰는 outcome 라벨\n",
    "    labels = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "    # Figure 8 (본문): 각 outcome당 대표 클러스터 4개 × (median/max/min)\n",
    "    build_figure8_all_labels(labels)\n",
    "\n",
    "    # Figure 9 (본문): 각 outcome당 borderline vs TP 페어 3쌍\n",
    "    build_figure9_all_labels(labels)\n",
    "\n",
    "    # Supplement: 클러스터 갤러리(eFigure S6) & 개인 워터폴(eFigure S7)\n",
    "    for lb in labels:\n",
    "        build_supp_cluster_gallery(lb, ncols=3)\n",
    "        build_supp_personal_gallery(\n",
    "            lb, ncols=3, which=(\"true_positive\", \"false_negative\", \"borderline\")\n",
    "        )\n",
    "\n",
    "    # eTable S6: 케이스 카드(이미지 경로/메타/Top3)\n",
    "    build_case_cards(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343a28e",
   "metadata": {},
   "source": [
    "## Top 2 제외 heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4253b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 경로/전역 ===============\n",
    "MODEL_DIR = str(PROJECT_ROOT / \"results/step2_modeling/251127_Final_modeling_globalunion25\")\n",
    "OUT_ROOT = (\n",
    "    Path(MODEL_DIR) / \"figures/clinic_interpretation/PatientClusters/cluster_no_top2\"\n",
    ")\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eecc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============== 유틸 ===============\n",
    "# def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "#     X = np.asarray(X, dtype=float)\n",
    "#     X = np.nan_to_num(\n",
    "#         X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "#     )\n",
    "#     return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "# def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "#     plt.savefig(figpath, **savefig_kwargs)\n",
    "#     try:\n",
    "#         plt.savefig(\n",
    "#             figpath.with_suffix(\".svg\"),\n",
    "#             bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "#         )\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# # =============== Youden 임계값 ===============\n",
    "# from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "# def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "#     for sep in [\"__\", \"___\"]:\n",
    "#         f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "#         if f.exists():\n",
    "#             try:\n",
    "#                 dfy = pd.read_csv(f)\n",
    "#                 if \"threshold\" in dfy.columns:\n",
    "#                     return float(dfy.loc[0, \"threshold\"])\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "#     fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "#     J = tpr - fpr\n",
    "#     return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# # =============== SHAP 로딩 ===============\n",
    "# def get_shap_matrix(label: str):\n",
    "#     \"\"\"\n",
    "#     Returns:\n",
    "#       expl: shap.Explanation (n, p)\n",
    "#       vals: ndarray (n, p)\n",
    "#       feat_names: list[str] length p (cleaned)\n",
    "#       y_prob: ndarray (n,)\n",
    "#       y_true: ndarray (n,)\n",
    "#       X_te_t: ndarray (n, p_after_preproc)\n",
    "#       df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "#     \"\"\"\n",
    "#     # ⚠️ 기존 프로젝트의 함수 가정\n",
    "#     expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "#     vals = np.asarray(expl.values, dtype=float)\n",
    "#     vals = np.nan_to_num(\n",
    "#         vals,\n",
    "#         nan=0.0,\n",
    "#         posinf=np.finfo(float).max / 1e6,\n",
    "#         neginf=-np.finfo(float).max / 1e6,\n",
    "#     )\n",
    "\n",
    "#     if getattr(expl, \"feature_names\", None) is not None:\n",
    "#         raw_names = list(expl.feature_names)\n",
    "#     else:\n",
    "#         raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "#     feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "#     y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "#     y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "#     return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# # =============== k-distance & eps 추정 ===============\n",
    "# def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "#     nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "#     nn.fit(X_latent)\n",
    "#     dists, _ = nn.kneighbors(X_latent)\n",
    "#     kth = np.sort(dists[:, -1])\n",
    "#     base_eps = float(np.quantile(kth, pct))\n",
    "#     return base_eps, kth\n",
    "\n",
    "\n",
    "# def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "#     plt.figure(figsize=(4, 3))\n",
    "#     plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "#     plt.xlabel(\"Points (sorted)\")\n",
    "#     plt.ylabel(f\"{k}-NN distance\")\n",
    "#     plt.title(f\"k-distance curve (k={k})\")\n",
    "#     plt.tight_layout()\n",
    "#     save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# # =============== 히트맵 ===============\n",
    "# def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "#     return (\n",
    "#         np.abs(values).mean(axis=0)\n",
    "#         if how == \"mean\"\n",
    "#         else np.mean(np.abs(values), axis=0)\n",
    "#     )\n",
    "\n",
    "\n",
    "# def plot_cluster_heatmap(\n",
    "#     vals,\n",
    "#     feat_names,\n",
    "#     labels,\n",
    "#     topN,\n",
    "#     out_path: Path,\n",
    "#     cmap: str = \"rocket\",\n",
    "#     agg: str = \"mean\",\n",
    "#     skip_top: int = 0,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     반환:\n",
    "#       {\n",
    "#         \"raw_all\":  DataFrame (clusters × all_features) : 클러스터별 원시 집계 |SHAP|\n",
    "#         \"raw_top\":  DataFrame (clusters × topN)         : 히트맵에 사용된 TopN의 원시 |SHAP|\n",
    "#         \"norm_top\": DataFrame (clusters × topN)         : TopN의 행 정규화(상대값)\n",
    "#         \"feat_top\": List[str]                           : TopN 피처명\n",
    "#         \"clusters\": np.ndarray                          : 사용된 클러스터 ID (노이즈 -1 제외)\n",
    "#         \"fig_raw_path\": Path                            : raw heatmap PNG 경로\n",
    "#         \"fig_norm_path\": Path                           : normalized heatmap PNG 경로(= out_path)\n",
    "#       }\n",
    "#     \"\"\"\n",
    "#     from textwrap import fill\n",
    "\n",
    "#     # 노이즈(-1) 제외\n",
    "#     clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "#     if not clusters:\n",
    "#         return None\n",
    "\n",
    "#     # --- (A) 클러스터별 원시 집계 |SHAP|\n",
    "#     rows = []\n",
    "#     for c in clusters:\n",
    "#         mask = labels == c\n",
    "#         rows.append(_agg_abs(vals[mask], how=agg))  # shape: (p,)\n",
    "#     M = np.vstack(rows)  # (n_clusters, n_features)\n",
    "\n",
    "#     # --- (B) 히트맵에서 쓸 TopN 선택 (클러스터 max 기준)\n",
    "#     global_imp = _agg_abs(vals, how=agg)  # 전반적 중요도 (median|SHAP| 또는 mean|SHAP|)\n",
    "#     hetero = M.max(axis=0) - M.min(axis=0)  # 간단한 이질성 점수 (max - min)\n",
    "\n",
    "#     topK_imp = max(topN * 2, 30)  # 후보 풀(전반적 중요도 상위 K)\n",
    "#     cand = np.argsort(global_imp)[::-1][:topK_imp]\n",
    "\n",
    "#     # [수정] 이질성(hetero) 순으로 정렬된 인덱스에서 앞부분(skip_top)을 건너뛰고 선택\n",
    "#     sorted_candidates = cand[np.argsort(hetero[cand])[::-1]]\n",
    "#     top_idx = sorted_candidates[skip_top : skip_top + topN]\n",
    "\n",
    "#     M_top = M[:, top_idx]\n",
    "#     feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "#     # --- (C) 행 정규화(클러스터 내부 상대값)\n",
    "#     row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "#     M_disp = M_top / row_max  # normalized topN\n",
    "\n",
    "#     # --- (D) DataFrame 구성\n",
    "#     idx_names = [f\"cluster_{c}\" for c in clusters]\n",
    "#     df_raw_all = pd.DataFrame(M, index=idx_names, columns=feat_names)\n",
    "#     df_raw_top = pd.DataFrame(M_top, index=idx_names, columns=feat_top)\n",
    "#     df_norm_top = pd.DataFrame(M_disp, index=idx_names, columns=feat_top)\n",
    "\n",
    "#     # ---------- (E) Normalized Heatmap (기존 파일명 유지) ----------\n",
    "#     xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "#     yticks = [fill(f, width=28) for f in feat_top]\n",
    "#     fig_w = max(5, 0.8 * len(clusters))\n",
    "#     fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "#     sns.heatmap(\n",
    "#         df_norm_top.T.values,\n",
    "#         cmap=cmap,\n",
    "#         cbar_kws={\"label\": \"Relative importance\"},\n",
    "#         xticklabels=xticks,\n",
    "#         yticklabels=yticks,\n",
    "#         linewidths=0.3,\n",
    "#         linecolor=\"white\",\n",
    "#         ax=ax,\n",
    "#     )\n",
    "#     ax.set_title(\n",
    "#         f\"Cluster × Feature importance ({agg} |SHAP|)\",  #  – Row-normalized\n",
    "#         fontsize=12,\n",
    "#         weight=\"bold\",\n",
    "#         pad=12,\n",
    "#     )\n",
    "#     ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "#     ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "#     ax.tick_params(\n",
    "#         axis=\"x\",\n",
    "#         labelrotation=30,\n",
    "#         labelsize=9,\n",
    "#         bottom=True,\n",
    "#         length=4,\n",
    "#         width=1,\n",
    "#         color=\"black\",\n",
    "#     )\n",
    "#     ax.tick_params(\n",
    "#         axis=\"y\", labelsize=8, pad=4, left=True, length=4, width=1, color=\"black\"\n",
    "#     )\n",
    "#     ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "#     plt.tight_layout()\n",
    "#     save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#     # ---------- (F) Raw Heatmap (새 파일) ----------\n",
    "#     out_path_raw = out_path.with_name(out_path.stem + \"__RAW.png\")\n",
    "#     fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "#     sns.heatmap(\n",
    "#         df_raw_top.T.values,\n",
    "#         cmap=cmap,\n",
    "#         cbar_kws={\"label\": f\"{agg} |SHAP| (raw)\"},\n",
    "#         xticklabels=xticks,\n",
    "#         yticklabels=yticks,\n",
    "#         linewidths=0.3,\n",
    "#         linecolor=\"white\",\n",
    "#         ax=ax,\n",
    "#     )\n",
    "#     ax.set_title(\n",
    "#         f\"Cluster × Feature importance ({agg} |SHAP|) – Raw\",\n",
    "#         fontsize=12,\n",
    "#         weight=\"bold\",\n",
    "#         pad=12,\n",
    "#     )\n",
    "#     ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "#     ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "#     ax.tick_params(\n",
    "#         axis=\"x\",\n",
    "#         labelrotation=30,\n",
    "#         labelsize=9,\n",
    "#         bottom=True,\n",
    "#         length=4,\n",
    "#         width=1,\n",
    "#         color=\"black\",\n",
    "#     )\n",
    "#     ax.tick_params(\n",
    "#         axis=\"y\", labelsize=8, pad=4, left=True, length=4, width=1, color=\"black\"\n",
    "#     )\n",
    "#     ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "#     plt.tight_layout()\n",
    "#     save_png_svg(out_path_raw, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "#     return {\n",
    "#         \"raw_all\": df_raw_all,\n",
    "#         \"raw_top\": df_raw_top,\n",
    "#         \"norm_top\": df_norm_top,\n",
    "#         \"feat_top\": feat_top,\n",
    "#         \"clusters\": np.array(clusters),\n",
    "#         \"fig_raw_path\": out_path_raw,\n",
    "#         \"fig_norm_path\": out_path,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # =============== 대표 케이스 ===============\n",
    "# def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "#     reps = []\n",
    "#     for c in np.unique(labels):\n",
    "#         if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "#             continue\n",
    "#         idx = np.where(labels == c)[0]\n",
    "#         probs = y_prob[idx]\n",
    "#         med = np.mean(probs)\n",
    "#         med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "#         max_idx = idx[np.argmax(probs)]\n",
    "#         min_idx = idx[np.argmin(probs)]\n",
    "#         reps.append(\n",
    "#             {\n",
    "#                 \"cluster\": int(c),\n",
    "#                 \"mean_idx\": int(med_idx),\n",
    "#                 \"max_idx\": int(max_idx),\n",
    "#                 \"min_idx\": int(min_idx),\n",
    "#             }\n",
    "#         )\n",
    "#     return reps\n",
    "\n",
    "\n",
    "# def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "#     reps = []\n",
    "#     for c in np.unique(labels):\n",
    "#         if c == -1:\n",
    "#             continue\n",
    "#         idx = np.where(labels == c)[0]\n",
    "#         d = df_pred.iloc[idx]\n",
    "#         rec = {\"cluster\": int(c)}\n",
    "#         fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "#         fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "#         if len(fp):\n",
    "#             rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "#         if len(fn):\n",
    "#             rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "#         reps.append(rec)\n",
    "#     return reps\n",
    "\n",
    "\n",
    "# def save_local_plots(\n",
    "#     label: str,\n",
    "#     expl: shap.Explanation,\n",
    "#     idx: int,\n",
    "#     out_dir: Path,\n",
    "#     feat_names=None,\n",
    "#     max_display=15,\n",
    "#     df_pred: pd.DataFrame | None = None,\n",
    "#     youden_thr: float | None = None,\n",
    "#     name_tag: str | None = None,\n",
    "# ):\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     y_t = y_p = None\n",
    "#     y_prob = None\n",
    "#     thr_txt = \"\"\n",
    "#     if df_pred is not None:\n",
    "#         if \"y_true\" in df_pred.columns:\n",
    "#             y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "#         if \"y_prob\" in df_pred.columns:\n",
    "#             y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "#         if \"y_pred\" in df_pred.columns:\n",
    "#             y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "#         else:\n",
    "#             thr = youden_thr if youden_thr is not None else 0.5\n",
    "#             y_p = (\n",
    "#                 int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "#                 if \"y_prob\" in df_pred.columns\n",
    "#                 else None\n",
    "#             )\n",
    "#         if youden_thr is not None:\n",
    "#             thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "#     # waterfall\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     try:\n",
    "#         shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "#     except Exception:\n",
    "#         row = expl[idx]\n",
    "#         shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "#     title_bits = [f\"{label} | id={idx}\"]\n",
    "#     if y_t is not None:\n",
    "#         title_bits.append(f\"y_true={y_t}\")\n",
    "#     if y_p is not None:\n",
    "#         title_bits.append(f\"y_pred={y_p}\")\n",
    "#     if y_prob is not None:\n",
    "#         title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "#     if name_tag:\n",
    "#         title_bits.append(f\"[{name_tag}]\")\n",
    "#     ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "#     plt.title(ttl)\n",
    "#     plt.tight_layout()\n",
    "#     save_png_svg(\n",
    "#         out_dir\n",
    "#         / (\n",
    "#             f\"{label}__waterfall_id{idx}\"\n",
    "#             + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "#             + \".png\"\n",
    "#         ),\n",
    "#         dpi=300,\n",
    "#         bbox_inches=\"tight\",\n",
    "#     )\n",
    "\n",
    "#     # force (옵션)\n",
    "#     try:\n",
    "#         plt.figure(figsize=(7, 2.8))\n",
    "#         shap.plots.force(\n",
    "#             expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "#         )\n",
    "#         plt.tight_layout()\n",
    "#         save_png_svg(\n",
    "#             out_dir\n",
    "#             / (\n",
    "#                 f\"{label}__force_id{idx}\"\n",
    "#                 + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "#                 + \".png\"\n",
    "#             ),\n",
    "#             dpi=300,\n",
    "#         )\n",
    "#     except Exception:\n",
    "#         plt.close()\n",
    "\n",
    "\n",
    "# # =============== DBSCAN 통계/요약 ===============\n",
    "# def add_dbscan_stats(\n",
    "#     out_dir: Path,\n",
    "#     label: str,\n",
    "#     y_true: np.ndarray,\n",
    "#     y_prob: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "# ):\n",
    "#     lab = np.asarray(labels)\n",
    "#     clusters = [c for c in np.unique(lab) if c != -1]\n",
    "#     rows = []\n",
    "#     for c in clusters:\n",
    "#         m = lab == c\n",
    "#         rows.append(\n",
    "#             dict(\n",
    "#                 cluster=int(c),\n",
    "#                 n=int(m.sum()),\n",
    "#                 event_rate=float(np.mean(y_true[m] == 1)),\n",
    "#                 mean_y_prob=float(np.mean(y_prob[m])),\n",
    "#             )\n",
    "#         )\n",
    "#     df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "#     chi_p = np.nan\n",
    "#     kw_p = np.nan\n",
    "#     if len(clusters) >= 2:\n",
    "#         table = [\n",
    "#             [\n",
    "#                 int(np.sum((lab == c) & (y_true == 1))),\n",
    "#                 int(np.sum((lab == c) & (y_true == 0))),\n",
    "#             ]\n",
    "#             for c in clusters\n",
    "#         ]\n",
    "#         chi_p = chi2_contingency(table)[1]\n",
    "#         groups = [y_prob[lab == c] for c in clusters]\n",
    "#         kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "#     df[\"p_event_rate(chi2)\"] = chi_p\n",
    "#     df[\"p_yprob(kruskal)\"] = kw_p\n",
    "#     df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # =============== 메인 루틴 ===============\n",
    "# def run_cluster_analysis_for_label(\n",
    "#     label: str,\n",
    "#     umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "#     umap_n_neighbors: int = 12,\n",
    "#     umap_min_dist: float = 0.01,\n",
    "#     umap_metric: str = \"euclidean\",  # cosine\n",
    "#     min_samples: int = 6,  # DBSCAN\n",
    "#     eps: float | None = None,\n",
    "#     dbscan_metric: str = \"euclidean\",\n",
    "#     target_min_clusters: int = 2,\n",
    "#     max_noise_ratio: float = 0.45,\n",
    "#     max_trials: int = 5,\n",
    "#     topN: int = 10,\n",
    "#     agg_for_heatmap: str = \"mean\",\n",
    "#     skip_top: int = 0,\n",
    "#     save_representatives: bool = True,\n",
    "#     random_state: int = 42,\n",
    "# ):\n",
    "#     assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "#     print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "#     out_dir = OUT_ROOT / label\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # 0) 데이터 로드\n",
    "#     expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "#     # Youden 임계값 & df_pred 보정\n",
    "#     ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "#     if ythr is None:\n",
    "#         ythr = _compute_youden_from_preds(\n",
    "#             np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "#         )\n",
    "#     if \"y_true\" not in df_pred.columns:\n",
    "#         df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "#     if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "#         df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "#     # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "#     # V = _standardize(vals) # 표준화\n",
    "#     V = vals\n",
    "#     um_hi = UMAP.UMAP(\n",
    "#         n_components=umap_n_components,\n",
    "#         n_neighbors=umap_n_neighbors,\n",
    "#         min_dist=umap_min_dist,\n",
    "#         metric=umap_metric,\n",
    "#         random_state=random_state,\n",
    "#     )\n",
    "#     X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "#     # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "#     if eps is None:\n",
    "#         eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.9)  # 0.85\n",
    "#         eps = eps0\n",
    "#         save_kdist_plot(\n",
    "#             kth,\n",
    "#             k=min_samples,\n",
    "#             out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "#         )\n",
    "\n",
    "#     # 3) DBSCAN 적응형 루프\n",
    "#     trial = 0\n",
    "#     labels = None\n",
    "#     info_hist = []\n",
    "#     cur_min_samples = int(min_samples)\n",
    "#     cur_eps = float(eps)\n",
    "\n",
    "#     while trial < max_trials:\n",
    "#         db = DBSCAN(\n",
    "#             eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "#         )\n",
    "#         labels = db.fit_predict(X_hi)  # X_hi\n",
    "#         noise_ratio = float(np.mean(labels == -1))\n",
    "#         n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "#         info_hist.append(\n",
    "#             dict(\n",
    "#                 trial=trial,\n",
    "#                 eps=cur_eps,\n",
    "#                 min_samples=cur_min_samples,\n",
    "#                 n_clusters=n_clusters,\n",
    "#                 noise_ratio=noise_ratio,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "#             break\n",
    "\n",
    "#         # 조정 규칙\n",
    "#         if noise_ratio > max_noise_ratio:\n",
    "#             cur_eps *= 1.20\n",
    "#             if cur_min_samples > 3:\n",
    "#                 cur_min_samples -= 1\n",
    "#         elif n_clusters < target_min_clusters:\n",
    "#             cur_eps *= 0.88\n",
    "#         else:\n",
    "#             cur_eps *= 1.05\n",
    "\n",
    "#         trial += 1\n",
    "\n",
    "#     # all-noise 대비: 마지막 완화 시도 1회\n",
    "#     if (labels is None) or np.all(labels == -1):\n",
    "#         cur_eps *= 1.30\n",
    "#         db = DBSCAN(\n",
    "#             eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "#         )\n",
    "#         labels = db.fit_predict(X_hi)  # X_hi\n",
    "#         noise_ratio = float(np.mean(labels == -1))\n",
    "#         n_clusters = int(len(set(labels) - {-1}))\n",
    "#         info_hist.append(\n",
    "#             dict(\n",
    "#                 trial=\"final_relax\",\n",
    "#                 eps=cur_eps,\n",
    "#                 min_samples=cur_min_samples,\n",
    "#                 n_clusters=n_clusters,\n",
    "#                 noise_ratio=noise_ratio,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "#     if umap_n_components == 2:\n",
    "#         Z2 = X_hi\n",
    "#     else:\n",
    "#         um_2d = UMAP.UMAP(\n",
    "#             n_components=2,\n",
    "#             n_neighbors=max(umap_n_neighbors, 15),\n",
    "#             min_dist=max(umap_min_dist, 0.05),\n",
    "#             metric=umap_metric,\n",
    "#             random_state=random_state,\n",
    "#         )\n",
    "#         Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "#     # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "#     m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "#     m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "#     # 6) 2D 플롯 (클러스터 색)\n",
    "#     uniq = sorted(np.unique(labels))\n",
    "#     # 클러스터 개수가 많을 수 있으므로 tab10 사용 권장\n",
    "#     cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "#     # 범례 공간 확보를 위해 가로 사이즈를 약간 늘림\n",
    "#     plt.figure(figsize=(8.5, 6.0))\n",
    "\n",
    "#     # 1. 노이즈(-1) 먼저 그리기 (배경으로 깔리게 처리)\n",
    "#     if -1 in uniq:\n",
    "#         mask = labels == -1\n",
    "#         plt.scatter(\n",
    "#             Z2[mask, 0],\n",
    "#             Z2[mask, 1],\n",
    "#             c=\"#e0e0e0\",  # 아주 연한 회색\n",
    "#             edgecolor=\"#bbbbbb\",  # 테두리는 조금 진하게\n",
    "#             linewidth=0.1,\n",
    "#             s=15,  # 크기는 작게\n",
    "#             alpha=0.4,  # 투명하게\n",
    "#             label=\"Noise\",  # 범례 이름\n",
    "#             zorder=1,  # 가장 뒤쪽에 배치\n",
    "#         )\n",
    "\n",
    "#     # 2. 실제 클러스터 그리기 Loop\n",
    "#     for c in uniq:\n",
    "#         if c == -1:\n",
    "#             continue\n",
    "#         mask = labels == c\n",
    "#         color = cmap(c % 20)  # 색상 순환 적용\n",
    "\n",
    "#         plt.scatter(\n",
    "#             Z2[mask, 0],\n",
    "#             Z2[mask, 1],\n",
    "#             c=[color],  # 단일 색상 적용\n",
    "#             s=22,  # 클러스터 포인트는 조금 더 크게\n",
    "#             alpha=0.9,  # 진하게\n",
    "#             label=f\"Cluster {c}\",\n",
    "#             edgecolor=\"white\",  # 포인트 구분감\n",
    "#             linewidth=0.3,\n",
    "#             zorder=2,  # 노이즈보다 앞쪽에 배치\n",
    "#         )\n",
    "\n",
    "#     title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "#     # 제목에 전체 대비 노이즈 비율 표기\n",
    "#     n_noise = np.sum(labels == -1)\n",
    "#     n_total = len(labels)\n",
    "#     plt.title(\n",
    "#         f\"{title}\\nTotal: {n_total}, Clusters: {len(set(labels)-{-1})}, Noise: {n_noise} ({n_noise/n_total:.1%})\",\n",
    "#         fontsize=11,\n",
    "#     )\n",
    "#     plt.xlabel(\"UMAP-1\")\n",
    "#     plt.ylabel(\"UMAP-2\")\n",
    "\n",
    "#     # 범례 설정 (그래프 영역 밖 우측 상단에 배치)\n",
    "#     plt.legend(\n",
    "#         bbox_to_anchor=(1.02, 1),\n",
    "#         loc=\"upper left\",\n",
    "#         borderaxespad=0,\n",
    "#         frameon=False,\n",
    "#         fontsize=9,\n",
    "#         markerscale=1.5,  # 범례의 점 크기 키우기\n",
    "#     )\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     save_png_svg(\n",
    "#         out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "#     )\n",
    "\n",
    "#     # 6-2) 2D 플롯 (위험도 색)\n",
    "#     plt.figure(figsize=(7.2, 6.2))\n",
    "#     sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "#     plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "#     plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "#     plt.xlabel(\"UMAP-1\")\n",
    "#     plt.ylabel(\"UMAP-2\")\n",
    "#     plt.margins(0.02)\n",
    "#     save_png_svg(\n",
    "#         out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "#     )\n",
    "\n",
    "#     # 7) 클러스터 통계/요약 + 히트맵\n",
    "#     df_stats = add_dbscan_stats(\n",
    "#         out_dir,\n",
    "#         label,\n",
    "#         y_true=np.asarray(y_true),\n",
    "#         y_prob=np.asarray(y_prob),\n",
    "#         labels=np.asarray(labels),\n",
    "#     )\n",
    "#     ret = plot_cluster_heatmap(\n",
    "#         vals,\n",
    "#         feat_names,\n",
    "#         labels,\n",
    "#         topN=topN,\n",
    "#         out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "#         cmap=\"magma_r\",  # rocket_r, viridis_r\n",
    "#         agg=agg_for_heatmap,\n",
    "#         skip_top=skip_top,\n",
    "#     )\n",
    "\n",
    "#     # === 수치 저장(요청 파일명 규칙) ===\n",
    "#     if ret is not None:\n",
    "#         # 모든 피처의 원시 |SHAP| (클러스터 × 전체 피처)\n",
    "#         ret[\"raw_all\"].to_csv(\n",
    "#             out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__ALL_FEATURES.csv\",\n",
    "#             index=True,\n",
    "#         )\n",
    "#         # 히트맵에 쓰인 TopN의 원시 |SHAP|\n",
    "#         ret[\"raw_top\"].to_csv(\n",
    "#             out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_RAW.csv\",\n",
    "#             index=True,\n",
    "#         )\n",
    "#         # 히트맵에 쓰인 TopN의 행 정규화(상대값)\n",
    "#         ret[\"norm_top\"].to_csv(\n",
    "#             out_dir\n",
    "#             / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_ROWNORM.csv\",\n",
    "#             index=True,\n",
    "#         )\n",
    "\n",
    "#     # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "#     if save_representatives:\n",
    "#         rep_dir = out_dir / \"representatives_dbscan\"\n",
    "#         # mean/max/min\n",
    "#         reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "#         for rep in reps:\n",
    "#             c = rep[\"cluster\"]\n",
    "#             for tag in [\"mean_idx\", \"max_idx\", \"min_idx\"]:\n",
    "#                 i = rep[tag]\n",
    "#                 name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "#                 save_local_plots(\n",
    "#                     label,\n",
    "#                     expl,\n",
    "#                     i,\n",
    "#                     rep_dir / f\"cluster_{c}\",\n",
    "#                     feat_names=feat_names,\n",
    "#                     max_display=15,\n",
    "#                     df_pred=df_pred,\n",
    "#                     youden_thr=ythr,\n",
    "#                     name_tag=name_tag,\n",
    "#                 )\n",
    "#         # FP/FN\n",
    "#         err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "#         for rep in err_reps:\n",
    "#             c = rep[\"cluster\"]\n",
    "#             for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "#                 if tag in rep:\n",
    "#                     i = rep[tag]\n",
    "#                     name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "#                     save_local_plots(\n",
    "#                         label,\n",
    "#                         expl,\n",
    "#                         i,\n",
    "#                         rep_dir / f\"cluster_{c}\",\n",
    "#                         feat_names=feat_names,\n",
    "#                         max_display=15,\n",
    "#                         df_pred=df_pred,\n",
    "#                         youden_thr=ythr,\n",
    "#                         name_tag=name_tag,\n",
    "#                     )\n",
    "\n",
    "#     # 9) 메타(JSON)\n",
    "#     meta = dict(\n",
    "#         timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#         algo=\"dbscan_tuned\",\n",
    "#         random_state=random_state,\n",
    "#         umap=dict(\n",
    "#             n_components=umap_n_components,\n",
    "#             n_neighbors=umap_n_neighbors,\n",
    "#             min_dist=umap_min_dist,\n",
    "#             metric=umap_metric,\n",
    "#         ),\n",
    "#         final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "#         trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "#         n_clusters=int(len(set(labels) - {-1})),\n",
    "#         noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "#         history=info_hist,\n",
    "#     )\n",
    "#     with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "#         json.dump(meta, f, indent=2)\n",
    "\n",
    "#     print(f\"-> saved: {out_dir}\")\n",
    "#     return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796568b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_for_ppt(\n",
    "    Z2: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    y_true: np.ndarray,  # [NEW] 실제 정답 레이블 추가\n",
    "    out_path: Path,\n",
    "    title: str,\n",
    "):\n",
    "\n",
    "    # plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    # --- [설정] Risk 판단 기준 (Event Rate 기준) ---\n",
    "    def get_risk_style(event_rate):\n",
    "        # Event Rate는 보통 확률보다 낮으므로 기준을 조금 조정할 수도 있음 (현행 유지)\n",
    "        if event_rate < 0.1:\n",
    "            return \"Low\", \"#4575b4\"\n",
    "        elif event_rate < 0.3:\n",
    "            return \"Moderate\", \"#7b3294\"\n",
    "        elif event_rate < 0.5:\n",
    "            return \"High\", \"#f1a340\"\n",
    "        else:\n",
    "            return \"Very High\", \"#d73027\"\n",
    "\n",
    "    uniq = sorted(np.unique(labels))\n",
    "    clusters = [c for c in uniq if c != -1]\n",
    "    n_clusters = len(clusters)\n",
    "\n",
    "    global_center = np.mean(Z2, axis=0)\n",
    "    cluster_cmap = plt.get_cmap(\"tab20\" if n_clusters > 10 else \"tab10\")\n",
    "\n",
    "    centroids = {}\n",
    "    for c in clusters:\n",
    "        centroids[c] = np.mean(Z2[labels == c], axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.scatter(\n",
    "        Z2[:, 0], Z2[:, 1], c=\"#e0e0e0\", s=20, alpha=0.4, edgecolors=\"none\", zorder=0\n",
    "    )\n",
    "\n",
    "    label_items = []\n",
    "    legend_handles = []\n",
    "\n",
    "    for i, c in enumerate(clusters):\n",
    "        mask = labels == c\n",
    "        points = Z2[mask]\n",
    "\n",
    "        # [변경] 실제 Event Rate 계산\n",
    "        # y_true가 0/1로 되어 있다고 가정 (1=Event)\n",
    "        true_vals = y_true[mask]\n",
    "        event_rate = np.mean(true_vals == 1)\n",
    "\n",
    "        risk_name, risk_color = get_risk_style(event_rate)  # Event Rate 기준 스타일\n",
    "        cluster_color = cluster_cmap(i % 20)\n",
    "\n",
    "        handle = mpatches.Patch(color=cluster_color, label=f\"Cluster {c}\")\n",
    "        legend_handles.append(handle)\n",
    "\n",
    "        if len(points) < 3:\n",
    "            ax.scatter(\n",
    "                points[:, 0], points[:, 1], c=cluster_color, s=20, alpha=0.8, zorder=2\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        ax.scatter(\n",
    "            points[:, 0],\n",
    "            points[:, 1],\n",
    "            c=[cluster_color],\n",
    "            s=20,\n",
    "            alpha=0.8,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.2,\n",
    "            zorder=2,\n",
    "        )\n",
    "        hull = ConvexHull(points)\n",
    "        hull_coords = points[np.append(hull.vertices, hull.vertices[0])]\n",
    "        ax.plot(\n",
    "            hull_coords[:, 0],\n",
    "            hull_coords[:, 1],\n",
    "            color=cluster_color,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.2,\n",
    "            alpha=0.8,\n",
    "            zorder=3,\n",
    "        )\n",
    "        ax.add_patch(\n",
    "            mpatches.Polygon(\n",
    "                hull_coords,\n",
    "                closed=True,\n",
    "                fc=cluster_color,\n",
    "                ec=None,\n",
    "                alpha=0.08,\n",
    "                zorder=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- 위치 계산 ---\n",
    "        current_centroid = centroids[c]\n",
    "        vec_radial = current_centroid - global_center\n",
    "        vec_repulsion = np.array([0.0, 0.0])\n",
    "        crowding = 0\n",
    "\n",
    "        for other_c, other_cent in centroids.items():\n",
    "            if c == other_c:\n",
    "                continue\n",
    "            diff = current_centroid - other_cent\n",
    "            dist = np.linalg.norm(diff)\n",
    "            if dist > 1e-4:\n",
    "                w = 1.0 / (dist**2.0)\n",
    "                vec_repulsion += (diff / dist) * w\n",
    "                crowding += w\n",
    "\n",
    "        final_vec = vec_radial + (vec_repulsion * 4.0)\n",
    "        if crowding > 0.5 or np.linalg.norm(vec_radial) < 1.0:\n",
    "            final_vec[1] += (1.0 if i % 2 == 0 else -1.0) * 3.0\n",
    "\n",
    "        norm = np.linalg.norm(final_vec)\n",
    "        unit_vec = final_vec / norm if norm > 1e-4 else np.array([0, 1])\n",
    "\n",
    "        hull_points = points[hull.vertices]\n",
    "        anchor_point = hull_points[np.argmax(np.dot(hull_points, unit_vec))]\n",
    "\n",
    "        offset = 2.5 + min(crowding, 3.5)\n",
    "        text_pos = anchor_point + (unit_vec * offset)\n",
    "\n",
    "        label_items.append(\n",
    "            {\n",
    "                \"text_pos\": text_pos,\n",
    "                \"anchor_point\": anchor_point,\n",
    "                # [변경] 라벨 텍스트에 Event Rate 표시\n",
    "                \"text_str\": f\"{risk_name}\\n(Event:{event_rate:.2f})\",\n",
    "                \"color\": risk_color,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --- 물리 엔진 ---\n",
    "    iterations = 50\n",
    "    min_dist = 3.0\n",
    "    for _ in range(iterations):\n",
    "        for i in range(len(label_items)):\n",
    "            for j in range(i + 1, len(label_items)):\n",
    "                p1 = label_items[i][\"text_pos\"]\n",
    "                p2 = label_items[j][\"text_pos\"]\n",
    "                diff = p1 - p2\n",
    "                dist = np.linalg.norm(diff)\n",
    "                if dist < min_dist:\n",
    "                    if dist < 1e-4:\n",
    "                        force = np.array([0.1, 0.1])\n",
    "                    else:\n",
    "                        force = (diff / dist) * (min_dist - dist) * 0.5\n",
    "                    label_items[i][\"text_pos\"] += force\n",
    "                    label_items[j][\"text_pos\"] -= force\n",
    "\n",
    "    # --- 축 확장 ---\n",
    "    all_x = list(Z2[:, 0]) + [item[\"text_pos\"][0] for item in label_items]\n",
    "    all_y = list(Z2[:, 1]) + [item[\"text_pos\"][1] for item in label_items]\n",
    "    margin_x = (max(all_x) - min(all_x)) * 0.1\n",
    "    margin_y = (max(all_y) - min(all_y)) * 0.1\n",
    "    ax.set_xlim(min(all_x) - margin_x, max(all_x) + margin_x)\n",
    "    ax.set_ylim(min(all_y) - margin_y, max(all_y) + margin_y)\n",
    "\n",
    "    # --- Annotation 그리기 ---\n",
    "    for item in label_items:\n",
    "        bbox_props = dict(\n",
    "            boxstyle=\"round,pad=0.2\", fc=\"white\", ec=item[\"color\"], lw=1.5, alpha=0.95\n",
    "        )\n",
    "\n",
    "        ax.annotate(\n",
    "            text=item[\"text_str\"],\n",
    "            xy=(item[\"anchor_point\"][0], item[\"anchor_point\"][1]),\n",
    "            xytext=(item[\"text_pos\"][0], item[\"text_pos\"][1]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=14,\n",
    "            weight=\"bold\",\n",
    "            color=item[\"color\"],\n",
    "            bbox=bbox_props,\n",
    "            arrowprops=dict(\n",
    "                arrowstyle=\"-\",\n",
    "                color=\"black\",\n",
    "                linewidth=0.8,\n",
    "                shrinkB=4,\n",
    "                connectionstyle=\"arc3,rad=0.1\",\n",
    "            ),\n",
    "            zorder=10,\n",
    "        )\n",
    "\n",
    "    # 범례 및 타이틀\n",
    "    if -1 in uniq:\n",
    "        legend_handles.insert(0, mpatches.Patch(color=\"#e0e0e0\", label=\"Noise\"))\n",
    "\n",
    "    ax.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Clusters\",\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=\"upper left\",\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    noise_pct = (labels == -1).sum() / len(labels) if len(labels) > 0 else 0\n",
    "    full_title = f\"{title}\\n(Noise ratio={noise_pct:.1%})\"\n",
    "\n",
    "    ax.set_title(full_title, fontsize=14, weight=\"bold\", pad=20)\n",
    "    ax.set_xlabel(\"UMAP-1\", fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel(\"UMAP-2\", fontsize=12, labelpad=10)\n",
    "    # ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# def plot_umap_with_hulls_and_labels(\n",
    "#     Z2: np.ndarray,\n",
    "#     labels: np.ndarray,\n",
    "#     y_prob: np.ndarray,\n",
    "#     out_path: Path,\n",
    "#     title: str,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     [Ultimate Version with Legend]\n",
    "#     - 우측 상단에 클러스터 범례(Cluster 0, 1...)와 Noise 범례 추가\n",
    "#     - 물리 엔진(Simulation) 기반 라벨 겹침 방지\n",
    "#     - 축 자동 확장 및 스마트 배치\n",
    "#     \"\"\"\n",
    "#     import matplotlib.cm as cm\n",
    "#     import matplotlib.patches as mpatches\n",
    "#     from scipy.spatial import ConvexHull\n",
    "\n",
    "#     # --- 설정 ---\n",
    "#     def get_risk_style(mean_prob):\n",
    "#         if mean_prob < 0.1: return \"Low\", \"#4575b4\"\n",
    "#         elif mean_prob < 0.3: return \"Moderate\", \"#7b3294\"\n",
    "#         elif mean_prob < 0.5: return \"High\", \"#f1a340\"\n",
    "#         else: return \"Very High\", \"#d73027\"\n",
    "\n",
    "#     uniq = sorted(np.unique(labels))\n",
    "#     clusters = [c for c in uniq if c != -1]\n",
    "#     n_clusters = len(clusters)\n",
    "\n",
    "#     global_center = np.mean(Z2, axis=0)\n",
    "#     cluster_cmap = plt.get_cmap(\"tab20\" if n_clusters > 10 else \"tab10\")\n",
    "\n",
    "#     centroids = {}\n",
    "#     for c in clusters:\n",
    "#         centroids[c] = np.mean(Z2[labels == c], axis=0)\n",
    "\n",
    "#     # 1. 캔버스 설정 (범례 공간 확보를 위해 가로폭 넉넉하게)\n",
    "#     fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "#     # 배경 산점도\n",
    "#     ax.scatter(Z2[:, 0], Z2[:, 1], c='#e0e0e0', s=20, alpha=0.4, edgecolors='none', zorder=0)\n",
    "\n",
    "#     # -------------------------------------------------------\n",
    "#     # [Step 1] 데이터 수집, 초기 위치, 범례 핸들 생성\n",
    "#     # -------------------------------------------------------\n",
    "#     label_items = []\n",
    "#     legend_handles = [] # 범례용 핸들 리스트\n",
    "\n",
    "#     # 노이즈가 있으면 범례에 먼저 추가\n",
    "#     if -1 in uniq:\n",
    "#         legend_handles.append(mpatches.Patch(color='#e0e0e0', label='Noise'))\n",
    "\n",
    "#     for i, c in enumerate(clusters):\n",
    "#         mask = (labels == c)\n",
    "#         points = Z2[mask]\n",
    "#         probs = y_prob[mask]\n",
    "\n",
    "#         mean_p = np.mean(probs)\n",
    "#         risk_name, risk_color = get_risk_style(mean_p)\n",
    "#         cluster_color = cluster_cmap(i % 20)\n",
    "\n",
    "#         # 범례에 추가 (Cluster ID와 색상 매핑)\n",
    "#         legend_handles.append(mpatches.Patch(color=cluster_color, label=f'Cluster {c}'))\n",
    "\n",
    "#         if len(points) < 3:\n",
    "#             ax.scatter(points[:, 0], points[:, 1], c=cluster_color, s=20, alpha=0.8, zorder=2)\n",
    "#             continue\n",
    "\n",
    "#         # Hull 그리기\n",
    "#         ax.scatter(points[:, 0], points[:, 1], c=[cluster_color], s=20, alpha=0.8, edgecolors='white', linewidth=0.2, zorder=2)\n",
    "#         hull = ConvexHull(points)\n",
    "#         hull_coords = points[np.append(hull.vertices, hull.vertices[0])]\n",
    "#         ax.plot(hull_coords[:, 0], hull_coords[:, 1], color=cluster_color, linestyle='--', linewidth=1.2, alpha=0.8, zorder=3)\n",
    "#         ax.add_patch(mpatches.Polygon(hull_coords, closed=True, fc=cluster_color, ec=None, alpha=0.08, zorder=1))\n",
    "\n",
    "#         # --- 초기 위치 계산 ---\n",
    "#         current_centroid = centroids[c]\n",
    "\n",
    "#         vec_radial = current_centroid - global_center\n",
    "#         vec_repulsion = np.array([0.0, 0.0])\n",
    "#         crowding = 0\n",
    "\n",
    "#         for other_c, other_cent in centroids.items():\n",
    "#             if c == other_c: continue\n",
    "#             diff = current_centroid - other_cent\n",
    "#             dist = np.linalg.norm(diff)\n",
    "#             if dist > 1e-4:\n",
    "#                 w = 1.0 / (dist**2.0)\n",
    "#                 vec_repulsion += (diff / dist) * w\n",
    "#                 crowding += w\n",
    "\n",
    "#         final_vec = vec_radial + (vec_repulsion * 4.0)\n",
    "#         if crowding > 0.5 or np.linalg.norm(vec_radial) < 1.0:\n",
    "#             final_vec[1] += (1.0 if i % 2 == 0 else -1.0) * 3.0\n",
    "\n",
    "#         norm = np.linalg.norm(final_vec)\n",
    "#         unit_vec = final_vec / norm if norm > 1e-4 else np.array([0, 1])\n",
    "\n",
    "#         hull_points = points[hull.vertices]\n",
    "#         anchor_point = hull_points[np.argmax(np.dot(hull_points, unit_vec))]\n",
    "\n",
    "#         offset = 2.5 + min(crowding, 3.5) # 2.0, 3.0\n",
    "#         text_pos = anchor_point + (unit_vec * offset)\n",
    "\n",
    "#         label_items.append({\n",
    "#             'text_pos': text_pos,\n",
    "#             'anchor_point': anchor_point,\n",
    "#             'text_str': f\"{risk_name}\\n(Risk:{mean_p:.2f})\",\n",
    "#             'color': risk_color,\n",
    "#         })\n",
    "\n",
    "#     # -------------------------------------------------------\n",
    "#     # [Step 2] 물리 엔진: 라벨 겹침 해결\n",
    "#     # -------------------------------------------------------\n",
    "#     iterations = 50\n",
    "#     min_dist = 3.0 # 2.5\n",
    "\n",
    "#     for _ in range(iterations):\n",
    "#         for i in range(len(label_items)):\n",
    "#             for j in range(i + 1, len(label_items)):\n",
    "#                 p1 = label_items[i]['text_pos']\n",
    "#                 p2 = label_items[j]['text_pos']\n",
    "\n",
    "#                 diff = p1 - p2\n",
    "#                 dist = np.linalg.norm(diff)\n",
    "\n",
    "#                 if dist < min_dist:\n",
    "#                     if dist < 1e-4:\n",
    "#                         force = np.array([0.1, 0.1])\n",
    "#                     else:\n",
    "#                         force = (diff / dist) * (min_dist - dist) * 0.5\n",
    "\n",
    "#                     label_items[i]['text_pos'] += force\n",
    "#                     label_items[j]['text_pos'] -= force\n",
    "\n",
    "#     # -------------------------------------------------------\n",
    "#     # [Step 3] 최종 그리기 & 축 확장\n",
    "#     # -------------------------------------------------------\n",
    "#     all_x = list(Z2[:, 0]) + [item['text_pos'][0] for item in label_items]\n",
    "#     all_y = list(Z2[:, 1]) + [item['text_pos'][1] for item in label_items]\n",
    "\n",
    "#     margin_x = (max(all_x) - min(all_x)) * 0.1\n",
    "#     margin_y = (max(all_y) - min(all_y)) * 0.1\n",
    "\n",
    "#     ax.set_xlim(min(all_x) - margin_x, max(all_x) + margin_x)\n",
    "#     ax.set_ylim(min(all_y) - margin_y, max(all_y) + margin_y)\n",
    "\n",
    "#     # Annotation 그리기\n",
    "#     for item in label_items:\n",
    "#         bbox_props = dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=item['color'], lw=1.5, alpha=0.95)\n",
    "\n",
    "#         ax.annotate(\n",
    "#             text=item['text_str'],\n",
    "#             xy=(item['anchor_point'][0], item['anchor_point'][1]),\n",
    "#             xytext=(item['text_pos'][0], item['text_pos'][1]),\n",
    "#             ha='center', va='center',\n",
    "#             fontsize=14, weight='bold',\n",
    "#             color=item['color'],\n",
    "#             bbox=bbox_props,\n",
    "#             arrowprops=dict(\n",
    "#                 arrowstyle=\"-\", color=\"black\", linewidth=0.8, shrinkB=4,\n",
    "#                 connectionstyle=\"arc3,rad=0.1\"\n",
    "#             ),\n",
    "#             zorder=10\n",
    "#         )\n",
    "\n",
    "#     # [NEW] 범례 표시 (Legend)\n",
    "#     # bbox_to_anchor로 그래프 영역 바깥 우측 상단에 배치\n",
    "#     ax.legend(\n",
    "#         handles=legend_handles,\n",
    "#         title=\"Clusters\",\n",
    "#         bbox_to_anchor=(1.02, 1),\n",
    "#         loc='upper left',\n",
    "#         borderaxespad=0,\n",
    "#         frameon=False,\n",
    "#         fontsize=11,\n",
    "#         title_fontsize=12\n",
    "#     )\n",
    "\n",
    "#     # 마무리\n",
    "#     noise_pct = (labels == -1).sum() / len(labels) if len(labels) > 0 else 0\n",
    "#     full_title = f\"{title}\\n(Noise ratio={noise_pct:.1%})\"\n",
    "\n",
    "#     ax.set_title(full_title, fontsize=14, weight='bold', pad=20)\n",
    "#     ax.set_xlabel(\"UMAP-1\")\n",
    "#     ax.set_ylabel(\"UMAP-2\")\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 유틸 ===============\n",
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.nan_to_num(\n",
    "        X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "    )\n",
    "    return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "    plt.savefig(figpath, **savefig_kwargs)\n",
    "    try:\n",
    "        plt.savefig(\n",
    "            figpath.with_suffix(\".svg\"),\n",
    "            bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============== Youden 임계값 ===============\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# =============== SHAP 로딩 ===============\n",
    "def get_shap_matrix(label: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      expl: shap.Explanation (n, p)\n",
    "      vals: ndarray (n, p)\n",
    "      feat_names: list[str] length p (cleaned)\n",
    "      y_prob: ndarray (n,)\n",
    "      y_true: ndarray (n,)\n",
    "      X_te_t: ndarray (n, p_after_preproc)\n",
    "      df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "    \"\"\"\n",
    "    # ⚠️ 기존 프로젝트의 함수 가정\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    vals = np.asarray(expl.values, dtype=float)\n",
    "    vals = np.nan_to_num(\n",
    "        vals,\n",
    "        nan=0.0,\n",
    "        posinf=np.finfo(float).max / 1e6,\n",
    "        neginf=-np.finfo(float).max / 1e6,\n",
    "    )\n",
    "\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        raw_names = list(expl.feature_names)\n",
    "    else:\n",
    "        raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "    feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "    y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "    y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "    return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# =============== k-distance & eps 추정 ===============\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps, kth\n",
    "\n",
    "\n",
    "def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "    plt.xlabel(\"Points (sorted)\")\n",
    "    plt.ylabel(f\"{k}-NN distance\")\n",
    "    plt.title(f\"k-distance curve (k={k})\")\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 히트맵 ===============\n",
    "def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "    return (\n",
    "        np.abs(values).mean(axis=0)\n",
    "        if how == \"mean\"\n",
    "        else np.mean(np.abs(values), axis=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cluster_heatmap(\n",
    "    vals,\n",
    "    feat_names,\n",
    "    labels,\n",
    "    topN,\n",
    "    out_path: Path,\n",
    "    cmap: str = \"rocket\",\n",
    "    agg: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      {\n",
    "        \"raw_all\":  DataFrame (clusters × all_features) : 클러스터별 원시 집계 |SHAP|\n",
    "        \"raw_top\":  DataFrame (clusters × topN)         : 히트맵에 사용된 TopN의 원시 |SHAP|\n",
    "        \"norm_top\": DataFrame (clusters × topN)         : TopN의 행 정규화(상대값)\n",
    "        \"feat_top\": List[str]                           : TopN 피처명\n",
    "        \"clusters\": np.ndarray                          : 사용된 클러스터 ID (노이즈 -1 제외)\n",
    "        \"fig_raw_path\": Path                            : raw heatmap PNG 경로\n",
    "        \"fig_norm_path\": Path                           : normalized heatmap PNG 경로(= out_path)\n",
    "      }\n",
    "    \"\"\"\n",
    "    from textwrap import fill\n",
    "\n",
    "    # 노이즈(-1) 제외\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "    if not clusters:\n",
    "        return None\n",
    "\n",
    "    # --- (A) 클러스터별 원시 집계 |SHAP|\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        rows.append(_agg_abs(vals[mask], how=agg))  # shape: (p,)\n",
    "    M = np.vstack(rows)  # (n_clusters, n_features)\n",
    "\n",
    "    # --- (B) 히트맵에서 쓸 TopN 선택 (클러스터 max 기준)\n",
    "    global_imp = _agg_abs(vals, how=agg)  # 전반적 중요도 (median|SHAP| 또는 mean|SHAP|)\n",
    "    hetero = M.max(axis=0) - M.min(axis=0)  # 간단한 이질성 점수 (max - min)\n",
    "\n",
    "    topK_imp = max(topN * 2, 30)  # 후보 풀(전반적 중요도 상위 K)\n",
    "    cand = np.argsort(global_imp)[::-1][:topK_imp]\n",
    "    top_idx = cand[\n",
    "        np.argsort(hetero[cand])[::-1][:topN]\n",
    "    ]  # 후보 안에서 이질성 큰 순 TopN\n",
    "\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    ###########################################\n",
    "    # global_imp = _agg_abs(vals, how=agg)  # shape (p,)  # median(|SHAP|)\n",
    "    # top_idx = np.argsort(global_imp)[::-1][:topN]\n",
    "\n",
    "    # M_top = M[:, top_idx]\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "    ###########################################\n",
    "    # top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    # M_top = M[:, top_idx]  # raw topN\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    # --- (C) 행 정규화(클러스터 내부 상대값)\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max  # normalized topN\n",
    "\n",
    "    # --- (D) DataFrame 구성\n",
    "    idx_names = [f\"cluster_{c}\" for c in clusters]\n",
    "    df_raw_all = pd.DataFrame(M, index=idx_names, columns=feat_names)\n",
    "    df_raw_top = pd.DataFrame(M_top, index=idx_names, columns=feat_top)\n",
    "    df_norm_top = pd.DataFrame(M_disp, index=idx_names, columns=feat_top)\n",
    "\n",
    "    # ---------- (E) Normalized Heatmap (기존 파일명 유지) ----------\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_norm_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",  #  – Row-normalized\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # ---------- (F) Raw Heatmap (새 파일) ----------\n",
    "    out_path_raw = out_path.with_name(out_path.stem + \"__RAW.png\")\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_raw_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": f\"{agg} |SHAP|\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path_raw, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return {\n",
    "        \"raw_all\": df_raw_all,\n",
    "        \"raw_top\": df_raw_top,\n",
    "        \"norm_top\": df_norm_top,\n",
    "        \"feat_top\": feat_top,\n",
    "        \"clusters\": np.array(clusters),\n",
    "        \"fig_raw_path\": out_path_raw,\n",
    "        \"fig_norm_path\": out_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============== 대표 케이스 ===============\n",
    "def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        probs = y_prob[idx]\n",
    "        med = np.mean(probs)\n",
    "        med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "        max_idx = idx[np.argmax(probs)]\n",
    "        min_idx = idx[np.argmin(probs)]\n",
    "        reps.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"mean_idx\": int(med_idx),\n",
    "                \"max_idx\": int(max_idx),\n",
    "                \"min_idx\": int(min_idx),\n",
    "            }\n",
    "        )\n",
    "    return reps\n",
    "\n",
    "\n",
    "def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        d = df_pred.iloc[idx]\n",
    "        rec = {\"cluster\": int(c)}\n",
    "        fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "        fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "        if len(fp):\n",
    "            rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "        if len(fn):\n",
    "            rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "        reps.append(rec)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def save_local_plots(\n",
    "    label: str,\n",
    "    expl: shap.Explanation,\n",
    "    idx: int,\n",
    "    out_dir: Path,\n",
    "    feat_names=None,\n",
    "    max_display=15,\n",
    "    df_pred: pd.DataFrame | None = None,\n",
    "    youden_thr: float | None = None,\n",
    "    name_tag: str | None = None,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    y_t = y_p = None\n",
    "    y_prob = None\n",
    "    thr_txt = \"\"\n",
    "    if df_pred is not None:\n",
    "        if \"y_true\" in df_pred.columns:\n",
    "            y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "        if \"y_prob\" in df_pred.columns:\n",
    "            y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "        if \"y_pred\" in df_pred.columns:\n",
    "            y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "        else:\n",
    "            thr = youden_thr if youden_thr is not None else 0.5\n",
    "            y_p = (\n",
    "                int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "                if \"y_prob\" in df_pred.columns\n",
    "                else None\n",
    "            )\n",
    "        if youden_thr is not None:\n",
    "            thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "    # waterfall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "    except Exception:\n",
    "        row = expl[idx]\n",
    "        shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "    title_bits = [f\"{label} | id={idx}\"]\n",
    "    if y_t is not None:\n",
    "        title_bits.append(f\"y_true={y_t}\")\n",
    "    if y_p is not None:\n",
    "        title_bits.append(f\"y_pred={y_p}\")\n",
    "    if y_prob is not None:\n",
    "        title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "    if name_tag:\n",
    "        title_bits.append(f\"[{name_tag}]\")\n",
    "    ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "    plt.title(ttl)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(\n",
    "        out_dir\n",
    "        / (\n",
    "            f\"{label}__waterfall_id{idx}\"\n",
    "            + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "            + \".png\"\n",
    "        ),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # force (옵션)\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 2.8))\n",
    "        shap.plots.force(\n",
    "            expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_png_svg(\n",
    "            out_dir\n",
    "            / (\n",
    "                f\"{label}__force_id{idx}\"\n",
    "                + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "                + \".png\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "    except Exception:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============== DBSCAN 통계/요약 ===============\n",
    "def add_dbscan_stats(\n",
    "    out_dir: Path,\n",
    "    label: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "):\n",
    "    lab = np.asarray(labels)\n",
    "    clusters = [c for c in np.unique(lab) if c != -1]\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        m = lab == c\n",
    "        rows.append(\n",
    "            dict(\n",
    "                cluster=int(c),\n",
    "                n=int(m.sum()),\n",
    "                event_rate=float(np.mean(y_true[m] == 1)),\n",
    "                mean_y_prob=float(np.mean(y_prob[m])),\n",
    "            )\n",
    "        )\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    chi_p = np.nan\n",
    "    kw_p = np.nan\n",
    "    if len(clusters) >= 2:\n",
    "        table = [\n",
    "            [\n",
    "                int(np.sum((lab == c) & (y_true == 1))),\n",
    "                int(np.sum((lab == c) & (y_true == 0))),\n",
    "            ]\n",
    "            for c in clusters\n",
    "        ]\n",
    "        chi_p = chi2_contingency(table)[1]\n",
    "        groups = [y_prob[lab == c] for c in clusters]\n",
    "        kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "    df[\"p_event_rate(chi2)\"] = chi_p\n",
    "    df[\"p_yprob(kruskal)\"] = kw_p\n",
    "    df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 메인 루틴 ===============\n",
    "\n",
    "\n",
    "def run_cluster_analysis_for_label(\n",
    "    label: str,\n",
    "    umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"euclidean\",  # cosine\n",
    "    min_samples: int = 6,  # DBSCAN\n",
    "    eps: float | None = None,\n",
    "    dbscan_metric: str = \"euclidean\",\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "    topN: int = 10,\n",
    "    agg_for_heatmap: str = \"mean\",\n",
    "    save_representatives: bool = True,\n",
    "    random_state: int = 42,\n",
    "    drop_top_n_features: int = 0,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "    out_dir = OUT_ROOT / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 0) 데이터 로드\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # [NEW] Top N Feature 제거 로직\n",
    "    if drop_top_n_features > 0:\n",
    "        print(f\"-> Filtering out Top {drop_top_n_features} SHAP features...\")\n",
    "\n",
    "        # 1. 각 변수의 절대값 평균(|SHAP|) 계산\n",
    "        mean_abs_shap = np.mean(np.abs(vals), axis=0)\n",
    "\n",
    "        # 2. 중요도가 높은 순서대로 인덱스 정렬\n",
    "        top_indices = np.argsort(mean_abs_shap)[::-1][:drop_top_n_features]\n",
    "\n",
    "        # 3. 제거될 변수 이름 출력 (확인용)\n",
    "        dropped_names = [feat_names[i] for i in top_indices]\n",
    "        print(f\"   Dropped: {dropped_names}\")\n",
    "\n",
    "        # 4. 해당 변수들을 vals와 feat_names에서 제거\n",
    "        # (남길 인덱스: 전체 인덱스 집합 - Top 인덱스 집합)\n",
    "        all_indices = np.arange(vals.shape[1])\n",
    "        keep_indices = np.setdiff1d(all_indices, top_indices)\n",
    "\n",
    "        # 원래 순서 유지하면서 필터링\n",
    "        keep_indices = np.sort(keep_indices)\n",
    "\n",
    "        vals = vals[:, keep_indices]  # SHAP 값 행렬 업데이트\n",
    "        feat_names = [feat_names[i] for i in keep_indices]  # 변수명 리스트 업데이트\n",
    "\n",
    "        print(f\"   Remaining features: {vals.shape[1]}\")\n",
    "\n",
    "    # Youden 임계값 & df_pred 보정\n",
    "    ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "    if ythr is None:\n",
    "        ythr = _compute_youden_from_preds(\n",
    "            np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "        )\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "    # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "    # V = _standardize(vals) # 표준화\n",
    "    V = vals\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "    if eps is None:\n",
    "        eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.9)  # 0.85\n",
    "        eps = eps0\n",
    "        save_kdist_plot(\n",
    "            kth,\n",
    "            k=min_samples,\n",
    "            out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "        )\n",
    "\n",
    "    # 3) DBSCAN 적응형 루프\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "    cur_min_samples = int(min_samples)\n",
    "    cur_eps = float(eps)\n",
    "\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            cur_eps *= 1.20\n",
    "            if cur_min_samples > 3:\n",
    "                cur_min_samples -= 1\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            cur_eps *= 0.88\n",
    "        else:\n",
    "            cur_eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # all-noise 대비: 마지막 완화 시도 1회\n",
    "    if (labels is None) or np.all(labels == -1):\n",
    "        cur_eps *= 1.30\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=\"final_relax\",\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "    if umap_n_components == 2:\n",
    "        Z2 = X_hi\n",
    "    else:\n",
    "        um_2d = UMAP.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=max(umap_n_neighbors, 15),\n",
    "            min_dist=max(umap_min_dist, 0.05),\n",
    "            metric=umap_metric,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    # [NEW] 위험도 순으로 라벨 재정렬 (이 부분 추가!)\n",
    "    # Cluster 0 = Low Risk, Cluster N = High Risk\n",
    "    if labels is not None:\n",
    "        print(\"-> Reordering clusters by risk (0: Low -> N: High)...\")\n",
    "        labels = reorder_labels_by_risk(labels, np.asarray(y_true))\n",
    "\n",
    "    # 5) 라벨 매핑 저장 (이제 정렬된 labels가 저장됨)\n",
    "    m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "    # m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    # m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # # ⭐️⭐️⭐️⭐️기존 코드 6) 2D 플롯 (클러스터 색) ⭐️⭐️⭐️⭐️\n",
    "    # uniq = sorted(np.unique(labels))\n",
    "    # # 클러스터 개수가 많을 수 있으므로 tab10 사용 권장\n",
    "    # cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "    # # 범례 공간 확보를 위해 가로 사이즈를 약간 늘림\n",
    "    # plt.figure(figsize=(8.5, 6.0))\n",
    "\n",
    "    # # 1. 노이즈(-1) 먼저 그리기 (배경으로 깔리게 처리)\n",
    "    # if -1 in uniq:\n",
    "    #     mask = labels == -1\n",
    "    #     plt.scatter(\n",
    "    #         Z2[mask, 0],\n",
    "    #         Z2[mask, 1],\n",
    "    #         c=\"#e0e0e0\",  # 아주 연한 회색\n",
    "    #         edgecolor=\"#bbbbbb\",  # 테두리는 조금 진하게\n",
    "    #         linewidth=0.1,\n",
    "    #         s=15,  # 크기는 작게\n",
    "    #         alpha=0.4,  # 투명하게\n",
    "    #         label=\"Noise\",  # 범례 이름\n",
    "    #         zorder=1,  # 가장 뒤쪽에 배치\n",
    "    #     )\n",
    "\n",
    "    # # 2. 실제 클러스터 그리기 Loop\n",
    "    # for c in uniq:\n",
    "    #     if c == -1:\n",
    "    #         continue\n",
    "    #     mask = labels == c\n",
    "    #     color = cmap(c % 20)  # 색상 순환 적용\n",
    "\n",
    "    #     plt.scatter(\n",
    "    #         Z2[mask, 0],\n",
    "    #         Z2[mask, 1],\n",
    "    #         c=[color],  # 단일 색상 적용\n",
    "    #         s=22,  # 클러스터 포인트는 조금 더 크게\n",
    "    #         alpha=0.9,  # 진하게\n",
    "    #         label=f\"Cluster {c}\",\n",
    "    #         edgecolor=\"white\",  # 포인트 구분감\n",
    "    #         linewidth=0.3,\n",
    "    #         zorder=2,  # 노이즈보다 앞쪽에 배치\n",
    "    #     )\n",
    "\n",
    "    # title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "    # # 제목에 전체 대비 노이즈 비율 표기\n",
    "    # n_noise = np.sum(labels == -1)\n",
    "    # n_total = len(labels)\n",
    "    # plt.title(\n",
    "    #     f\"{title}\\nTotal: {n_total}, Clusters: {len(set(labels)-{-1})}, Noise: {n_noise} ({n_noise/n_total:.1%})\",\n",
    "    #     fontsize=11,\n",
    "    # )\n",
    "    # plt.xlabel(\"UMAP-1\")\n",
    "    # plt.ylabel(\"UMAP-2\")\n",
    "\n",
    "    # # 범례 설정 (그래프 영역 밖 우측 상단에 배치)\n",
    "    # plt.legend(\n",
    "    #     bbox_to_anchor=(1.02, 1),\n",
    "    #     loc=\"upper left\",\n",
    "    #     borderaxespad=0,\n",
    "    #     frameon=False,\n",
    "    #     fontsize=9,\n",
    "    #     markerscale=1.5,  # 범례의 점 크기 키우기\n",
    "    # )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # save_png_svg(\n",
    "    #     out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    # )\n",
    "\n",
    "    # 6) 2D 플롯 (Hull + Annotation 적용 버전)\n",
    "    # print(\"-> Plotting UMAP with Threshold-based Risk Labels...\")\n",
    "    # plot_umap_with_hulls_and_labels(\n",
    "    #     Z2=Z2,\n",
    "    #     labels=labels,\n",
    "    #     y_prob=y_prob,  # [중요] 여기에 y_prob를 반드시 넘겨주세요!\n",
    "    #     out_path=out_dir / f\"{label}__UMAP2D_DBSCAN_annotated.png\",\n",
    "    #     title=f\"Risk Patterns ({label})\",\n",
    "    # )\n",
    "\n",
    "    print(\"-> Plotting UMAP for PPT (Event Rate based)...\")\n",
    "    plot_umap_for_ppt(\n",
    "        Z2=Z2,\n",
    "        labels=labels,\n",
    "        y_prob=y_prob,  # 기존 인자\n",
    "        y_true=y_true,  # [NEW] 실제 정답값 전달\n",
    "        out_path=out_dir / f\"{label}__UMAP2D_DBSCAN_annotated_ppt.png\",\n",
    "        title=f\"Risk Patterns ({label})\",\n",
    "    )\n",
    "\n",
    "    # 6-2) 2D 플롯 (위험도 색)\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 7) 클러스터 통계/요약 + 히트맵\n",
    "    df_stats = add_dbscan_stats(\n",
    "        out_dir,\n",
    "        label,\n",
    "        y_true=np.asarray(y_true),\n",
    "        y_prob=np.asarray(y_prob),\n",
    "        labels=np.asarray(labels),\n",
    "    )\n",
    "    ret = plot_cluster_heatmap(\n",
    "        vals,\n",
    "        feat_names,\n",
    "        labels,\n",
    "        topN=topN,\n",
    "        out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "        cmap=\"magma_r\",  # rocket_r, viridis_r\n",
    "        agg=agg_for_heatmap,\n",
    "    )\n",
    "\n",
    "    # === 수치 저장(요청 파일명 규칙) ===\n",
    "    if ret is not None:\n",
    "        # 모든 피처의 원시 |SHAP| (클러스터 × 전체 피처)\n",
    "        ret[\"raw_all\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__ALL_FEATURES.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 원시 |SHAP|\n",
    "        ret[\"raw_top\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_RAW.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 행 정규화(상대값)\n",
    "        ret[\"norm_top\"].to_csv(\n",
    "            out_dir\n",
    "            / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_ROWNORM.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "    # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "    if save_representatives:\n",
    "        rep_dir = out_dir / \"representatives_dbscan\"\n",
    "        # mean/max/min\n",
    "        reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "        for rep in reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"mean_idx\", \"max_idx\", \"min_idx\"]:\n",
    "                i = rep[tag]\n",
    "                name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                save_local_plots(\n",
    "                    label,\n",
    "                    expl,\n",
    "                    i,\n",
    "                    rep_dir / f\"cluster_{c}\",\n",
    "                    feat_names=feat_names,\n",
    "                    max_display=15,\n",
    "                    df_pred=df_pred,\n",
    "                    youden_thr=ythr,\n",
    "                    name_tag=name_tag,\n",
    "                )\n",
    "        # FP/FN\n",
    "        err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "        for rep in err_reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "                if tag in rep:\n",
    "                    i = rep[tag]\n",
    "                    name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                    save_local_plots(\n",
    "                        label,\n",
    "                        expl,\n",
    "                        i,\n",
    "                        rep_dir / f\"cluster_{c}\",\n",
    "                        feat_names=feat_names,\n",
    "                        max_display=15,\n",
    "                        df_pred=df_pred,\n",
    "                        youden_thr=ythr,\n",
    "                        name_tag=name_tag,\n",
    "                    )\n",
    "\n",
    "    # 9) 메타(JSON)\n",
    "    meta = dict(\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        algo=\"dbscan_tuned\",\n",
    "        random_state=random_state,\n",
    "        umap=dict(\n",
    "            n_components=umap_n_components,\n",
    "            n_neighbors=umap_n_neighbors,\n",
    "            min_dist=umap_min_dist,\n",
    "            metric=umap_metric,\n",
    "        ),\n",
    "        final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "        trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "        n_clusters=int(len(set(labels) - {-1})),\n",
    "        noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "        history=info_hist,\n",
    "    )\n",
    "    with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"-> saved: {out_dir}\")\n",
    "    return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d27a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 전체 라벨 반복 실행 ===============\n",
    "all_cluster_summaries = []\n",
    "for lb in LABELS:\n",
    "    try:\n",
    "        df_sum, labels = run_cluster_analysis_for_label(\n",
    "            lb,\n",
    "            umap_n_components=2,\n",
    "            umap_n_neighbors=45,  # 40\n",
    "            umap_min_dist=0.0,  # 0.05, 0.01\n",
    "            umap_metric=\"euclidean\",  # cosine\n",
    "            min_samples=17,  # 20\n",
    "            eps=None,\n",
    "            dbscan_metric=\"euclidean\",\n",
    "            target_min_clusters=3,\n",
    "            max_noise_ratio=0.45,  # 0.45\n",
    "            max_trials=5,\n",
    "            topN=10,\n",
    "            agg_for_heatmap=\"mean\",\n",
    "            save_representatives=True,\n",
    "            random_state=42,\n",
    "            drop_top_n_features=2,\n",
    "        )\n",
    "        df_sum.insert(0, \"label\", lb)\n",
    "        all_cluster_summaries.append(df_sum)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {lb}: {e}\")\n",
    "\n",
    "if all_cluster_summaries:\n",
    "    all_summ = pd.concat(all_cluster_summaries, ignore_index=True)\n",
    "    out_csv = OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.csv\"\n",
    "    all_summ.to_csv(out_csv, index=False)\n",
    "    print(\"✅ saved:\", out_csv)\n",
    "    # Excel(라벨별 시트) 옵션\n",
    "    try:\n",
    "        with pd.ExcelWriter(\n",
    "            OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\"\n",
    "        ) as xw:\n",
    "            for lb, df_ in all_summ.groupby(\"label\"):\n",
    "                df_.to_excel(xw, sheet_name=lb, index=False)\n",
    "        print(\"✅ saved:\", OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Excel save:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6523948",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}