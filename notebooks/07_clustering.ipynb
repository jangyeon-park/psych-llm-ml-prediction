{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.config import PROJECT_ROOT, MODEL_SEED, LABELS\n",
    "from src.variables import LLM_COLS, LAB_COLS, CODE_COLS, CATEGORY_COLS\n",
    "from src.feature_utils import (\n",
    "    find_feature_list_file, read_feature_list, load_xy,\n",
    "    load_feature_name_map, prettify_name, prettify_names,\n",
    "    clean_ct_feature_name, inject_psych_scale_aliases, prettify_psych_name,\n",
    "    FRIENDLY_OVERRIDES, LEVEL_MAP, to_bayes_space\n",
    ")\n",
    "from src.models import get_models_and_search_space\n",
    "from src.preprocessing import create_preprocessor\n",
    "from src.evaluation import youden_threshold, metrics_at_threshold, decision_curve\n",
    "\n",
    "import os, re, joblib, json, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    precision_recall_fscore_support, accuracy_score,\n",
    "    roc_curve, confusion_matrix,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from upsetplot import from_contents, plot\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update(\n",
    "    {\"figure.dpi\": 130, \"axes.spines.top\": False, \"axes.spines.right\": False}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 0) 경로/조합 설정\n",
    "# ---------------------------\n",
    "OUT_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMP_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"data/processed_imp/260114_split_corr_LLM_ADER/imputation/simple_imput\")\n",
    ")\n",
    "FS_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/Feature_Selection/simple_20/step2_FS\")\n",
    ")\n",
    "\n",
    "# Train/Test 파일 이름\n",
    "DATA_FILES = {\n",
    "    \"label_30d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_30d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_30d_test.csv\",\n",
    "    },\n",
    "    \"label_60d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_60d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_60d_test.csv\",\n",
    "    },\n",
    "    \"label_90d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_90d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_90d_test.csv\",\n",
    "    },\n",
    "    \"label_180d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_180d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_180d_test.csv\",\n",
    "    },\n",
    "    \"label_365d\": {\n",
    "        \"train\": IMP_DIR / \"simple_label_365d_train.csv\",\n",
    "        \"test\": IMP_DIR / \"simple_label_365d_test.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# 당신의 탐색 결과를 반영한 권장조합 (필요시 수정)\n",
    "BEST_COMBOS = {\n",
    "    \"label_30d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_60d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_90d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_180d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "    \"label_365d\": {\"model\": \"rf\", \"feature_set\": \"All_Features\"},\n",
    "}\n",
    "\n",
    "TARGET_COL_FALLBACK = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f584ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_MAP_CSV = str(PROJECT_ROOT / \"results/new_analysis/260106/Feature Selection/simple_20/feature_summary.csv\")\n",
    "NAME_MAP = load_feature_name_map(FEATURE_MAP_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda691d6",
   "metadata": {},
   "source": [
    "## UMAP + DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125ad28",
   "metadata": {},
   "source": [
    "## ⭐️⭐️⭐️ version 3 ⭐️⭐️⭐️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8945a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import chi2_contingency, kruskal\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "from adjustText import adjust_text\n",
    "\n",
    "try:\n",
    "    import umap.umap_ as UMAP\n",
    "\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e59fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 경로/전역 ===============\n",
    "MODEL_DIR = str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20\")\n",
    "OUT_ROOT = Path(MODEL_DIR) / \"figures/clinic_interpretation/PatientClusters\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ALIAS = {\n",
    "    \"Impaired_Social_Function\": \"Social Function Impairment (LLM)\",\n",
    "    \"Religious_Affiliation\": \"Religious Affiliation (LLM)\",\n",
    "    \"Violence_and_Impulsivity\": \"Aggression/Impulsivity (LLM)\",\n",
    "    \"Domestic_Violence\": \"Domestic Violence (LLM)\",\n",
    "    \"Physical_Abuse\": \"Physical Abuse (LLM)\",\n",
    "    \"Divorce\": \"Divorce Experience (LLM)\",\n",
    "    \"Death_of_Family_Member\": \"Family Loss (LLM)\",\n",
    "    \"Emotional_Abuse\": \"Emotional Abuse (LLM)\",\n",
    "    \"Lack_of_Family_Support\": \"Lack of Family Support (LLM)\",\n",
    "    \"Social_Isolation_and_Lack_of_Support\": \"Social Isolation (LLM)\",\n",
    "    \"Psychotic_Symptoms\": \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "    \"Interpersonal_Conflict\": \"Interpersonal Conflict (LLM)\",\n",
    "    \"Exposure_to_Suicide\": \"Suicide Exposure (LLM)\",\n",
    "    \"Alcohol_Use_Problems\": \"Alcohol Use Issues (LLM)\",\n",
    "    \"Sexual_Abuse\": \"Sexual Victimization (LLM)\",\n",
    "    \"Physical_and_Emotional_Neglect\": \"Neglect (LLM)\",\n",
    "}\n",
    "\n",
    "def harmonize_columns_for_pipeline(X, pipe):\n",
    "    \"\"\"\n",
    "    파이프라인의 전처리기가 기대하는 컬럼 구성과 순서로 데이터프레임을 재정렬하는 함수\n",
    "    \"\"\"\n",
    "    # 보통 파이프라인의 첫 번째 스텝(전처리기)에서 피처 이름을 가져옵니다.\n",
    "    try:\n",
    "        expected_cols = pipe.named_steps[\"preprocessor\"].feature_names_in_\n",
    "    except AttributeError:\n",
    "        # 피처 이름이 저장되지 않은 경우 등에 대한 예외 처리\n",
    "        return X\n",
    "\n",
    "    # 누락된 컬럼은 0으로 채우고, 순서를 맞춤\n",
    "    for col in expected_cols:\n",
    "        if col not in X.columns:\n",
    "            X[col] = 0\n",
    "\n",
    "    return X[expected_cols]\n",
    "\n",
    "def harmonize_with_alias(X, pipe, alias_map=LLM_ALIAS, fill_value=0):\n",
    "    X = X.copy()\n",
    "\n",
    "    # 1) alias 컬럼이 있으면, 파이프라인이 기대하는 \"원본키\" 컬럼로 복사 생성\n",
    "    for raw_key, pretty_name in alias_map.items():\n",
    "        if (raw_key not in X.columns) and (pretty_name in X.columns):\n",
    "            X[raw_key] = X[pretty_name]\n",
    "\n",
    "    # 2) 기존 harmonize (없는 컬럼 생성/정렬 등)\n",
    "    X = harmonize_columns_for_pipeline(X, pipe)\n",
    "\n",
    "    return X\n",
    "\n",
    "def shap_on_test(label: str, max_bg=200, force_tree_interventional=True):\n",
    "    pipe = load_best_pipeline(label)\n",
    "    preproc = pipe.named_steps[\"preprocessor\"]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "\n",
    "    # ✅ 이거 반드시 필요\n",
    "    X_raw, y_true, df_pred = load_train_test(label)\n",
    "\n",
    "    # ✅ alias -> raw_key 복구 후 harmonize\n",
    "    X_raw_h = harmonize_with_alias(X_raw, pipe)\n",
    "\n",
    "    X_te_t = preproc.transform(X_raw_h)\n",
    "    feat_names = load_feature_names_transformed(label)\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    bg_idx = rng.choice(X_te_t.shape[0], size=min(max_bg, X_te_t.shape[0]), replace=False)\n",
    "    X_bg_t = X_te_t[bg_idx]\n",
    "\n",
    "    try:\n",
    "        if force_tree_interventional:\n",
    "            explainer = shap.TreeExplainer(\n",
    "                clf,\n",
    "                data=X_bg_t,\n",
    "                feature_perturbation=\"interventional\",\n",
    "                model_output=\"probability\",\n",
    "            )\n",
    "        else:\n",
    "            explainer = shap.Explainer(clf, X_bg_t)\n",
    "        expl = explainer(X_te_t)\n",
    "    except Exception:\n",
    "        expl = shap.Explainer(clf, X_bg_t)(X_te_t)\n",
    "\n",
    "    vals = np.array(expl.values)\n",
    "    if vals.ndim == 3 and vals.shape[2] == 2:\n",
    "        expl = expl[:, :, 1]\n",
    "\n",
    "    try:\n",
    "        expl.feature_names = feat_names\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return expl, X_te_t, y_true, df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.nan_to_num(\n",
    "        X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "    )\n",
    "    return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "    plt.savefig(figpath, **savefig_kwargs)\n",
    "    try:\n",
    "        plt.savefig(\n",
    "            figpath.with_suffix(\".svg\"),\n",
    "            bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def reorder_labels_by_risk(labels: np.ndarray, y_true: np.ndarray):\n",
    "    \"\"\"\n",
    "    클러스터 라벨을 '실제 발생률(Event Rate)' 오름차순으로 재정렬합니다.\n",
    "    - Cluster 0: 발생률이 가장 낮은 그룹\n",
    "    - Cluster N: 발생률이 가장 높은 그룹\n",
    "    - Cluster -1: 노이즈 (변경 없음)\n",
    "    \"\"\"\n",
    "    # 노이즈 제외한 유니크 라벨\n",
    "    unique_labels = [c for c in np.unique(labels) if c != -1]\n",
    "\n",
    "    if not unique_labels:\n",
    "        return labels\n",
    "\n",
    "    # (구 라벨, Event Rate) 리스트 생성\n",
    "    risk_scores = []\n",
    "    print(\"\\n[Event Rate Reordering Check]\")\n",
    "    for c in unique_labels:\n",
    "        # 해당 클러스터의 실제 발생률 계산 (Mean of y_true)\n",
    "        # y_true가 0/1 이진값이라 가정하면 평균이 곧 발생률입니다.\n",
    "        event_rate = np.mean(y_true[labels == c])\n",
    "        risk_scores.append((c, event_rate))\n",
    "        print(f\" - Original Cluster {c}: Event Rate = {event_rate:.4f}\")\n",
    "\n",
    "    # Event Rate 기준 오름차순 정렬 (낮음 -> 높음)\n",
    "    risk_scores.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 매핑 딕셔너리 생성\n",
    "    print(\"-> Mapping Result:\")\n",
    "    mapping = {-1: -1}\n",
    "    for new_idx, (old_label, rate) in enumerate(risk_scores):\n",
    "        mapping[old_label] = new_idx\n",
    "        print(f\"   Old {old_label} (Rate {rate:.4f}) -> New {new_idx} (Low to High)\")\n",
    "\n",
    "    # 새로운 라벨 적용\n",
    "    new_labels = np.array([mapping[x] for x in labels])\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# =============== Youden 임계값 ===============\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# =============== SHAP 로딩 ===============\n",
    "def get_shap_matrix(label: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      expl: shap.Explanation (n, p)\n",
    "      vals: ndarray (n, p)\n",
    "      feat_names: list[str] length p (cleaned)\n",
    "      y_prob: ndarray (n,)\n",
    "      y_true: ndarray (n,)\n",
    "      X_te_t: ndarray (n, p_after_preproc)\n",
    "      df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "    \"\"\"\n",
    "    # ⚠️ 기존 프로젝트의 함수 가정\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    vals = np.asarray(expl.values, dtype=float)\n",
    "    vals = np.nan_to_num(\n",
    "        vals,\n",
    "        nan=0.0,\n",
    "        posinf=np.finfo(float).max / 1e6,\n",
    "        neginf=-np.finfo(float).max / 1e6,\n",
    "    )\n",
    "\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        raw_names = list(expl.feature_names)\n",
    "    else:\n",
    "        raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "    feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "    y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "    y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "    return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# =============== k-distance & eps 추정 ===============\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps, kth\n",
    "\n",
    "\n",
    "def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "    plt.xlabel(\"Points (sorted)\")\n",
    "    plt.ylabel(f\"{k}-NN distance\")\n",
    "    plt.title(f\"k-distance curve (k={k})\")\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 히트맵 ===============\n",
    "def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "    return (\n",
    "        np.abs(values).mean(axis=0)\n",
    "        if how == \"mean\"\n",
    "        else np.mean(np.abs(values), axis=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cluster_heatmap(\n",
    "    vals,\n",
    "    feat_names,\n",
    "    labels,\n",
    "    topN,\n",
    "    out_path: Path,\n",
    "    cmap: str = \"rocket\",\n",
    "    agg: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      {\n",
    "        \"raw_all\":  DataFrame (clusters × all_features) : 클러스터별 원시 집계 |SHAP|\n",
    "        \"raw_top\":  DataFrame (clusters × topN)         : 히트맵에 사용된 TopN의 원시 |SHAP|\n",
    "        \"norm_top\": DataFrame (clusters × topN)         : TopN의 행 정규화(상대값)\n",
    "        \"feat_top\": List[str]                           : TopN 피처명\n",
    "        \"clusters\": np.ndarray                          : 사용된 클러스터 ID (노이즈 -1 제외)\n",
    "        \"fig_raw_path\": Path                            : raw heatmap PNG 경로\n",
    "        \"fig_norm_path\": Path                           : normalized heatmap PNG 경로(= out_path)\n",
    "      }\n",
    "    \"\"\"\n",
    "    from textwrap import fill\n",
    "\n",
    "    # 노이즈(-1) 제외\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "    if not clusters:\n",
    "        return None\n",
    "\n",
    "    # --- (A) 클러스터별 원시 집계 |SHAP|\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        rows.append(_agg_abs(vals[mask], how=agg))  # shape: (p,)\n",
    "    M = np.vstack(rows)  # (n_clusters, n_features)\n",
    "\n",
    "    # --- (B) 히트맵에서 쓸 TopN 선택 (클러스터 max 기준)\n",
    "    global_imp = _agg_abs(vals, how=agg)  # 전반적 중요도 (median|SHAP| 또는 mean|SHAP|)\n",
    "    hetero = M.max(axis=0) - M.min(axis=0)  # 간단한 이질성 점수 (max - min)\n",
    "\n",
    "    topK_imp = max(topN * 2, 30)  # 후보 풀(전반적 중요도 상위 K)\n",
    "    cand = np.argsort(global_imp)[::-1][:topK_imp]\n",
    "    top_idx = cand[\n",
    "        np.argsort(hetero[cand])[::-1][:topN]\n",
    "    ]  # 후보 안에서 이질성 큰 순 TopN\n",
    "\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    ###########################################\n",
    "    # global_imp = _agg_abs(vals, how=agg)  # shape (p,)  # median(|SHAP|)\n",
    "    # top_idx = np.argsort(global_imp)[::-1][:topN]\n",
    "\n",
    "    # M_top = M[:, top_idx]\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "    ###########################################\n",
    "    # top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    # M_top = M[:, top_idx]  # raw topN\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    # --- (C) 행 정규화(클러스터 내부 상대값)\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max  # normalized topN\n",
    "\n",
    "    # --- (D) DataFrame 구성\n",
    "    idx_names = [f\"cluster_{c}\" for c in clusters]\n",
    "    df_raw_all = pd.DataFrame(M, index=idx_names, columns=feat_names)\n",
    "    df_raw_top = pd.DataFrame(M_top, index=idx_names, columns=feat_top)\n",
    "    df_norm_top = pd.DataFrame(M_disp, index=idx_names, columns=feat_top)\n",
    "\n",
    "    # ---------- (E) Normalized Heatmap (기존 파일명 유지) ----------\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_norm_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",  #  – Row-normalized\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # ---------- (F) Raw Heatmap (새 파일) ----------\n",
    "    out_path_raw = out_path.with_name(out_path.stem + \"__RAW.png\")\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_raw_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": f\"{agg} |SHAP| (raw)\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|) – Raw\",\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path_raw, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return {\n",
    "        \"raw_all\": df_raw_all,\n",
    "        \"raw_top\": df_raw_top,\n",
    "        \"norm_top\": df_norm_top,\n",
    "        \"feat_top\": feat_top,\n",
    "        \"clusters\": np.array(clusters),\n",
    "        \"fig_raw_path\": out_path_raw,\n",
    "        \"fig_norm_path\": out_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============== 대표 케이스 ===============\n",
    "def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        probs = y_prob[idx]\n",
    "        med = np.mean(probs)\n",
    "        med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "        max_idx = idx[np.argmax(probs)]\n",
    "        min_idx = idx[np.argmin(probs)]\n",
    "        reps.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"mean_idx\": int(med_idx),\n",
    "                \"max_idx\": int(max_idx),\n",
    "                \"min_idx\": int(min_idx),\n",
    "            }\n",
    "        )\n",
    "    return reps\n",
    "\n",
    "\n",
    "def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        d = df_pred.iloc[idx]\n",
    "        rec = {\"cluster\": int(c)}\n",
    "        fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "        fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "        if len(fp):\n",
    "            rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "        if len(fn):\n",
    "            rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "        reps.append(rec)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def save_local_plots(\n",
    "    label: str,\n",
    "    expl: shap.Explanation,\n",
    "    idx: int,\n",
    "    out_dir: Path,\n",
    "    feat_names=None,\n",
    "    max_display=15,\n",
    "    df_pred: pd.DataFrame | None = None,\n",
    "    youden_thr: float | None = None,\n",
    "    name_tag: str | None = None,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    y_t = y_p = None\n",
    "    y_prob = None\n",
    "    thr_txt = \"\"\n",
    "    if df_pred is not None:\n",
    "        if \"y_true\" in df_pred.columns:\n",
    "            y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "        if \"y_prob\" in df_pred.columns:\n",
    "            y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "        if \"y_pred\" in df_pred.columns:\n",
    "            y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "        else:\n",
    "            thr = youden_thr if youden_thr is not None else 0.5\n",
    "            y_p = (\n",
    "                int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "                if \"y_prob\" in df_pred.columns\n",
    "                else None\n",
    "            )\n",
    "        if youden_thr is not None:\n",
    "            thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "    # waterfall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "    except Exception:\n",
    "        row = expl[idx]\n",
    "        shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "    title_bits = [f\"{label} | id={idx}\"]\n",
    "    if y_t is not None:\n",
    "        title_bits.append(f\"y_true={y_t}\")\n",
    "    if y_p is not None:\n",
    "        title_bits.append(f\"y_pred={y_p}\")\n",
    "    if y_prob is not None:\n",
    "        title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "    if name_tag:\n",
    "        title_bits.append(f\"[{name_tag}]\")\n",
    "    ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "    plt.title(ttl)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(\n",
    "        out_dir\n",
    "        / (\n",
    "            f\"{label}__waterfall_id{idx}\"\n",
    "            + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "            + \".png\"\n",
    "        ),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # force (옵션)\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 2.8))\n",
    "        shap.plots.force(\n",
    "            expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_png_svg(\n",
    "            out_dir\n",
    "            / (\n",
    "                f\"{label}__force_id{idx}\"\n",
    "                + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "                + \".png\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "    except Exception:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============== DBSCAN 통계/요약 ===============\n",
    "def add_dbscan_stats(\n",
    "    out_dir: Path,\n",
    "    label: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "):\n",
    "    lab = np.asarray(labels)\n",
    "    clusters = [c for c in np.unique(lab) if c != -1]\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        m = lab == c\n",
    "        rows.append(\n",
    "            dict(\n",
    "                cluster=int(c),\n",
    "                n=int(m.sum()),\n",
    "                event_rate=float(np.mean(y_true[m] == 1)),\n",
    "                mean_y_prob=float(np.mean(y_prob[m])),\n",
    "            )\n",
    "        )\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    chi_p = np.nan\n",
    "    kw_p = np.nan\n",
    "    if len(clusters) >= 2:\n",
    "        table = [\n",
    "            [\n",
    "                int(np.sum((lab == c) & (y_true == 1))),\n",
    "                int(np.sum((lab == c) & (y_true == 0))),\n",
    "            ]\n",
    "            for c in clusters\n",
    "        ]\n",
    "        chi_p = chi2_contingency(table)[1]\n",
    "        groups = [y_prob[lab == c] for c in clusters]\n",
    "        kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "    df[\"p_event_rate(chi2)\"] = chi_p\n",
    "    df[\"p_yprob(kruskal)\"] = kw_p\n",
    "    df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 메인 루틴 ===============\n",
    "def run_cluster_analysis_for_label(\n",
    "    label: str,\n",
    "    umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"euclidean\",  # cosine\n",
    "    min_samples: int = 6,  # DBSCAN\n",
    "    eps: float | None = None,\n",
    "    dbscan_metric: str = \"euclidean\",\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "    topN: int = 10,\n",
    "    agg_for_heatmap: str = \"mean\",\n",
    "    save_representatives: bool = True,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "    out_dir = OUT_ROOT / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 0) 데이터 로드\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # Youden 임계값 & df_pred 보정\n",
    "    ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "    if ythr is None:\n",
    "        ythr = _compute_youden_from_preds(\n",
    "            np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "        )\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "    # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "    # V = _standardize(vals) # 표준화\n",
    "    V = vals\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "    if eps is None:\n",
    "        eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.9)  # 0.85\n",
    "        eps = eps0\n",
    "        save_kdist_plot(\n",
    "            kth,\n",
    "            k=min_samples,\n",
    "            out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "        )\n",
    "\n",
    "    # 3) DBSCAN 적응형 루프\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "    cur_min_samples = int(min_samples)\n",
    "    cur_eps = float(eps)\n",
    "\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            cur_eps *= 1.20\n",
    "            if cur_min_samples > 3:\n",
    "                cur_min_samples -= 1\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            cur_eps *= 0.88\n",
    "        else:\n",
    "            cur_eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # all-noise 대비: 마지막 완화 시도 1회\n",
    "    if (labels is None) or np.all(labels == -1):\n",
    "        cur_eps *= 1.30\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=\"final_relax\",\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "    if umap_n_components == 2:\n",
    "        Z2 = X_hi\n",
    "    else:\n",
    "        um_2d = UMAP.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=max(umap_n_neighbors, 15),\n",
    "            min_dist=max(umap_min_dist, 0.05),\n",
    "            metric=umap_metric,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "    m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # 6) 2D 플롯 (클러스터 색)\n",
    "    uniq = sorted(np.unique(labels))\n",
    "    cmap = plt.get_cmap(\"tab10\")  # tab20\n",
    "    color_map = {c: (\"#bbbbbb\" if c == -1 else cmap(c % 20)) for c in uniq}\n",
    "    colors = [color_map[c] for c in labels]\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    plt.scatter(Z2[:, 0], Z2[:, 1], c=colors, s=18, alpha=0.9)\n",
    "    title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "    plt.title(f\"{title} — {len(set(labels)-{-1})} clusters, noise={(labels==-1).sum()}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 6-2) 2D 플롯 (위험도 색)\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 7) 클러스터 통계/요약 + 히트맵\n",
    "    df_stats = add_dbscan_stats(\n",
    "        out_dir,\n",
    "        label,\n",
    "        y_true=np.asarray(y_true),\n",
    "        y_prob=np.asarray(y_prob),\n",
    "        labels=np.asarray(labels),\n",
    "    )\n",
    "    ret = plot_cluster_heatmap(\n",
    "        vals,\n",
    "        feat_names,\n",
    "        labels,\n",
    "        topN=topN,\n",
    "        out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "        cmap=\"magma_r\",  # rocket_r, viridis_r\n",
    "        agg=agg_for_heatmap,\n",
    "    )\n",
    "\n",
    "    # === 수치 저장(요청 파일명 규칙) ===\n",
    "    if ret is not None:\n",
    "        # 모든 피처의 원시 |SHAP| (클러스터 × 전체 피처)\n",
    "        ret[\"raw_all\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__ALL_FEATURES.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 원시 |SHAP|\n",
    "        ret[\"raw_top\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_RAW.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 행 정규화(상대값)\n",
    "        ret[\"norm_top\"].to_csv(\n",
    "            out_dir\n",
    "            / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_ROWNORM.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "    # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "    if save_representatives:\n",
    "        rep_dir = out_dir / \"representatives_dbscan\"\n",
    "        # mean/max/min\n",
    "        reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "        for rep in reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"mean_idx\", \"max_idx\", \"min_idx\"]:\n",
    "                i = rep[tag]\n",
    "                name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                save_local_plots(\n",
    "                    label,\n",
    "                    expl,\n",
    "                    i,\n",
    "                    rep_dir / f\"cluster_{c}\",\n",
    "                    feat_names=feat_names,\n",
    "                    max_display=15,\n",
    "                    df_pred=df_pred,\n",
    "                    youden_thr=ythr,\n",
    "                    name_tag=name_tag,\n",
    "                )\n",
    "        # FP/FN\n",
    "        err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "        for rep in err_reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "                if tag in rep:\n",
    "                    i = rep[tag]\n",
    "                    name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                    save_local_plots(\n",
    "                        label,\n",
    "                        expl,\n",
    "                        i,\n",
    "                        rep_dir / f\"cluster_{c}\",\n",
    "                        feat_names=feat_names,\n",
    "                        max_display=15,\n",
    "                        df_pred=df_pred,\n",
    "                        youden_thr=ythr,\n",
    "                        name_tag=name_tag,\n",
    "                    )\n",
    "\n",
    "    # 9) 메타(JSON)\n",
    "    meta = dict(\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        algo=\"dbscan_tuned\",\n",
    "        random_state=random_state,\n",
    "        umap=dict(\n",
    "            n_components=umap_n_components,\n",
    "            n_neighbors=umap_n_neighbors,\n",
    "            min_dist=umap_min_dist,\n",
    "            metric=umap_metric,\n",
    "        ),\n",
    "        final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "        trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "        n_clusters=int(len(set(labels) - {-1})),\n",
    "        noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "        history=info_hist,\n",
    "    )\n",
    "    with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"-> saved: {out_dir}\")\n",
    "    return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e25977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_for_ppt(\n",
    "    Z2: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    y_true: np.ndarray,  # [NEW] 실제 정답 레이블 추가\n",
    "    out_path: Path,\n",
    "    title: str,\n",
    "):\n",
    "\n",
    "    # plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    # --- [설정] Risk 판단 기준 (Event Rate 기준) ---\n",
    "    def get_risk_style(event_rate):\n",
    "        # Event Rate는 보통 확률보다 낮으므로 기준을 조금 조정할 수도 있음 (현행 유지)\n",
    "        if event_rate < 0.1:\n",
    "            return \"Low\", \"#4575b4\"\n",
    "        elif event_rate < 0.3:\n",
    "            return \"Moderate\", \"#7b3294\"\n",
    "        elif event_rate < 0.5:\n",
    "            return \"High\", \"#f1a340\"\n",
    "        else:\n",
    "            return \"Very High\", \"#d73027\"\n",
    "\n",
    "    uniq = sorted(np.unique(labels))\n",
    "    clusters = [c for c in uniq if c != -1]\n",
    "    n_clusters = len(clusters)\n",
    "\n",
    "    global_center = np.mean(Z2, axis=0)\n",
    "    cluster_cmap = plt.get_cmap(\"tab20\" if n_clusters > 10 else \"tab10\")\n",
    "\n",
    "    centroids = {}\n",
    "    for c in clusters:\n",
    "        centroids[c] = np.mean(Z2[labels == c], axis=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.scatter(\n",
    "        Z2[:, 0], Z2[:, 1], c=\"#e0e0e0\", s=20, alpha=0.4, edgecolors=\"none\", zorder=0\n",
    "    )\n",
    "\n",
    "    label_items = []\n",
    "    legend_handles = []\n",
    "\n",
    "    for i, c in enumerate(clusters):\n",
    "        mask = labels == c\n",
    "        points = Z2[mask]\n",
    "\n",
    "        # [변경] 실제 Event Rate 계산\n",
    "        # y_true가 0/1로 되어 있다고 가정 (1=Event)\n",
    "        true_vals = y_true[mask]\n",
    "        event_rate = np.mean(true_vals == 1)\n",
    "\n",
    "        risk_name, risk_color = get_risk_style(event_rate)  # Event Rate 기준 스타일\n",
    "        cluster_color = cluster_cmap(i % 20)\n",
    "\n",
    "        handle = mpatches.Patch(color=cluster_color, label=f\"Cluster {c}\")\n",
    "        legend_handles.append(handle)\n",
    "\n",
    "        if len(points) < 3:\n",
    "            ax.scatter(\n",
    "                points[:, 0], points[:, 1], c=cluster_color, s=20, alpha=0.8, zorder=2\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        ax.scatter(\n",
    "            points[:, 0],\n",
    "            points[:, 1],\n",
    "            c=[cluster_color],\n",
    "            s=20,\n",
    "            alpha=0.8,\n",
    "            edgecolors=\"white\",\n",
    "            linewidth=0.2,\n",
    "            zorder=2,\n",
    "        )\n",
    "        hull = ConvexHull(points)\n",
    "        hull_coords = points[np.append(hull.vertices, hull.vertices[0])]\n",
    "        ax.plot(\n",
    "            hull_coords[:, 0],\n",
    "            hull_coords[:, 1],\n",
    "            color=cluster_color,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.2,\n",
    "            alpha=0.8,\n",
    "            zorder=3,\n",
    "        )\n",
    "        ax.add_patch(\n",
    "            mpatches.Polygon(\n",
    "                hull_coords,\n",
    "                closed=True,\n",
    "                fc=cluster_color,\n",
    "                ec=None,\n",
    "                alpha=0.08,\n",
    "                zorder=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- 위치 계산 ---\n",
    "        current_centroid = centroids[c]\n",
    "        vec_radial = current_centroid - global_center\n",
    "        vec_repulsion = np.array([0.0, 0.0])\n",
    "        crowding = 0\n",
    "\n",
    "        for other_c, other_cent in centroids.items():\n",
    "            if c == other_c:\n",
    "                continue\n",
    "            diff = current_centroid - other_cent\n",
    "            dist = np.linalg.norm(diff)\n",
    "            if dist > 1e-4:\n",
    "                w = 1.0 / (dist**2.0)\n",
    "                vec_repulsion += (diff / dist) * w\n",
    "                crowding += w\n",
    "\n",
    "        final_vec = vec_radial + (vec_repulsion * 4.0)\n",
    "        if crowding > 0.5 or np.linalg.norm(vec_radial) < 1.0:\n",
    "            final_vec[1] += (1.0 if i % 2 == 0 else -1.0) * 3.0\n",
    "\n",
    "        norm = np.linalg.norm(final_vec)\n",
    "        unit_vec = final_vec / norm if norm > 1e-4 else np.array([0, 1])\n",
    "\n",
    "        hull_points = points[hull.vertices]\n",
    "        anchor_point = hull_points[np.argmax(np.dot(hull_points, unit_vec))]\n",
    "\n",
    "        offset = 2.5 + min(crowding, 3.5)\n",
    "        text_pos = anchor_point + (unit_vec * offset)\n",
    "\n",
    "        label_items.append(\n",
    "            {\n",
    "                \"text_pos\": text_pos,\n",
    "                \"anchor_point\": anchor_point,\n",
    "                # [변경] 라벨 텍스트에 Event Rate 표시\n",
    "                \"text_str\": f\"{risk_name}\\n(Event:{event_rate:.2f})\",\n",
    "                \"color\": risk_color,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --- 물리 엔진 ---\n",
    "    iterations = 50\n",
    "    min_dist = 3.0\n",
    "    for _ in range(iterations):\n",
    "        for i in range(len(label_items)):\n",
    "            for j in range(i + 1, len(label_items)):\n",
    "                p1 = label_items[i][\"text_pos\"]\n",
    "                p2 = label_items[j][\"text_pos\"]\n",
    "                diff = p1 - p2\n",
    "                dist = np.linalg.norm(diff)\n",
    "                if dist < min_dist:\n",
    "                    if dist < 1e-4:\n",
    "                        force = np.array([0.1, 0.1])\n",
    "                    else:\n",
    "                        force = (diff / dist) * (min_dist - dist) * 0.5\n",
    "                    label_items[i][\"text_pos\"] += force\n",
    "                    label_items[j][\"text_pos\"] -= force\n",
    "\n",
    "    # --- 축 확장 ---\n",
    "    all_x = list(Z2[:, 0]) + [item[\"text_pos\"][0] for item in label_items]\n",
    "    all_y = list(Z2[:, 1]) + [item[\"text_pos\"][1] for item in label_items]\n",
    "    margin_x = (max(all_x) - min(all_x)) * 0.1\n",
    "    margin_y = (max(all_y) - min(all_y)) * 0.1\n",
    "    ax.set_xlim(min(all_x) - margin_x, max(all_x) + margin_x)\n",
    "    ax.set_ylim(min(all_y) - margin_y, max(all_y) + margin_y)\n",
    "\n",
    "    # # --- Annotation 그리기 ---\n",
    "    # for item in label_items:\n",
    "    #     bbox_props = dict(\n",
    "    #         boxstyle=\"round,pad=0.2\", fc=\"white\", ec=item[\"color\"], lw=1.5, alpha=0.95\n",
    "    #     )\n",
    "\n",
    "    #     ax.annotate(\n",
    "    #         text=item[\"text_str\"],\n",
    "    #         xy=(item[\"anchor_point\"][0], item[\"anchor_point\"][1]),\n",
    "    #         xytext=(item[\"text_pos\"][0], item[\"text_pos\"][1]),\n",
    "    #         ha=\"center\",\n",
    "    #         va=\"center\",\n",
    "    #         fontsize=14,\n",
    "    #         weight=\"bold\",\n",
    "    #         color=item[\"color\"],\n",
    "    #         bbox=bbox_props,\n",
    "    #         arrowprops=dict(\n",
    "    #             arrowstyle=\"-\",\n",
    "    #             color=\"black\",\n",
    "    #             linewidth=0.8,\n",
    "    #             shrinkB=4,\n",
    "    #             connectionstyle=\"arc3,rad=0.1\",\n",
    "    #         ),\n",
    "    #         zorder=10,\n",
    "    #     )\n",
    "\n",
    "    # 범례 및 타이틀\n",
    "    if -1 in uniq:\n",
    "        legend_handles.insert(0, mpatches.Patch(color=\"#e0e0e0\", label=\"Noise\"))\n",
    "\n",
    "    ax.legend(\n",
    "        handles=legend_handles,\n",
    "        title=\"Clusters\",\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=\"upper left\",\n",
    "        borderaxespad=0,\n",
    "        frameon=False,\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    noise_pct = (labels == -1).sum() / len(labels) if len(labels) > 0 else 0\n",
    "    full_title = f\"{title}\\n(Noise ratio={noise_pct:.1%})\"\n",
    "\n",
    "    ax.set_title(full_title, fontsize=14, weight=\"bold\", pad=20)\n",
    "    ax.set_xlabel(\"UMAP-1\", fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel(\"UMAP-2\", fontsize=12, labelpad=10)\n",
    "    # ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 유틸 ===============\n",
    "def _standardize(X: np.ndarray) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    X = np.nan_to_num(\n",
    "        X, nan=0.0, posinf=np.finfo(float).max / 1e6, neginf=-np.finfo(float).max / 1e6\n",
    "    )\n",
    "    return (X - X.mean(0)) / (X.std(0) + 1e-9)\n",
    "\n",
    "\n",
    "def save_png_svg(figpath: Path, **savefig_kwargs):\n",
    "    plt.savefig(figpath, **savefig_kwargs)\n",
    "    try:\n",
    "        plt.savefig(\n",
    "            figpath.with_suffix(\".svg\"),\n",
    "            bbox_inches=savefig_kwargs.get(\"bbox_inches\", \"tight\"),\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============== Youden 임계값 ===============\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def _try_read_youden_threshold(label: str, base_dir: Path = OUT_ROOT.parent):\n",
    "    for sep in [\"__\", \"___\"]:\n",
    "        f = base_dir / f\"{label}{sep}youden_metrics_Test.csv\"\n",
    "        if f.exists():\n",
    "            try:\n",
    "                dfy = pd.read_csv(f)\n",
    "                if \"threshold\" in dfy.columns:\n",
    "                    return float(dfy.loc[0, \"threshold\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _compute_youden_from_preds(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
    "    J = tpr - fpr\n",
    "    return float(thr[int(np.argmax(J))])\n",
    "\n",
    "\n",
    "# =============== SHAP 로딩 ===============\n",
    "def get_shap_matrix(label: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      expl: shap.Explanation (n, p)\n",
    "      vals: ndarray (n, p)\n",
    "      feat_names: list[str] length p (cleaned)\n",
    "      y_prob: ndarray (n,)\n",
    "      y_true: ndarray (n,)\n",
    "      X_te_t: ndarray (n, p_after_preproc)\n",
    "      df_pred: DataFrame with y_prob[, y_true, y_pred]\n",
    "    \"\"\"\n",
    "    # ⚠️ 기존 프로젝트의 함수 가정\n",
    "    expl, X_te_t, y_true, df_pred = shap_on_test(label)\n",
    "\n",
    "    vals = np.asarray(expl.values, dtype=float)\n",
    "    vals = np.nan_to_num(\n",
    "        vals,\n",
    "        nan=0.0,\n",
    "        posinf=np.finfo(float).max / 1e6,\n",
    "        neginf=-np.finfo(float).max / 1e6,\n",
    "    )\n",
    "\n",
    "    if getattr(expl, \"feature_names\", None) is not None:\n",
    "        raw_names = list(expl.feature_names)\n",
    "    else:\n",
    "        raw_names = [f\"f{i}\" for i in range(vals.shape[1])]\n",
    "    feat_names = [clean_name(n) for n in raw_names]\n",
    "\n",
    "    y_prob = np.asarray(df_pred[\"y_prob\"].values, dtype=float)\n",
    "    y_true = np.asarray(getattr(y_true, \"values\", y_true)).astype(int)\n",
    "    return expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred\n",
    "\n",
    "\n",
    "# =============== k-distance & eps 추정 ===============\n",
    "def _kdist_eps_heuristic(X_latent: np.ndarray, k: int = 5, pct: float = 0.85) -> float:\n",
    "    nn = NearestNeighbors(n_neighbors=max(k, 2), metric=\"euclidean\")\n",
    "    nn.fit(X_latent)\n",
    "    dists, _ = nn.kneighbors(X_latent)\n",
    "    kth = np.sort(dists[:, -1])\n",
    "    base_eps = float(np.quantile(kth, pct))\n",
    "    return base_eps, kth\n",
    "\n",
    "\n",
    "def save_kdist_plot(kth_sorted: np.ndarray, k: int, out_path: Path):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(np.arange(len(kth_sorted)), kth_sorted)\n",
    "    plt.xlabel(\"Points (sorted)\")\n",
    "    plt.ylabel(f\"{k}-NN distance\")\n",
    "    plt.title(f\"k-distance curve (k={k})\")\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# =============== 히트맵 ===============\n",
    "def _agg_abs(values: np.ndarray, how=\"mean\"):\n",
    "    return (\n",
    "        np.abs(values).mean(axis=0)\n",
    "        if how == \"mean\"\n",
    "        else np.mean(np.abs(values), axis=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cluster_heatmap(\n",
    "    vals,\n",
    "    feat_names,\n",
    "    labels,\n",
    "    topN,\n",
    "    out_path: Path,\n",
    "    cmap: str = \"rocket\",\n",
    "    agg: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      {\n",
    "        \"raw_all\":  DataFrame (clusters × all_features) : 클러스터별 원시 집계 |SHAP|\n",
    "        \"raw_top\":  DataFrame (clusters × topN)         : 히트맵에 사용된 TopN의 원시 |SHAP|\n",
    "        \"norm_top\": DataFrame (clusters × topN)         : TopN의 행 정규화(상대값)\n",
    "        \"feat_top\": List[str]                           : TopN 피처명\n",
    "        \"clusters\": np.ndarray                          : 사용된 클러스터 ID (노이즈 -1 제외)\n",
    "        \"fig_raw_path\": Path                            : raw heatmap PNG 경로\n",
    "        \"fig_norm_path\": Path                           : normalized heatmap PNG 경로(= out_path)\n",
    "      }\n",
    "    \"\"\"\n",
    "    from textwrap import fill\n",
    "\n",
    "    # 노이즈(-1) 제외\n",
    "    clusters = [c for c in sorted(np.unique(labels)) if c != -1]\n",
    "    if not clusters:\n",
    "        return None\n",
    "\n",
    "    # --- (A) 클러스터별 원시 집계 |SHAP|\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        mask = labels == c\n",
    "        rows.append(_agg_abs(vals[mask], how=agg))  # shape: (p,)\n",
    "    M = np.vstack(rows)  # (n_clusters, n_features)\n",
    "\n",
    "    # --- (B) 히트맵에서 쓸 TopN 선택 (클러스터 max 기준)\n",
    "    global_imp = _agg_abs(vals, how=agg)  # 전반적 중요도 (median|SHAP| 또는 mean|SHAP|)\n",
    "    hetero = M.max(axis=0) - M.min(axis=0)  # 간단한 이질성 점수 (max - min)\n",
    "\n",
    "    topK_imp = max(topN * 2, 30)  # 후보 풀(전반적 중요도 상위 K)\n",
    "    cand = np.argsort(global_imp)[::-1][:topK_imp]\n",
    "    top_idx = cand[\n",
    "        np.argsort(hetero[cand])[::-1][:topN]\n",
    "    ]  # 후보 안에서 이질성 큰 순 TopN\n",
    "\n",
    "    M_top = M[:, top_idx]\n",
    "    feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    ###########################################\n",
    "    # global_imp = _agg_abs(vals, how=agg)  # shape (p,)  # median(|SHAP|)\n",
    "    # top_idx = np.argsort(global_imp)[::-1][:topN]\n",
    "\n",
    "    # M_top = M[:, top_idx]\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "    ###########################################\n",
    "    # top_idx = np.argsort(M.max(axis=0))[::-1][:topN]\n",
    "    # M_top = M[:, top_idx]  # raw topN\n",
    "    # feat_top = [feat_names[i] for i in top_idx]\n",
    "\n",
    "    # --- (C) 행 정규화(클러스터 내부 상대값)\n",
    "    row_max = np.maximum(M_top.max(axis=1, keepdims=True), 1e-9)\n",
    "    M_disp = M_top / row_max  # normalized topN\n",
    "\n",
    "    # --- (D) DataFrame 구성\n",
    "    idx_names = [f\"cluster_{c}\" for c in clusters]\n",
    "    df_raw_all = pd.DataFrame(M, index=idx_names, columns=feat_names)\n",
    "    df_raw_top = pd.DataFrame(M_top, index=idx_names, columns=feat_top)\n",
    "    df_norm_top = pd.DataFrame(M_disp, index=idx_names, columns=feat_top)\n",
    "\n",
    "    # ---------- (E) Normalized Heatmap (기존 파일명 유지) ----------\n",
    "    xticks = [f\"Cluster {c}\" for c in clusters]\n",
    "    yticks = [fill(f, width=28) for f in feat_top]\n",
    "    fig_w = max(5, 0.8 * len(clusters))\n",
    "    fig_h = max(4, 0.35 * len(yticks))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_norm_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": \"Relative importance\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",  #  – Row-normalized\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # ---------- (F) Raw Heatmap (새 파일) ----------\n",
    "    out_path_raw = out_path.with_name(out_path.stem + \"__RAW.png\")\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    sns.heatmap(\n",
    "        df_raw_top.T.values,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": f\"{agg} |SHAP|\"},\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks,\n",
    "        linewidths=0.3,\n",
    "        linecolor=\"white\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\n",
    "        f\"Cluster × Feature importance ({agg} |SHAP|)\",\n",
    "        fontsize=12,\n",
    "        weight=\"bold\",\n",
    "        pad=12,\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\", fontsize=10, labelpad=6)\n",
    "    ax.set_ylabel(\"Top features\", fontsize=10, labelpad=6)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=30, labelsize=9)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8, pad=4)\n",
    "    ax.collections[0].colorbar.ax.tick_params(labelsize=9)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(out_path_raw, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    return {\n",
    "        \"raw_all\": df_raw_all,\n",
    "        \"raw_top\": df_raw_top,\n",
    "        \"norm_top\": df_norm_top,\n",
    "        \"feat_top\": feat_top,\n",
    "        \"clusters\": np.array(clusters),\n",
    "        \"fig_raw_path\": out_path_raw,\n",
    "        \"fig_norm_path\": out_path,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============== 대표 케이스 ===============\n",
    "def pick_representatives(y_prob: np.ndarray, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:  # 노이즈 제외(원하면 포함 가능)\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        probs = y_prob[idx]\n",
    "        med = np.mean(probs)\n",
    "        med_idx = idx[np.argmin(np.abs(probs - med))]\n",
    "        max_idx = idx[np.argmax(probs)]\n",
    "        min_idx = idx[np.argmin(probs)]\n",
    "        reps.append(\n",
    "            {\n",
    "                \"cluster\": int(c),\n",
    "                \"mean_idx\": int(med_idx),\n",
    "                \"max_idx\": int(max_idx),\n",
    "                \"min_idx\": int(min_idx),\n",
    "            }\n",
    "        )\n",
    "    return reps\n",
    "\n",
    "\n",
    "def pick_error_representatives(df_pred: pd.DataFrame, labels: np.ndarray):\n",
    "    reps = []\n",
    "    for c in np.unique(labels):\n",
    "        if c == -1:\n",
    "            continue\n",
    "        idx = np.where(labels == c)[0]\n",
    "        d = df_pred.iloc[idx]\n",
    "        rec = {\"cluster\": int(c)}\n",
    "        fp = d[(d[\"y_true\"] == 0) & (d[\"y_pred\"] == 1)]\n",
    "        fn = d[(d[\"y_true\"] == 1) & (d[\"y_pred\"] == 0)]\n",
    "        if len(fp):\n",
    "            rec[\"fp_idx\"] = int(fp[\"y_prob\"].idxmax())\n",
    "        if len(fn):\n",
    "            rec[\"fn_idx\"] = int(fn[\"y_prob\"].idxmin())\n",
    "        reps.append(rec)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def save_local_plots(\n",
    "    label: str,\n",
    "    expl: shap.Explanation,\n",
    "    idx: int,\n",
    "    out_dir: Path,\n",
    "    feat_names=None,\n",
    "    max_display=15,\n",
    "    df_pred: pd.DataFrame | None = None,\n",
    "    youden_thr: float | None = None,\n",
    "    name_tag: str | None = None,\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    y_t = y_p = None\n",
    "    y_prob = None\n",
    "    thr_txt = \"\"\n",
    "    if df_pred is not None:\n",
    "        if \"y_true\" in df_pred.columns:\n",
    "            y_t = int(df_pred.loc[idx, \"y_true\"])\n",
    "        if \"y_prob\" in df_pred.columns:\n",
    "            y_prob = float(df_pred.loc[idx, \"y_prob\"])\n",
    "        if \"y_pred\" in df_pred.columns:\n",
    "            y_p = int(df_pred.loc[idx, \"y_pred\"])\n",
    "        else:\n",
    "            thr = youden_thr if youden_thr is not None else 0.5\n",
    "            y_p = (\n",
    "                int((df_pred.loc[idx, \"y_prob\"]) >= thr)\n",
    "                if \"y_prob\" in df_pred.columns\n",
    "                else None\n",
    "            )\n",
    "        if youden_thr is not None:\n",
    "            thr_txt = f\", thr={youden_thr:.3f}\"\n",
    "\n",
    "    # waterfall\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        shap.plots.waterfall(expl[idx], show=False, max_display=max_display)\n",
    "    except Exception:\n",
    "        row = expl[idx]\n",
    "        shap.plots.waterfall(row, show=False, max_display=max_display)\n",
    "    title_bits = [f\"{label} | id={idx}\"]\n",
    "    if y_t is not None:\n",
    "        title_bits.append(f\"y_true={y_t}\")\n",
    "    if y_p is not None:\n",
    "        title_bits.append(f\"y_pred={y_p}\")\n",
    "    if y_prob is not None:\n",
    "        title_bits.append(f\"y_prob={y_prob:.3f}\")\n",
    "    if name_tag:\n",
    "        title_bits.append(f\"[{name_tag}]\")\n",
    "    ttl = \" , \".join(title_bits) + thr_txt + \" (Youden thr on TEST)\"\n",
    "    plt.title(ttl)\n",
    "    plt.tight_layout()\n",
    "    save_png_svg(\n",
    "        out_dir\n",
    "        / (\n",
    "            f\"{label}__waterfall_id{idx}\"\n",
    "            + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "            + \".png\"\n",
    "        ),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    # force (옵션)\n",
    "    try:\n",
    "        plt.figure(figsize=(7, 2.8))\n",
    "        shap.plots.force(\n",
    "            expl[idx].base_values, expl[idx].values, matplotlib=True, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_png_svg(\n",
    "            out_dir\n",
    "            / (\n",
    "                f\"{label}__force_id{idx}\"\n",
    "                + (f\"__{name_tag}\" if name_tag else \"\")\n",
    "                + \".png\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "    except Exception:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============== DBSCAN 통계/요약 ===============\n",
    "def add_dbscan_stats(\n",
    "    out_dir: Path,\n",
    "    label: str,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "):\n",
    "    lab = np.asarray(labels)\n",
    "    clusters = [c for c in np.unique(lab) if c != -1]\n",
    "    rows = []\n",
    "    for c in clusters:\n",
    "        m = lab == c\n",
    "        rows.append(\n",
    "            dict(\n",
    "                cluster=int(c),\n",
    "                n=int(m.sum()),\n",
    "                event_rate=float(np.mean(y_true[m] == 1)),\n",
    "                mean_y_prob=float(np.mean(y_prob[m])),\n",
    "            )\n",
    "        )\n",
    "    df = pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    chi_p = np.nan\n",
    "    kw_p = np.nan\n",
    "    if len(clusters) >= 2:\n",
    "        table = [\n",
    "            [\n",
    "                int(np.sum((lab == c) & (y_true == 1))),\n",
    "                int(np.sum((lab == c) & (y_true == 0))),\n",
    "            ]\n",
    "            for c in clusters\n",
    "        ]\n",
    "        chi_p = chi2_contingency(table)[1]\n",
    "        groups = [y_prob[lab == c] for c in clusters]\n",
    "        kw_p = kruskal(*groups).pvalue\n",
    "\n",
    "    df[\"p_event_rate(chi2)\"] = chi_p\n",
    "    df[\"p_yprob(kruskal)\"] = kw_p\n",
    "    df.to_csv(out_dir / f\"{label}__dbscan_cluster_stats.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============== 메인 루틴 ===============\n",
    "\n",
    "\n",
    "def run_cluster_analysis_for_label(\n",
    "    label: str,\n",
    "    umap_n_components: int = 12,  # 군집용 UMAP 차원\n",
    "    umap_n_neighbors: int = 12,\n",
    "    umap_min_dist: float = 0.01,\n",
    "    umap_metric: str = \"euclidean\",  # cosine\n",
    "    min_samples: int = 6,  # DBSCAN\n",
    "    eps: float | None = None,\n",
    "    dbscan_metric: str = \"euclidean\",\n",
    "    target_min_clusters: int = 2,\n",
    "    max_noise_ratio: float = 0.45,\n",
    "    max_trials: int = 5,\n",
    "    topN: int = 10,\n",
    "    agg_for_heatmap: str = \"mean\",\n",
    "    save_representatives: bool = True,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    assert HAS_UMAP, \"umap-learn 패키지가 필요합니다.\"\n",
    "    print(f\"\\n=== {label}: UMAP+DBSCAN (adaptive) ===\")\n",
    "    out_dir = OUT_ROOT / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 0) 데이터 로드\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # Youden 임계값 & df_pred 보정\n",
    "    ythr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "    if ythr is None:\n",
    "        ythr = _compute_youden_from_preds(\n",
    "            np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "        )\n",
    "    if \"y_true\" not in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_true=np.asarray(y_true).astype(int))\n",
    "    if \"y_pred\" not in df_pred.columns and \"y_prob\" in df_pred.columns:\n",
    "        df_pred = df_pred.assign(y_pred=(df_pred[\"y_prob\"] >= ythr).astype(int))\n",
    "\n",
    "    # 1) 표준화 & UMAP(고차원; 군집용)\n",
    "    # V = _standardize(vals) # 표준화\n",
    "    V = vals\n",
    "    um_hi = UMAP.UMAP(\n",
    "        n_components=umap_n_components,\n",
    "        n_neighbors=umap_n_neighbors,\n",
    "        min_dist=umap_min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    X_hi = um_hi.fit_transform(V)\n",
    "\n",
    "    # 2) eps 초기값 (k-distance) & 곡선 저장\n",
    "    if eps is None:\n",
    "        eps0, kth = _kdist_eps_heuristic(X_hi, k=min_samples, pct=0.9)  # 0.85\n",
    "        eps = eps0\n",
    "        save_kdist_plot(\n",
    "            kth,\n",
    "            k=min_samples,\n",
    "            out_path=(out_dir / f\"{label}__kdist_k{min_samples}.png\"),\n",
    "        )\n",
    "\n",
    "    # 3) DBSCAN 적응형 루프\n",
    "    trial = 0\n",
    "    labels = None\n",
    "    info_hist = []\n",
    "    cur_min_samples = int(min_samples)\n",
    "    cur_eps = float(eps)\n",
    "\n",
    "    while trial < max_trials:\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=trial,\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if (n_clusters >= target_min_clusters) and (noise_ratio <= max_noise_ratio):\n",
    "            break\n",
    "\n",
    "        # 조정 규칙\n",
    "        if noise_ratio > max_noise_ratio:\n",
    "            cur_eps *= 1.20\n",
    "            if cur_min_samples > 3:\n",
    "                cur_min_samples -= 1\n",
    "        elif n_clusters < target_min_clusters:\n",
    "            cur_eps *= 0.88\n",
    "        else:\n",
    "            cur_eps *= 1.05\n",
    "\n",
    "        trial += 1\n",
    "\n",
    "    # all-noise 대비: 마지막 완화 시도 1회\n",
    "    if (labels is None) or np.all(labels == -1):\n",
    "        cur_eps *= 1.30\n",
    "        db = DBSCAN(\n",
    "            eps=cur_eps, min_samples=cur_min_samples, metric=dbscan_metric, n_jobs=-1\n",
    "        )\n",
    "        labels = db.fit_predict(X_hi)  # X_hi\n",
    "        noise_ratio = float(np.mean(labels == -1))\n",
    "        n_clusters = int(len(set(labels) - {-1}))\n",
    "        info_hist.append(\n",
    "            dict(\n",
    "                trial=\"final_relax\",\n",
    "                eps=cur_eps,\n",
    "                min_samples=cur_min_samples,\n",
    "                n_clusters=n_clusters,\n",
    "                noise_ratio=noise_ratio,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) 시각화용 2D UMAP (시각화 전용)\n",
    "    if umap_n_components == 2:\n",
    "        Z2 = X_hi\n",
    "    else:\n",
    "        um_2d = UMAP.UMAP(\n",
    "            n_components=2,\n",
    "            n_neighbors=max(umap_n_neighbors, 15),\n",
    "            min_dist=max(umap_min_dist, 0.05),\n",
    "            metric=umap_metric,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        Z2 = um_2d.fit_transform(V)\n",
    "\n",
    "    # [NEW] 위험도 순으로 라벨 재정렬 (이 부분 추가!)\n",
    "    # Cluster 0 = Low Risk, Cluster N = High Risk\n",
    "    if labels is not None:\n",
    "        print(\"-> Reordering clusters by risk (0: Low -> N: High)...\")\n",
    "        labels = reorder_labels_by_risk(labels, np.asarray(y_true))\n",
    "\n",
    "    # 5) 라벨 매핑 저장 (이제 정렬된 labels가 저장됨)\n",
    "    m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # # 5) 라벨 매핑 저장(노이즈 포함)\n",
    "    # m = pd.DataFrame({\"idx\": np.arange(len(labels)), \"cluster\": labels})\n",
    "    # m.to_csv(out_dir / f\"{label}__dbscan_labels.csv\", index=False)\n",
    "\n",
    "    # # ⭐️⭐️⭐️⭐️기존 코드 6) 2D 플롯 (클러스터 색) ⭐️⭐️⭐️⭐️\n",
    "    # uniq = sorted(np.unique(labels))\n",
    "    # # 클러스터 개수가 많을 수 있으므로 tab10 사용 권장\n",
    "    # cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "    # # 범례 공간 확보를 위해 가로 사이즈를 약간 늘림\n",
    "    # plt.figure(figsize=(8.5, 6.0))\n",
    "\n",
    "    # # 1. 노이즈(-1) 먼저 그리기 (배경으로 깔리게 처리)\n",
    "    # if -1 in uniq:\n",
    "    #     mask = labels == -1\n",
    "    #     plt.scatter(\n",
    "    #         Z2[mask, 0],\n",
    "    #         Z2[mask, 1],\n",
    "    #         c=\"#e0e0e0\",  # 아주 연한 회색\n",
    "    #         edgecolor=\"#bbbbbb\",  # 테두리는 조금 진하게\n",
    "    #         linewidth=0.1,\n",
    "    #         s=15,  # 크기는 작게\n",
    "    #         alpha=0.4,  # 투명하게\n",
    "    #         label=\"Noise\",  # 범례 이름\n",
    "    #         zorder=1,  # 가장 뒤쪽에 배치\n",
    "    #     )\n",
    "\n",
    "    # # 2. 실제 클러스터 그리기 Loop\n",
    "    # for c in uniq:\n",
    "    #     if c == -1:\n",
    "    #         continue\n",
    "    #     mask = labels == c\n",
    "    #     color = cmap(c % 20)  # 색상 순환 적용\n",
    "\n",
    "    #     plt.scatter(\n",
    "    #         Z2[mask, 0],\n",
    "    #         Z2[mask, 1],\n",
    "    #         c=[color],  # 단일 색상 적용\n",
    "    #         s=22,  # 클러스터 포인트는 조금 더 크게\n",
    "    #         alpha=0.9,  # 진하게\n",
    "    #         label=f\"Cluster {c}\",\n",
    "    #         edgecolor=\"white\",  # 포인트 구분감\n",
    "    #         linewidth=0.3,\n",
    "    #         zorder=2,  # 노이즈보다 앞쪽에 배치\n",
    "    #     )\n",
    "\n",
    "    # title = f\"UMAP + DBSCAN ({umap_n_components}D)\"\n",
    "    # # 제목에 전체 대비 노이즈 비율 표기\n",
    "    # n_noise = np.sum(labels == -1)\n",
    "    # n_total = len(labels)\n",
    "    # plt.title(\n",
    "    #     f\"{title}\\nTotal: {n_total}, Clusters: {len(set(labels)-{-1})}, Noise: {n_noise} ({n_noise/n_total:.1%})\",\n",
    "    #     fontsize=11,\n",
    "    # )\n",
    "    # plt.xlabel(\"UMAP-1\")\n",
    "    # plt.ylabel(\"UMAP-2\")\n",
    "\n",
    "    # # 범례 설정 (그래프 영역 밖 우측 상단에 배치)\n",
    "    # plt.legend(\n",
    "    #     bbox_to_anchor=(1.02, 1),\n",
    "    #     loc=\"upper left\",\n",
    "    #     borderaxespad=0,\n",
    "    #     frameon=False,\n",
    "    #     fontsize=9,\n",
    "    #     markerscale=1.5,  # 범례의 점 크기 키우기\n",
    "    # )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # save_png_svg(\n",
    "    #     out_dir / f\"{label}__UMAP2D_DBSCAN_tuned.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    # )\n",
    "\n",
    "    # 6) 2D 플롯 (Hull + Annotation 적용 버전)\n",
    "    # print(\"-> Plotting UMAP with Threshold-based Risk Labels...\")\n",
    "    # plot_umap_with_hulls_and_labels(\n",
    "    #     Z2=Z2,\n",
    "    #     labels=labels,\n",
    "    #     y_prob=y_prob,  # [중요] 여기에 y_prob를 반드시 넘겨주세요!\n",
    "    #     out_path=out_dir / f\"{label}__UMAP2D_DBSCAN_annotated.png\",\n",
    "    #     title=f\"Risk Patterns ({label})\",\n",
    "    # )\n",
    "\n",
    "    print(\"-> Plotting UMAP for PPT (Event Rate based)...\")\n",
    "    plot_umap_for_ppt(\n",
    "        Z2=Z2,\n",
    "        labels=labels,\n",
    "        y_prob=y_prob,  # 기존 인자\n",
    "        y_true=y_true,  # [NEW] 실제 정답값 전달\n",
    "        out_path=out_dir / f\"{label}__UMAP2D_DBSCAN_.png\", # annotated_ppt\n",
    "        title=f\"Risk Patterns ({label})\",\n",
    "    )\n",
    "\n",
    "    # 6-2) 2D 플롯 (위험도 색)\n",
    "    plt.figure(figsize=(7.2, 6.2))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y_prob, s=18, alpha=0.9, cmap=\"plasma\")\n",
    "    plt.colorbar(sc, label=\"Predicted risk (prob.)\")\n",
    "    plt.title(f\"UMAP(2D) (colored by risk) — {label}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.margins(0.02)\n",
    "    save_png_svg(\n",
    "        out_dir / f\"{label}__UMAP2D_risk_DBSCAN.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "\n",
    "    # 7) 클러스터 통계/요약 + 히트맵\n",
    "    df_stats = add_dbscan_stats(\n",
    "        out_dir,\n",
    "        label,\n",
    "        y_true=np.asarray(y_true),\n",
    "        y_prob=np.asarray(y_prob),\n",
    "        labels=np.asarray(labels),\n",
    "    )\n",
    "    ret = plot_cluster_heatmap(\n",
    "        vals,\n",
    "        feat_names,\n",
    "        labels,\n",
    "        topN=topN,\n",
    "        out_path=out_dir / f\"{label}__heatmap_top{topN}_dbscan.png\",\n",
    "        cmap=\"magma_r\",  # rocket_r, viridis_r\n",
    "        agg=agg_for_heatmap,\n",
    "    )\n",
    "\n",
    "    # === 수치 저장(요청 파일명 규칙) ===\n",
    "    if ret is not None:\n",
    "        # 모든 피처의 원시 |SHAP| (클러스터 × 전체 피처)\n",
    "        ret[\"raw_all\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__ALL_FEATURES.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 원시 |SHAP|\n",
    "        ret[\"raw_top\"].to_csv(\n",
    "            out_dir / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_RAW.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "        # 히트맵에 쓰인 TopN의 행 정규화(상대값)\n",
    "        ret[\"norm_top\"].to_csv(\n",
    "            out_dir\n",
    "            / f\"{label}__heatmap_values_{agg_for_heatmap}__TOP{topN}_ROWNORM.csv\",\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "    # 8) 대표 케이스 저장 (mean/max/min + FP/FN)\n",
    "    if save_representatives:\n",
    "        rep_dir = out_dir / \"representatives_dbscan\"\n",
    "        # mean/max/min\n",
    "        reps = pick_representatives(y_prob, np.asarray(labels))\n",
    "        for rep in reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"mean_idx\", \"max_idx\", \"min_idx\"]:\n",
    "                i = rep[tag]\n",
    "                name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                save_local_plots(\n",
    "                    label,\n",
    "                    expl,\n",
    "                    i,\n",
    "                    rep_dir / f\"cluster_{c}\",\n",
    "                    feat_names=feat_names,\n",
    "                    max_display=15,\n",
    "                    df_pred=df_pred,\n",
    "                    youden_thr=ythr,\n",
    "                    name_tag=name_tag,\n",
    "                )\n",
    "        # FP/FN\n",
    "        err_reps = pick_error_representatives(df_pred, np.asarray(labels))\n",
    "        for rep in err_reps:\n",
    "            c = rep[\"cluster\"]\n",
    "            for tag in [\"fp_idx\", \"fn_idx\"]:\n",
    "                if tag in rep:\n",
    "                    i = rep[tag]\n",
    "                    name_tag = f\"cluster{c}__{tag.replace('_idx','')}\"\n",
    "                    save_local_plots(\n",
    "                        label,\n",
    "                        expl,\n",
    "                        i,\n",
    "                        rep_dir / f\"cluster_{c}\",\n",
    "                        feat_names=feat_names,\n",
    "                        max_display=15,\n",
    "                        df_pred=df_pred,\n",
    "                        youden_thr=ythr,\n",
    "                        name_tag=name_tag,\n",
    "                    )\n",
    "\n",
    "    # 9) 메타(JSON)\n",
    "    meta = dict(\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        algo=\"dbscan_tuned\",\n",
    "        random_state=random_state,\n",
    "        umap=dict(\n",
    "            n_components=umap_n_components,\n",
    "            n_neighbors=umap_n_neighbors,\n",
    "            min_dist=umap_min_dist,\n",
    "            metric=umap_metric,\n",
    "        ),\n",
    "        final=dict(eps=float(cur_eps), min_samples=int(cur_min_samples)),\n",
    "        trials=len([h for h in info_hist if isinstance(h.get(\"trial\"), int)]),\n",
    "        n_clusters=int(len(set(labels) - {-1})),\n",
    "        noise_ratio=float(np.mean(np.asarray(labels) == -1)),\n",
    "        history=info_hist,\n",
    "    )\n",
    "    with open(out_dir / f\"{label}__dbscan_meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    print(f\"-> saved: {out_dir}\")\n",
    "    return df_stats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== 전체 라벨 반복 실행 ===============\n",
    "all_cluster_summaries = []\n",
    "for lb in LABELS:\n",
    "    try:\n",
    "        df_sum, labels = run_cluster_analysis_for_label(\n",
    "            lb,\n",
    "            umap_n_components=2,\n",
    "            umap_n_neighbors=45,  # 40\n",
    "            umap_min_dist=0.0,  # 0.05, 0.01\n",
    "            umap_metric=\"euclidean\",  # cosine\n",
    "            min_samples=17,  # 20\n",
    "            eps=None,\n",
    "            dbscan_metric=\"euclidean\",\n",
    "            target_min_clusters=3,\n",
    "            max_noise_ratio=0.45,  # 0.45\n",
    "            max_trials=5,\n",
    "            topN=10,\n",
    "            agg_for_heatmap=\"mean\",\n",
    "            save_representatives=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "        df_sum.insert(0, \"label\", lb)\n",
    "        all_cluster_summaries.append(df_sum)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {lb}: {e}\")\n",
    "\n",
    "if all_cluster_summaries:\n",
    "    all_summ = pd.concat(all_cluster_summaries, ignore_index=True)\n",
    "    out_csv = OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.csv\"\n",
    "    all_summ.to_csv(out_csv, index=False)\n",
    "    print(\"✅ saved:\", out_csv)\n",
    "    # Excel(라벨별 시트) 옵션\n",
    "    try:\n",
    "        with pd.ExcelWriter(\n",
    "            OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\"\n",
    "        ) as xw:\n",
    "            for lb, df_ in all_summ.groupby(\"label\"):\n",
    "                df_.to_excel(xw, sheet_name=lb, index=False)\n",
    "        print(\"✅ saved:\", OUT_ROOT / \"ALL_labels__cluster_summary_dbscan_tuned.xlsx\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Excel save:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 경로 설정 ----------\n",
    "csv_path = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters/ALL_labels__cluster_summary_dbscan_tuned.csv\")\n",
    ")\n",
    "out_path = csv_path.with_name(\"Table3_Cluster_Summary.xlsx\")\n",
    "\n",
    "# ---------- 데이터 로드 ----------\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ---------- 전처리 ----------\n",
    "# (1) Δ 계산: 예측확률 - 실제 event rate 차이\n",
    "df[\"Delta(abs)\"] = (df[\"mean_y_prob\"] - df[\"event_rate\"]).abs().round(3)\n",
    "df[\"Direction\"] = df.apply(\n",
    "    lambda x: (\n",
    "        \"Over (Pred>Event)\"\n",
    "        if x[\"mean_y_prob\"] > x[\"event_rate\"]\n",
    "        else \"Under (Pred<Event)\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# (2) 도메인/해석 컬럼 자리 만들기 (수동 입력용)\n",
    "df[\"Dominant domain\"] = \"\"\n",
    "df[\"Key top features\"] = \"\"\n",
    "df[\"Interpretation summary\"] = \"\"\n",
    "\n",
    "# (3) 정렬 및 보기 좋은 라벨링\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"label\": \"Outcome (label)\",\n",
    "        \"cluster\": \"Cluster\",\n",
    "        \"n\": \"N\",\n",
    "        \"event_rate\": \"Event rate\",\n",
    "        \"mean_y_prob\": \"Mean predicted prob\",\n",
    "        \"p_event_rate(chi2)\": \"p (event rate, χ²)\",\n",
    "        \"p_yprob(kruskal)\": \"p (pred prob, KW)\",\n",
    "    }\n",
    ")\n",
    "df = df[\n",
    "    [\n",
    "        \"Outcome (label)\",\n",
    "        \"Cluster\",\n",
    "        \"N\",\n",
    "        \"Event rate\",\n",
    "        \"Mean predicted prob\",\n",
    "        \"Delta(abs)\",\n",
    "        \"Direction\",\n",
    "        \"Dominant domain\",\n",
    "        \"Key top features\",\n",
    "        \"Interpretation summary\",\n",
    "        \"p (event rate, χ²)\",\n",
    "        \"p (pred prob, KW)\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# ---------- Excel 저장 ----------\n",
    "with pd.ExcelWriter(out_path) as writer:\n",
    "    for lb, dsub in df.groupby(\"Outcome (label)\"):\n",
    "        dsub.to_excel(writer, sheet_name=lb, index=False)\n",
    "    # 전체 통합 시트\n",
    "    df.to_excel(writer, sheet_name=\"All_labels_combined\", index=False)\n",
    "\n",
    "print(f\"✅ Saved Table 3 summary Excel → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e281791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 경로 설정 =====\n",
    "base = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "\n",
    "\n",
    "# ===== Domain 분류 함수 =====\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = feat.lower()\n",
    "    if any(k in t for k in [\"aggression\", \"support\", \"social\", \"interpersonal\", \"llm\"]):\n",
    "        return \"SDoMH\"\n",
    "    if any(k in t for k in [\"er visit\", \"≥2 er\", \"≥3 er\", \"admission\", \"admit\"]):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"k(\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"alc\",\n",
    "            \"lymphocyte\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"hcl\",\n",
    "            \"asrs\",\n",
    "            \"ham\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"treatment\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# ===== 초기화 =====\n",
    "labels = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "records_summary = []  # Table 4\n",
    "etable_dict = {}  # eTable S4 outcome별 sheet 저장\n",
    "\n",
    "# ===== outcome별 반복 =====\n",
    "for lb in labels:\n",
    "    fpath = base / lb / f\"{lb}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    print(\"using:\", fpath)\n",
    "    df = pd.read_csv(fpath, index_col=0)\n",
    "\n",
    "    # --- eTable S4용 wide-format 저장\n",
    "    df_wide = df.copy()\n",
    "    df_wide[\"Feature\"] = df_wide.index\n",
    "    etable_dict[lb] = df_wide.T  # transpose해서 Feature가 column으로 가도록 저장\n",
    "\n",
    "    # --- Table 4용 계산\n",
    "    for col in df.columns:\n",
    "        vals = df[col].values\n",
    "        # Kruskal–Wallis 검정 (클러스터 간 이질성 유의성)\n",
    "        try:\n",
    "            stat, pval = kruskal(*[df[col][df.index == c] for c in df.index])\n",
    "        except Exception:\n",
    "            pval = None\n",
    "\n",
    "        records_summary.append(\n",
    "            {\n",
    "                \"Outcome\": lb,\n",
    "                \"Feature\": col,\n",
    "                \"Mean(|SHAP|)\": vals.mean(),\n",
    "                \"SD\": vals.std(),\n",
    "                \"Range\": vals.max() - vals.min(),\n",
    "                \"Δ(Top−Bottom)\": vals.max() - vals.min(),\n",
    "                \"Max cluster\": df.index[vals.argmax()],\n",
    "                \"Min cluster\": df.index[vals.argmin()],\n",
    "                \"Top vs Bottom\": f\"{df.index[vals.argmax()]} > {df.index[vals.argmin()]}\",\n",
    "                \"p(KW)\": pval,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# ===== Table 4 (요약) =====\n",
    "df_table4 = pd.DataFrame(records_summary)\n",
    "df_table4[\"Domain\"] = df_table4[\"Feature\"].apply(infer_domain)\n",
    "df_table4 = df_table4.sort_values([\"Outcome\", \"SD\"], ascending=[True, False])\n",
    "\n",
    "# ===== 저장 =====\n",
    "out_summary = base / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "out_wide = base / \"eTableS4_SHAP_ClusterMatrix.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(out_summary) as writer:\n",
    "    df_table4.to_excel(writer, sheet_name=\"All_outcomes\", index=False)\n",
    "    for lb, sub in df_table4.groupby(\"Outcome\"):\n",
    "        sub.to_excel(writer, sheet_name=lb, index=False)\n",
    "\n",
    "with pd.ExcelWriter(out_wide) as writer:\n",
    "    for lb, dfw in etable_dict.items():\n",
    "        dfw.to_excel(writer, sheet_name=lb)\n",
    "\n",
    "print(f\"✅ Saved Table 4 summary → {out_summary}\")\n",
    "print(f\"✅ Saved eTable S4 matrix → {out_wide}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 경로 =====\n",
    "base = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "\n",
    "# Table4 요약 엑셀\n",
    "f_table4 = base / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "df = pd.read_excel(f_table4, sheet_name=\"All_outcomes\")\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# ===== (A) Feature별 Boxplot (x=cluster, y=mean |SHAP|) =====\n",
    "for outcome in [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]:\n",
    "    target_outcome = outcome\n",
    "    fpath = (\n",
    "        base / target_outcome / f\"{target_outcome}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    )\n",
    "    df_feat = pd.read_csv(fpath, index_col=0)\n",
    "    df_feat.index = [\n",
    "        c.replace(\"cluster_\", \"C\") for c in df_feat.index\n",
    "    ]  # cluster_0 → C0\n",
    "\n",
    "    # ===== (1) Dotplot (by Feature 색상) =====\n",
    "    df_melt = df_feat.T.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Cluster\", value_name=\"Mean |SHAP|\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = sns.stripplot(\n",
    "        x=\"Cluster\",\n",
    "        y=\"Mean |SHAP|\",\n",
    "        hue=\"index\",\n",
    "        data=df_melt,\n",
    "        dodge=True,\n",
    "        size=6,\n",
    "        jitter=False,\n",
    "        palette=\"tab10\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    # ➊ 카테고리 순서(좌→우) 얻기\n",
    "    cats = [t.get_text() for t in ax.get_xticklabels()]\n",
    "\n",
    "    # ➋ 각 카테고리 경계(정수 + 0.5 위치)에 점선 추가\n",
    "    for i in range(len(cats) - 1):\n",
    "        ax.axvline(i + 0.5, ls=\"--\", lw=0.8, color=\"gray\", alpha=0.5, zorder=0)\n",
    "\n",
    "    # ➌ 레이아웃/레이블\n",
    "    ax.set_title(\n",
    "        f\"{target_outcome} — Cluster mean |SHAP| per Feature\",\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Mean |SHAP|\")\n",
    "    plt.xticks()  # rotation=30\n",
    "    plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base / f\"Fig5A_{target_outcome}_dotplot_byFeature_guides.png\", dpi=300)\n",
    "\n",
    "    # ===== (2) Top N feature subplot 버전 =====\n",
    "    top_feats = df_feat.columns[:6]  # 상위 6개 feature 예시\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
    "\n",
    "    for ax, feat in zip(axes.flat, top_feats):\n",
    "        sns.lineplot(x=df_feat.index, y=df_feat[feat], marker=\"o\", color=\"black\", ax=ax)\n",
    "        ax.set_title(feat, fontsize=10)\n",
    "        ax.set_xlabel(\"Cluster\")\n",
    "        ax.set_ylabel(\"Mean |SHAP|\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")  # rotation=30\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"{target_outcome} — Cluster mean |SHAP| (Top 6 features)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(base / f\"Fig5A_{target_outcome}_TopFeatures_clean.png\", dpi=300)\n",
    "\n",
    "# ===== (B) Δ(Top−Bottom) Barplot (feature별 이질성 순위) =====\n",
    "topN = 20  # 상위 20개 표시 (필요시 조절)\n",
    "df_rank = df.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "\n",
    "plt.figure(figsize=(7.5, 6.5))\n",
    "sns.barplot(\n",
    "    y=\"Feature\",\n",
    "    x=\"Δ(Top−Bottom)\",\n",
    "    hue=\"Domain\",\n",
    "    data=df_rank,\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    ")\n",
    "plt.title(\n",
    "    \"Top features by SHAP heterogeneity (Δ Top−Bottom)\", fontsize=12, weight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Δ (max−min mean |SHAP| across clusters)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Domain\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base / \"Fig5B_SHAP_Heterogeneity_Rank.png\", dpi=300)\n",
    "\n",
    "# ===== (C) Domain별 평균 SD 막대그래프 =====\n",
    "df_dom = (\n",
    "    df.groupby(\"Domain\", as_index=False)[\"SD\"].mean().sort_values(\"SD\", ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6.5, 4.2))\n",
    "sns.barplot(y=\"Domain\", x=\"SD\", data=df_dom, palette=palette)\n",
    "plt.title(\"Average SHAP heterogeneity (SD) by Domain\", fontsize=12, weight=\"bold\")\n",
    "plt.xlabel(\"Mean SD of mean |SHAP| across clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(base / \"Fig5C_Domain_SHAP_Heterogeneity.png\", dpi=300)\n",
    "\n",
    "print(\"✅ Figures saved:\")\n",
    "print(\" -\", base / f\"Fig5A_{target_outcome}_boxplot.png\")\n",
    "print(\" -\", base / \"Fig5B_SHAP_Heterogeneity_Rank.png\")\n",
    "\n",
    "print(\" -\", base / \"Fig5C_Domain_SHAP_Heterogeneity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55690ed",
   "metadata": {},
   "source": [
    "# Raw SHAP 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 샘플별 SHAP 추출 & 저장 ===\n",
    "OUT_ROOT = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters/SHAP\")\n",
    ")\n",
    "\n",
    "\n",
    "def export_sample_shap(label: str, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) SHAP/예측 로드 (이미 있는 함수들 활용)\n",
    "    expl, vals, feat_names, y_prob, y_true, X_te_t, df_pred = get_shap_matrix(label)\n",
    "\n",
    "    # 2) 메타(정답/확률/예측) 정리\n",
    "    meta = pd.DataFrame(\n",
    "        {\n",
    "            \"sample_id\": np.arange(vals.shape[0], dtype=int),\n",
    "            \"y_true\": np.asarray(y_true, dtype=int),\n",
    "            \"y_prob\": np.asarray(y_prob, dtype=float),\n",
    "        }\n",
    "    )\n",
    "    if \"y_pred\" in df_pred.columns:\n",
    "        meta[\"y_pred\"] = df_pred[\"y_pred\"].astype(int)\n",
    "    else:\n",
    "        # 필요 시 Youden threshold로 예측치 생성\n",
    "        thr = _try_read_youden_threshold(label, base_dir=OUT_ROOT.parent)\n",
    "        if thr is None:\n",
    "            thr = _compute_youden_from_preds(\n",
    "                meta[\"y_true\"].values, meta[\"y_prob\"].values\n",
    "            )\n",
    "        meta[\"y_pred\"] = (meta[\"y_prob\"] >= thr).astype(int)\n",
    "\n",
    "    # 3) 샘플×피처 SHAP (wide)\n",
    "    df_shap_wide = pd.DataFrame(vals, columns=feat_names)\n",
    "    df_wide = pd.concat([meta, df_shap_wide], axis=1)\n",
    "\n",
    "    # 4) (선택) 클러스터 라벨 붙이기: 이미 DBSCAN 돌렸다면 라벨 파일을 자동 병합\n",
    "    lab_path = OUT_ROOT / label / f\"{label}__dbscan_labels.csv\"\n",
    "    if lab_path.exists():\n",
    "        lab = pd.read_csv(lab_path)  # columns: idx, cluster\n",
    "        lab = lab.rename(columns={\"idx\": \"sample_id\"})\n",
    "        df_wide = df_wide.merge(lab, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "    # 5) 저장 (wide & long)\n",
    "    out_wide_csv = OUT_ROOT / label / f\"{label}__sample_SHAP_wide.csv\"\n",
    "    out_wide_parq = OUT_ROOT / label / f\"{label}__sample_SHAP_wide.parquet\"\n",
    "    df_wide.to_csv(out_wide_csv, index=False)\n",
    "    try:\n",
    "        df_wide.to_parquet(out_wide_parq, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # long 포맷 (원하면 주석 해제; 용량 커질 수 있음)\n",
    "    # df_long = df_wide.melt(\n",
    "    #     id_vars=[c for c in [\"sample_id\",\"y_true\",\"y_prob\",\"y_pred\",\"cluster\"] if c in df_wide.columns],\n",
    "    #     var_name=\"feature\",\n",
    "    #     value_name=\"shap_value\"\n",
    "    # )\n",
    "    # df_long.to_parquet(OUT_ROOT / label / f\"{label}__sample_SHAP_long.parquet\", index=False)\n",
    "\n",
    "    print(f\"✅ saved: {out_wide_csv}\")\n",
    "    return df_wide\n",
    "\n",
    "\n",
    "for label in [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]:\n",
    "    export_sample_shap(label, OUT_ROOT / label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b06451",
   "metadata": {},
   "source": [
    "# Cluster 유의성 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kruskal, mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219fbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 경로\n",
    "# --------------------------\n",
    "ROOT = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "SHAP_ROOT = ROOT / \"SHAP\"\n",
    "DBSCAN_ROOT = ROOT\n",
    "SIG_ROOT = DBSCAN_ROOT / \"cluster_sig\"\n",
    "SIG_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff82d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 유틸\n",
    "# --------------------------\n",
    "META_COLS = {\"sample_id\", \"y_true\", \"y_prob\", \"y_pred\", \"cluster\"}\n",
    "\n",
    "\n",
    "def _feature_columns(df: pd.DataFrame):\n",
    "    return [c for c in df.columns if c not in META_COLS]\n",
    "\n",
    "\n",
    "def _all_identical(groups):\n",
    "    cat = np.concatenate([g.ravel() for g in groups if len(g) > 0])\n",
    "    return cat.size == 0 or np.nanstd(cat) == 0.0\n",
    "\n",
    "\n",
    "def _safe_mw(a, b):\n",
    "    if len(a) < 2 or len(b) < 2:\n",
    "        return np.nan, np.nan\n",
    "    if _all_identical([a, b]):\n",
    "        return 1.0, 0.0\n",
    "    p = mannwhitneyu(a, b, alternative=\"two-sided\").pvalue\n",
    "    # Cliff's delta\n",
    "    gt = sum(x > y for x in a for y in b)\n",
    "    lt = sum(x < y for x in a for y in b)\n",
    "    d = (gt - lt) / (len(a) * len(b))\n",
    "    return p, d\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 1) SHAP × Cluster 병합\n",
    "# --------------------------\n",
    "def attach_cluster_fast(label: str) -> Path:\n",
    "    \"\"\"\n",
    "    sample_id == idx 가정 하에 DBSCAN 라벨을 병합.\n",
    "    병합된 CSV를 SHAP_ROOT/<label>/에 저장하고 그 경로를 반환.\n",
    "    \"\"\"\n",
    "    shap_dir = SHAP_ROOT / label\n",
    "    dbscan_dir = DBSCAN_ROOT / label\n",
    "    shap_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    f_shap = shap_dir / f\"{label}__sample_SHAP_wide.csv\"\n",
    "    f_lab = dbscan_dir / f\"{label}__dbscan_labels.csv\"\n",
    "    assert f_shap.exists(), f\"Not found: {f_shap}\"\n",
    "    assert f_lab.exists(), f\"Not found: {f_lab}\"\n",
    "\n",
    "    df = pd.read_csv(f_shap)\n",
    "    lab = pd.read_csv(f_lab)  # columns: idx, cluster\n",
    "\n",
    "    merged = df.merge(\n",
    "        lab.rename(columns={\"idx\": \"sample_id\"}), on=\"sample_id\", how=\"left\"\n",
    "    )\n",
    "    cov = merged[\"cluster\"].notna().mean()\n",
    "    print(f\"[{label}] coverage by sample_id==idx: {cov*100:.1f}%\")\n",
    "\n",
    "    out = shap_dir / f\"{label}__sample_SHAP_wide_with_cluster.csv\"\n",
    "    merged.to_csv(out, index=False)\n",
    "    print(f\"✅ saved (merged): {out}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) 유의성 검정 (Kruskal + 모든 pairwise)\n",
    "#    - noise(-1) 제외 기본값\n",
    "#    - pairwise FDR은 '피처별'로 보정\n",
    "# --------------------------\n",
    "def run_significance_full(\n",
    "    label: str, use_abs: bool = True, include_noise: bool = False\n",
    ") -> Path:\n",
    "    f_merged = SHAP_ROOT / label / f\"{label}__sample_SHAP_wide_with_cluster.csv\"\n",
    "    df = pd.read_csv(f_merged)\n",
    "    if not include_noise:\n",
    "        df = df[df[\"cluster\"] != -1].copy()\n",
    "    print(f\"[{label}] Used {len(df)} samples (exclude noise={not include_noise})\")\n",
    "\n",
    "    feat_cols = _feature_columns(df)\n",
    "    clusters = sorted(df[\"cluster\"].dropna().unique())\n",
    "\n",
    "    # ---- (A) Kruskal ----\n",
    "    rows_kw = []\n",
    "    for feat in feat_cols:\n",
    "        vals = np.abs(df[feat].values) if use_abs else df[feat].values\n",
    "        groups = [vals[df[\"cluster\"].values == c] for c in clusters]\n",
    "        if len(groups) < 2 or any(len(g) < 2 for g in groups) or _all_identical(groups):\n",
    "            p_kw = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                p_kw = kruskal(*groups).pvalue\n",
    "            except Exception:\n",
    "                p_kw = np.nan\n",
    "        rows_kw.append({\"Feature\": feat, \"p_kw\": p_kw})\n",
    "    kw = pd.DataFrame(rows_kw)\n",
    "    kw[\"q_kw\"] = multipletests(kw[\"p_kw\"].fillna(1.0), method=\"fdr_bh\")[1]\n",
    "\n",
    "    # 보조 요약(중앙값 기반 Top/Bottom 및 범위)\n",
    "    if len(clusters) >= 1:\n",
    "        med = df.groupby(\"cluster\")[feat_cols].mean()\n",
    "        rng = med.max(0) - med.min(0)\n",
    "        top_c = med.idxmax()\n",
    "        bot_c = med.idxmin()\n",
    "        kw[\"Δ(Top−Bottom)\"] = kw[\"Feature\"].map(rng.to_dict())\n",
    "        kw[\"Top cluster\"] = kw[\"Feature\"].map(top_c.to_dict())\n",
    "        kw[\"Bottom cluster\"] = kw[\"Feature\"].map(bot_c.to_dict())\n",
    "        kw[\"Top vs Bottom\"] = (\n",
    "            kw[\"Top cluster\"].astype(str) + \" > \" + kw[\"Bottom cluster\"].astype(str)\n",
    "        )\n",
    "\n",
    "    # ---- (B) Pairwise 모든 클러스터 쌍 (피처별 FDR) ----\n",
    "    pair_tables = []\n",
    "    for feat in feat_cols:\n",
    "        vals = np.abs(df[feat].values) if use_abs else df[feat].values\n",
    "        rows = []\n",
    "        for c1, c2 in combinations(clusters, 2):\n",
    "            a, b = vals[df[\"cluster\"] == c1], vals[df[\"cluster\"] == c2]\n",
    "            p, d = _safe_mw(a, b)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Feature\": feat,\n",
    "                    \"Cluster1\": c1,\n",
    "                    \"Cluster2\": c2,\n",
    "                    \"p_raw\": p,\n",
    "                    \"cliffs_delta\": d,\n",
    "                    \"n1\": len(a),\n",
    "                    \"n2\": len(b),\n",
    "                }\n",
    "            )\n",
    "        pw_feat = pd.DataFrame(rows)\n",
    "        if len(pw_feat) > 0:\n",
    "            pw_feat[\"q_fdr\"] = multipletests(\n",
    "                pw_feat[\"p_raw\"].fillna(1.0), method=\"fdr_bh\"\n",
    "            )[1]\n",
    "        pair_tables.append(pw_feat)\n",
    "    pw = (\n",
    "        pd.concat(pair_tables, ignore_index=True)\n",
    "        if pair_tables\n",
    "        else pd.DataFrame(\n",
    "            columns=[\n",
    "                \"Feature\",\n",
    "                \"Cluster1\",\n",
    "                \"Cluster2\",\n",
    "                \"p_raw\",\n",
    "                \"cliffs_delta\",\n",
    "                \"n1\",\n",
    "                \"n2\",\n",
    "                \"q_fdr\",\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ---- (C) 저장 ----\n",
    "    out_xlsx = SIG_ROOT / f\"{label}__SHAP_cluster_significance_full.xlsx\"\n",
    "    with pd.ExcelWriter(out_xlsx) as xw:\n",
    "        kw.to_excel(xw, sheet_name=\"Kruskal\", index=False)\n",
    "        pw.to_excel(xw, sheet_name=\"Pairwise_All\", index=False)\n",
    "    print(f\"✅ saved full pairwise significance: {out_xlsx}\")\n",
    "    return out_xlsx\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) 전체 outcome 일괄 실행\n",
    "# --------------------------\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "\n",
    "def run_all(labels=LABELS):\n",
    "    for lb in labels:\n",
    "        try:\n",
    "            attach_cluster_fast(lb)\n",
    "            run_significance_full(\n",
    "                lb, use_abs=True, include_noise=False\n",
    "            )  # |SHAP| 기준, 노이즈 제외\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {lb}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72d716",
   "metadata": {},
   "source": [
    "## Table 및 시각화 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kruskal  # 기존 코드에서 사용\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 경로 설정 -----\n",
    "RAWNORM_BASE = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/260114_qwen/modeling/step2_modeling/simple_20/figures/clinic_interpretation/PatientClusters\")\n",
    ")\n",
    "SIG_ROOT = RAWNORM_BASE / \"cluster_sig\"\n",
    "FIGTAB_ROOT = RAWNORM_BASE / \"cluster_figure_table\"\n",
    "FIGTAB_ROOT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 유틸 -----\n",
    "def stars_from_q(q):\n",
    "    if pd.isna(q):\n",
    "        return \"\"\n",
    "    if q < 0.001:\n",
    "        return \"***\"\n",
    "    if q < 0.01:\n",
    "        return \"**\"\n",
    "    if q < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_cluster_str(s):\n",
    "    # \"cluster_3\" -> 3 / \"C3\" -> 3\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    m = re.search(r\"(-?\\d+)\", str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "# ===== Domain 분류 함수 (기존과 동일) =====\n",
    "# ===== Domain 분류 함수 =====\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = feat.lower()\n",
    "    if any(k in t for k in [\"aggression\", \"support\", \"social\", \"interpersonal\", \"llm\"]):\n",
    "        return \"SDoMH\"\n",
    "    if any(k in t for k in [\"er visit\", \"≥2 er\", \"≥3 er\", \"admission\", \"admit\"]):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"k(\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"alc\",\n",
    "            \"lymphocyte\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"hcl\",\n",
    "            \"asrs\",\n",
    "            \"ham\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"treatment\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# ===== 1) 기존 Table4 요약 읽기 =====\n",
    "f_table4 = RAWNORM_BASE / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "df_all = pd.read_excel(f_table4, sheet_name=\"All_outcomes\")\n",
    "# 안전하게 Domain 재계산(혹시 비어있을 경우)\n",
    "if \"Domain\" not in df_all.columns:\n",
    "    df_all[\"Domain\"] = df_all[\"Feature\"].apply(infer_domain)\n",
    "\n",
    "# ===== 2) 유의성 결과(각 label)와 결합 → 메인 테이블 확장 =====\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "# 결과 컬럼 초기화\n",
    "for col in [\"p_kw\", \"q_kw\", \"p_tb\", \"q_tb\", \"cliffs_delta\", \"sig_kw\", \"sig_tb\"]:\n",
    "    if col not in df_all.columns:\n",
    "        df_all[col] = np.nan\n",
    "\n",
    "# label별로 KW + Pairwise에서 Top-Bottom p를 주입\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if not sig_path.exists():\n",
    "        print(f\"[WARN] missing significance file: {sig_path}\")\n",
    "        continue\n",
    "    df_kw = pd.read_excel(sig_path, sheet_name=\"Kruskal\")\n",
    "    df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "\n",
    "    # 매핑(dict) 준비\n",
    "    qkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"q_kw\"]))\n",
    "    pkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"p_kw\"]))\n",
    "\n",
    "    # df_all 중 해당 outcome만 추림\n",
    "    m_out = df_all[\"Outcome\"] == lb\n",
    "    sub = df_all.loc[m_out].copy()\n",
    "\n",
    "    # 각 row에서 Top/Bottom 클러스터 번호 파싱 후 pairwise에서 해당 쌍의 p/q/효과크기 찾기\n",
    "    p_tb_list, q_tb_list, d_tb_list, sig_tb_list = [], [], [], []\n",
    "    p_kw_list, q_kw_list, sig_kw_list = [], [], []\n",
    "    for _, row in sub.iterrows():\n",
    "        feat = row[\"Feature\"]\n",
    "        # KW\n",
    "        pkw = pkw_map.get(feat, np.nan)\n",
    "        qkw = qkw_map.get(feat, np.nan)\n",
    "        sig_kw = stars_from_q(qkw)\n",
    "\n",
    "        # Top/Bottom 클러스터\n",
    "        topc = parse_cluster_str(row.get(\"Max cluster\") or row.get(\"Top cluster\") or \"\")\n",
    "        botc = parse_cluster_str(\n",
    "            row.get(\"Min cluster\") or row.get(\"Bottom cluster\") or \"\"\n",
    "        )\n",
    "\n",
    "        # pairwise에서 두 방향(Cluster1,Cluster2) 모두 확인\n",
    "        df_feat = df_pw[df_pw[\"Feature\"] == feat]\n",
    "        hit = df_feat[\n",
    "            ((df_feat[\"Cluster1\"] == topc) & (df_feat[\"Cluster2\"] == botc))\n",
    "            | ((df_feat[\"Cluster1\"] == botc) & (df_feat[\"Cluster2\"] == topc))\n",
    "        ]\n",
    "        if len(hit):\n",
    "            p_tb = float(hit[\"p_raw\"].iloc[0])\n",
    "            q_tb = float(hit[\"q_fdr\"].iloc[0])\n",
    "            d_tb = float(hit[\"cliffs_delta\"].iloc[0])\n",
    "            sig_tb = stars_from_q(q_tb)\n",
    "        else:\n",
    "            p_tb = q_tb = d_tb = np.nan\n",
    "            sig_tb = \"\"\n",
    "\n",
    "        p_kw_list.append(pkw)\n",
    "        q_kw_list.append(qkw)\n",
    "        sig_kw_list.append(sig_kw)\n",
    "        p_tb_list.append(p_tb)\n",
    "        q_tb_list.append(q_tb)\n",
    "        d_tb_list.append(d_tb)\n",
    "        sig_tb_list.append(sig_tb)\n",
    "\n",
    "    # 주입\n",
    "    df_all.loc[m_out, \"p_kw\"] = p_kw_list\n",
    "    df_all.loc[m_out, \"q_kw\"] = q_kw_list\n",
    "    df_all.loc[m_out, \"sig_kw\"] = sig_kw_list\n",
    "    df_all.loc[m_out, \"p_tb\"] = p_tb_list\n",
    "    df_all.loc[m_out, \"q_tb\"] = q_tb_list\n",
    "    df_all.loc[m_out, \"cliffs_delta\"] = d_tb_list\n",
    "    df_all.loc[m_out, \"sig_tb\"] = sig_tb_list\n",
    "\n",
    "# 저장(메인 테이블)\n",
    "out_table_main = FIGTAB_ROOT / \"Table4_with_significance.xlsx\"\n",
    "with pd.ExcelWriter(out_table_main) as xw:\n",
    "    df_all.to_excel(xw, sheet_name=\"All_outcomes\", index=False)\n",
    "    for lb, sub in df_all.groupby(\"Outcome\"):\n",
    "        sub.to_excel(xw, sheet_name=lb, index=False)\n",
    "print(\"✅ Saved main table with significance →\", out_table_main)\n",
    "\n",
    "# ===== 3) 서플: 모든 쌍 pairwise를 그대로 outcome별로 CSV 저장 =====\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if sig_path.exists():\n",
    "        df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "        df_pw.to_csv(FIGTAB_ROOT / f\"Supp_Pairwise_All_{lb}.csv\", index=False)\n",
    "\n",
    "# ===== 4) 그림에 별표 추가 (KW 기준) =====\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# (A) Feature별 Dotplot: 범례(Feature 이름)에 별표 붙이기 (q_kw 기준)\n",
    "for outcome in LABELS:\n",
    "    # 히트맵 수치 파일 로드\n",
    "    fpath = RAWNORM_BASE / outcome / f\"{outcome}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    if not fpath.exists():\n",
    "        print(f\"[WARN] missing heatmap CSV: {fpath}\")\n",
    "        continue\n",
    "    df_feat = pd.read_csv(fpath, index_col=0)\n",
    "    # C 라벨로 보기 좋게\n",
    "    df_feat.index = [c.replace(\"cluster_\", \"C\") for c in df_feat.index]\n",
    "\n",
    "    # 해당 outcome의 q_kw 맵(Feature→별표)\n",
    "    sub_main = df_all[df_all[\"Outcome\"] == outcome]\n",
    "    qkw_map = dict(zip(sub_main[\"Feature\"], sub_main[\"q_kw\"]))\n",
    "    star_map = {f: stars_from_q(q) for f, q in qkw_map.items()}\n",
    "\n",
    "    # legend용 라벨 교체를 위해 컬럼명을 \"원래+별\"로 임시 재라벨링\n",
    "    feat_renamed = {\n",
    "        col: (f\"{col}{star_map.get(col,'')}\" if col in star_map else col)\n",
    "        for col in df_feat.columns\n",
    "    }\n",
    "    df_feat_renamed = df_feat.rename(columns=feat_renamed)\n",
    "\n",
    "    # Dotplot\n",
    "    df_melt = df_feat_renamed.T.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Cluster\", value_name=\"Mean |SHAP|\"\n",
    "    )\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.stripplot(\n",
    "        x=\"Cluster\",\n",
    "        y=\"Mean |SHAP|\",\n",
    "        hue=\"index\",\n",
    "        data=df_melt,\n",
    "        dodge=True,\n",
    "        size=6,\n",
    "        jitter=False,\n",
    "        palette=\"tab10\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    # 경계선 가이드\n",
    "    cats = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    for i in range(len(cats) - 1):\n",
    "        ax.axvline(i + 0.5, ls=\"--\", lw=0.8, color=\"gray\", alpha=0.5, zorder=0)\n",
    "    ax.set_title(\n",
    "        f\"{outcome} — Cluster mean |SHAP| per Feature\", fontsize=11, weight=\"bold\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Mean |SHAP|\")\n",
    "    plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_dotplot_byFeature_guides_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # (B) Top N feature subplot: 제목에 별표 붙이기 (KW 기준)\n",
    "    top_feats = df_feat.columns[:6]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
    "    for ax, feat in zip(axes.flat, top_feats):\n",
    "        star = star_map.get(feat, \"\")\n",
    "        sns.lineplot(x=df_feat.index, y=df_feat[feat], marker=\"o\", color=\"black\", ax=ax)\n",
    "        ax.set_title(f\"{feat}{star}\", fontsize=10)\n",
    "        ax.set_xlabel(\"Cluster\")\n",
    "        ax.set_ylabel(\"Mean |SHAP|\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")\n",
    "    plt.suptitle(\n",
    "        f\"{outcome} — Cluster mean |SHAP| (Top 6 features)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_TopFeatures_clean_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "# (B) Δ(Top−Bottom) barplot & (C) Domain barplot은 기존 그림 유지\n",
    "#    (원하면 bar label에 별표 추가 가능: df_all에서 원하는 조건으로 star를 조합)\n",
    "# 그대로 복제해서 저장 위치만 figure_table로 변경:\n",
    "\n",
    "# Δ(Top−Bottom) TopN\n",
    "topN = 20\n",
    "df_rank = df_all.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "plt.figure(figsize=(7.5, 6.5))\n",
    "sns.barplot(\n",
    "    y=\"Feature\",\n",
    "    x=\"Δ(Top−Bottom)\",\n",
    "    hue=\"Domain\",\n",
    "    data=df_rank,\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    ")\n",
    "plt.title(\n",
    "    \"Top features by SHAP heterogeneity (Δ Top−Bottom)\", fontsize=12, weight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Δ (max−min mean |SHAP| across clusters)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Domain\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5B_SHAP_Heterogeneity_Rank.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Domain 별 평균 SD\n",
    "df_dom = (\n",
    "    df_all.groupby(\"Domain\", as_index=False)[\"SD\"]\n",
    "    .mean()\n",
    "    .sort_values(\"SD\", ascending=False)\n",
    ")\n",
    "plt.figure(figsize=(6.5, 4.2))\n",
    "sns.barplot(y=\"Domain\", x=\"SD\", data=df_dom, palette=palette)\n",
    "plt.title(\"Average SHAP heterogeneity (SD) by Domain\", fontsize=12, weight=\"bold\")\n",
    "plt.xlabel(\"Mean SD of mean |SHAP| across clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5C_Domain_SHAP_Heterogeneity.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Outputs saved to:\", FIGTAB_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b31bd1",
   "metadata": {},
   "source": [
    "- ER, AD 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGTAB_ROOT = RAWNORM_BASE / \"cluster_figure_table/no_util\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 유틸 -----\n",
    "def stars_from_q(q):\n",
    "    if pd.isna(q):\n",
    "        return \"\"\n",
    "    if q < 0.001:\n",
    "        return \"***\"\n",
    "    if q < 0.01:\n",
    "        return \"**\"\n",
    "    if q < 0.05:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def parse_cluster_str(s):\n",
    "    # \"cluster_3\" -> 3 / \"C3\" -> 3\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    m = re.search(r\"(-?\\d+)\", str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "# ===== Domain 분류 함수 (기존과 동일) =====\n",
    "# ===== Domain 분류 함수 =====\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = feat.lower()\n",
    "    if any(k in t for k in [\"aggression\", \"support\", \"social\", \"interpersonal\", \"llm\"]):\n",
    "        return \"Aggression / Social function (LLM)\"\n",
    "    if any(k in t for k in [\"er visit\", \"≥2 er\", \"≥3 er\", \"admission\", \"admit\"]):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"k(\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"alc\",\n",
    "            \"lymphocyte\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"hcl\",\n",
    "            \"asrs\",\n",
    "            \"ham\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"treatment\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# ===== 1) 기존 Table4 요약 읽기 =====\n",
    "f_table4 = RAWNORM_BASE / \"Table4_CrossCluster_SHAP_Heterogeneity.xlsx\"\n",
    "df_all = pd.read_excel(f_table4, sheet_name=\"All_outcomes\")\n",
    "# 안전하게 Domain 재계산(혹시 비어있을 경우)\n",
    "if \"Domain\" not in df_all.columns:\n",
    "    df_all[\"Domain\"] = df_all[\"Feature\"].apply(infer_domain)\n",
    "\n",
    "# ===== 2) 유의성 결과(각 label)와 결합 → 메인 테이블 확장 =====\n",
    "LABELS = [\"label_30d\", \"label_60d\", \"label_90d\", \"label_180d\", \"label_365d\"]\n",
    "\n",
    "# 결과 컬럼 초기화\n",
    "for col in [\"p_kw\", \"q_kw\", \"p_tb\", \"q_tb\", \"cliffs_delta\", \"sig_kw\", \"sig_tb\"]:\n",
    "    if col not in df_all.columns:\n",
    "        df_all[col] = np.nan\n",
    "\n",
    "# label별로 KW + Pairwise에서 Top-Bottom p를 주입\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if not sig_path.exists():\n",
    "        print(f\"[WARN] missing significance file: {sig_path}\")\n",
    "        continue\n",
    "    df_kw = pd.read_excel(sig_path, sheet_name=\"Kruskal\")\n",
    "    df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "\n",
    "    # 매핑(dict) 준비\n",
    "    qkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"q_kw\"]))\n",
    "    pkw_map = dict(zip(df_kw[\"Feature\"], df_kw[\"p_kw\"]))\n",
    "\n",
    "    # df_all 중 해당 outcome만 추림\n",
    "    m_out = df_all[\"Outcome\"] == lb\n",
    "    sub = df_all.loc[m_out].copy()\n",
    "\n",
    "    # 각 row에서 Top/Bottom 클러스터 번호 파싱 후 pairwise에서 해당 쌍의 p/q/효과크기 찾기\n",
    "    p_tb_list, q_tb_list, d_tb_list, sig_tb_list = [], [], [], []\n",
    "    p_kw_list, q_kw_list, sig_kw_list = [], [], []\n",
    "    for _, row in sub.iterrows():\n",
    "        feat = row[\"Feature\"]\n",
    "        # KW\n",
    "        pkw = pkw_map.get(feat, np.nan)\n",
    "        qkw = qkw_map.get(feat, np.nan)\n",
    "        sig_kw = stars_from_q(qkw)\n",
    "\n",
    "        # Top/Bottom 클러스터\n",
    "        topc = parse_cluster_str(row.get(\"Max cluster\") or row.get(\"Top cluster\") or \"\")\n",
    "        botc = parse_cluster_str(\n",
    "            row.get(\"Min cluster\") or row.get(\"Bottom cluster\") or \"\"\n",
    "        )\n",
    "\n",
    "        # pairwise에서 두 방향(Cluster1,Cluster2) 모두 확인\n",
    "        df_feat = df_pw[df_pw[\"Feature\"] == feat]\n",
    "        hit = df_feat[\n",
    "            ((df_feat[\"Cluster1\"] == topc) & (df_feat[\"Cluster2\"] == botc))\n",
    "            | ((df_feat[\"Cluster1\"] == botc) & (df_feat[\"Cluster2\"] == topc))\n",
    "        ]\n",
    "        if len(hit):\n",
    "            p_tb = float(hit[\"p_raw\"].iloc[0])\n",
    "            q_tb = float(hit[\"q_fdr\"].iloc[0])\n",
    "            d_tb = float(hit[\"cliffs_delta\"].iloc[0])\n",
    "            sig_tb = stars_from_q(q_tb)\n",
    "        else:\n",
    "            p_tb = q_tb = d_tb = np.nan\n",
    "            sig_tb = \"\"\n",
    "\n",
    "        p_kw_list.append(pkw)\n",
    "        q_kw_list.append(qkw)\n",
    "        sig_kw_list.append(sig_kw)\n",
    "        p_tb_list.append(p_tb)\n",
    "        q_tb_list.append(q_tb)\n",
    "        d_tb_list.append(d_tb)\n",
    "        sig_tb_list.append(sig_tb)\n",
    "\n",
    "    # 주입\n",
    "    df_all.loc[m_out, \"p_kw\"] = p_kw_list\n",
    "    df_all.loc[m_out, \"q_kw\"] = q_kw_list\n",
    "    df_all.loc[m_out, \"sig_kw\"] = sig_kw_list\n",
    "    df_all.loc[m_out, \"p_tb\"] = p_tb_list\n",
    "    df_all.loc[m_out, \"q_tb\"] = q_tb_list\n",
    "    df_all.loc[m_out, \"cliffs_delta\"] = d_tb_list\n",
    "    df_all.loc[m_out, \"sig_tb\"] = sig_tb_list\n",
    "\n",
    "# 저장(메인 테이블)\n",
    "out_table_main = FIGTAB_ROOT / \"Table4_with_significance.xlsx\"\n",
    "with pd.ExcelWriter(out_table_main) as xw:\n",
    "    df_all.to_excel(xw, sheet_name=\"All_outcomes\", index=False)\n",
    "    for lb, sub in df_all.groupby(\"Outcome\"):\n",
    "        sub.to_excel(xw, sheet_name=lb, index=False)\n",
    "print(\"✅ Saved main table with significance →\", out_table_main)\n",
    "\n",
    "# ===== 3) 서플: 모든 쌍 pairwise를 그대로 outcome별로 CSV 저장 =====\n",
    "for lb in LABELS:\n",
    "    sig_path = SIG_ROOT / f\"{lb}__SHAP_cluster_significance_full.xlsx\"\n",
    "    if sig_path.exists():\n",
    "        df_pw = pd.read_excel(sig_path, sheet_name=\"Pairwise_All\")\n",
    "        df_pw.to_csv(FIGTAB_ROOT / f\"Supp_Pairwise_All_{lb}.csv\", index=False)\n",
    "\n",
    "# ===== 4) 그림에 별표 추가 (KW 기준) =====\n",
    "palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# (A) Feature별 Dotplot: 범례(Feature 이름)에 별표 붙이기 (q_kw 기준)\n",
    "for outcome in LABELS:\n",
    "    # 히트맵 수치 파일 로드\n",
    "    fpath = RAWNORM_BASE / outcome / f\"{outcome}__heatmap_values_mean__TOP10_RAW.csv\"\n",
    "    if not fpath.exists():\n",
    "        print(f\"[WARN] missing heatmap CSV: {fpath}\")\n",
    "        continue\n",
    "    df_feat = pd.read_csv(fpath, index_col=0)\n",
    "\n",
    "    # 🌟🌟🌟 이 위치에 필터링 코드 추가 🌟🌟🌟\n",
    "    EXCLUDE_VARS = [\"≥2 ER Visits\", \"≥3 Admissions\"]\n",
    "    cols_to_keep = [col for col in df_feat.columns if col not in EXCLUDE_VARS]\n",
    "    df_feat = df_feat[cols_to_keep]\n",
    "    # 🌟🌟🌟 필터링 코드 끝 🌟🌟🌟\n",
    "\n",
    "    # C 라벨로 보기 좋게\n",
    "    df_feat.index = [c.replace(\"cluster_\", \"C\") for c in df_feat.index]\n",
    "\n",
    "    # 해당 outcome의 q_kw 맵(Feature→별표)\n",
    "    sub_main = df_all[df_all[\"Outcome\"] == outcome]\n",
    "    qkw_map = dict(zip(sub_main[\"Feature\"], sub_main[\"q_kw\"]))\n",
    "    star_map = {f: stars_from_q(q) for f, q in qkw_map.items()}\n",
    "\n",
    "    # legend용 라벨 교체를 위해 컬럼명을 \"원래+별\"로 임시 재라벨링\n",
    "    feat_renamed = {\n",
    "        col: (f\"{col}{star_map.get(col,'')}\" if col in star_map else col)\n",
    "        for col in df_feat.columns\n",
    "    }\n",
    "    df_feat_renamed = df_feat.rename(columns=feat_renamed)\n",
    "\n",
    "    # Dotplot\n",
    "    df_melt = df_feat_renamed.T.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Cluster\", value_name=\"Mean |SHAP|\"\n",
    "    )\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.stripplot(\n",
    "        x=\"Cluster\",\n",
    "        y=\"Mean |SHAP|\",\n",
    "        hue=\"index\",\n",
    "        data=df_melt,\n",
    "        dodge=True,\n",
    "        size=6,\n",
    "        jitter=False,\n",
    "        palette=\"tab10\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    # 경계선 가이드\n",
    "    cats = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    for i in range(len(cats) - 1):\n",
    "        ax.axvline(i + 0.5, ls=\"--\", lw=0.8, color=\"gray\", alpha=0.5, zorder=0)\n",
    "    ax.set_title(\n",
    "        f\"{outcome} — Cluster mean |SHAP| per Feature\", fontsize=11, weight=\"bold\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Mean |SHAP|\")\n",
    "    plt.legend(title=\"Feature\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_dotplot_byFeature_guides_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # (B) Top N feature subplot: 제목에 별표 붙이기 (KW 기준)\n",
    "    top_feats = df_feat.columns[:6]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)\n",
    "    for ax, feat in zip(axes.flat, top_feats):\n",
    "        star = star_map.get(feat, \"\")\n",
    "        sns.lineplot(x=df_feat.index, y=df_feat[feat], marker=\"o\", color=\"black\", ax=ax)\n",
    "        ax.set_title(f\"{feat}{star}\", fontsize=10)\n",
    "        ax.set_xlabel(\"Cluster\")\n",
    "        ax.set_ylabel(\"Mean |SHAP|\")\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\")\n",
    "    plt.suptitle(\n",
    "        f\"{outcome} — Cluster mean |SHAP| (Top 6 features)\",\n",
    "        fontsize=13,\n",
    "        weight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(\n",
    "        FIGTAB_ROOT / f\"Fig5A_{outcome}_TopFeatures_clean_withStars.png\", dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "# (B) Δ(Top−Bottom) barplot & (C) Domain barplot은 기존 그림 유지\n",
    "#    (원하면 bar label에 별표 추가 가능: df_all에서 원하는 조건으로 star를 조합)\n",
    "# 그대로 복제해서 저장 위치만 figure_table로 변경:\n",
    "\n",
    "# Δ(Top−Bottom) TopN\n",
    "\n",
    "# 🌟🌟🌟 이 위치에 필터링 코드 추가 🌟🌟🌟\n",
    "EXCLUDE_VARS = [\"AD_more_three\", \"ER_more_two\"]\n",
    "df_all_filtered = df_all[~df_all[\"Feature\"].isin(EXCLUDE_VARS)].copy()\n",
    "# 🌟🌟🌟 필터링 코드 끝 🌟🌟🌟\n",
    "\n",
    "topN = 20\n",
    "# df_rank = df_all.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "df_rank = df_all_filtered.sort_values(\"Δ(Top−Bottom)\", ascending=False).head(topN)\n",
    "plt.figure(figsize=(7.5, 6.5))\n",
    "sns.barplot(\n",
    "    y=\"Feature\",\n",
    "    x=\"Δ(Top−Bottom)\",\n",
    "    hue=\"Domain\",\n",
    "    data=df_rank,\n",
    "    palette=palette,\n",
    "    dodge=False,\n",
    ")\n",
    "plt.title(\n",
    "    \"Top features by SHAP heterogeneity (Δ Top−Bottom)\", fontsize=12, weight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Δ (max−min mean |SHAP| across clusters)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.legend(title=\"Domain\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5B_SHAP_Heterogeneity_Rank.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Domain 별 평균 SD\n",
    "# df_dom = (\n",
    "#     df_all.groupby(\"Domain\", as_index=False)[\"SD\"]\n",
    "#     .mean()\n",
    "#     .sort_values(\"SD\", ascending=False)\n",
    "# )\n",
    "df_all_viz = df_all[~df_all[\"Feature\"].isin(EXCLUDE_VARS)]  # 🌟 필터링된 데이터 사용\n",
    "\n",
    "df_dom = (\n",
    "    df_all_viz.groupby(\"Domain\", as_index=False)[\"SD\"]  # 🌟 df_all_viz 사용\n",
    "    .mean()\n",
    "    .sort_values(\"SD\", ascending=False)\n",
    ")\n",
    "plt.figure(figsize=(6.5, 4.2))\n",
    "sns.barplot(y=\"Domain\", x=\"SD\", data=df_dom, palette=palette)\n",
    "plt.title(\"Average SHAP heterogeneity (SD) by Domain\", fontsize=12, weight=\"bold\")\n",
    "plt.xlabel(\"Mean SD of mean |SHAP| across clusters\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGTAB_ROOT / \"Fig5C_Domain_SHAP_Heterogeneity.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Outputs saved to:\", FIGTAB_ROOT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}