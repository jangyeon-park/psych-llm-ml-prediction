{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Train/Test Split & Imputation\n",
    "\n",
    "Split data 70/30 (stratified on label_30d), then apply four imputation methods:\n",
    "Simple, MissForest, MICE, and Hybrid."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from missforest import MissForest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "from src.config import PROJECT_ROOT, SPLIT_SEED, MODEL_SEED, LABELS\n",
    "from src.variables import CATEGORY_COLS, LLM_COLS, CODE_COLS"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw data and outcome labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Paths (config-based) ──\n",
    "save_base_path = PROJECT_ROOT / \"data/processed_imp/260106_split_corr_LLM_ADER/outcome_dataset\"\n",
    "save_base_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_df = pd.read_csv(\n",
    "    PROJECT_ROOT / \"data/raw/ADER_windowday_dataset_number_with_llm_v2.csv\"\n",
    ")\n",
    "\n",
    "# Outcome labels (from EMR preprocessed data)\n",
    "outcome_root = PROJECT_ROOT.parent / \"Psychiatry/EMR Data/Raw_data/2010-2025/preprocessed_data/outcome\"\n",
    "outcome_dict = {\n",
    "    f\"label_{d}d\": pd.read_csv(outcome_root / f\"outcome_{d}d.csv\")\n",
    "    for d in [30, 60, 90, 180, 365]\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "valid_ids = set(base_df[\"환자번호\"])\n",
    "filtered_outcomes = {}\n",
    "\n",
    "for target, outcome_df in outcome_dict.items():\n",
    "    outcome_filtered = outcome_df[outcome_df[\"환자번호\"].isin(valid_ids)].copy()\n",
    "    filtered_outcomes[target] = outcome_filtered\n",
    "\n",
    "    target_ids = outcome_filtered[\"환자번호\"].unique()\n",
    "    filtered_df = base_df[base_df[\"환자번호\"].isin(target_ids)].copy()\n",
    "\n",
    "    target_save_path = save_base_path / f\"{target}_dataset.csv\"\n",
    "    filtered_df.to_csv(target_save_path, index=False, encoding=\"utf-8-sig\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for label_name, df in filtered_outcomes.items():\n",
    "    print(f\"\\n{label_name}: {df.shape[0]} samples\")\n",
    "    print(df[label_name].value_counts())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variable removal (>60% missing, zero variance, correlation >0.7)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "missing_cols = [\"BL3125\", \"BL3137\", \"NR4303\"]\n",
    "zero_var_cols = [\"BL201801\", \"BL201802\", \"BL201812\", \"BL201813\", \"BL201818\"]\n",
    "corr_cols = [\n",
    "    \"ASI-3-Score\", \"Agoraphobia-Score\", \"BDI-II-Score\", \"BL2011\", \"BL2012\",\n",
    "    \"BL2013\", \"BL201401\", \"BL201803\", \"BL201804\", \"BL201806\", \"BL3111\",\n",
    "    \"BL311201\", \"BL312002\", \"BL314201\", \"Cognitive-Score\", \"D\", \"F\", \"F(B)\",\n",
    "    \"FullScaleIQ-Compositescore\", \"HAM-A-Score\", \"Hs\", \"Interoceptive-Score\",\n",
    "    \"K\", \"LSAS-SR-Score\", \"PHQ-15-Score\", \"PSWQ-Score\", \"Pa\",\n",
    "    \"Physical-Score\", \"Pt\", \"Social-Score\", \"Socialphobia-Score\",\n",
    "]\n",
    "\n",
    "cols_to_drop = sorted(set(missing_cols + zero_var_cols + corr_cols))\n",
    "cols_to_drop_existing = [col for col in cols_to_drop if col in base_df.columns]\n",
    "df_filtered = base_df.drop(columns=cols_to_drop_existing)\n",
    "print(f\"Dropped {len(cols_to_drop_existing)} columns. Shape: {df_filtered.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/test split + imputation (4 methods)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Category cols for imputation (includes sleep/appetite/weight + LLM)\n",
    "category_cols_for_imp = [\n",
    "    \"sex\", \"edu\", \"job\", \"marry\", \"drink\", \"smoke\", \"substance_abuse\", \"psy_family\",\n",
    "    \"sleep\", \"appetite\", \"weight\", \"AD_more_three\", \"ER_more_two\",\n",
    "    \"Suicidalidea\", \"Suicidalplan\", \"Suicidalattempt\",\n",
    "    \"benzodiazepine\", \"quetiapine\", \"aripiprazole\", \"lithium\", \"divalproex\", \"olanzapine\",\n",
    "    \"bipolar\", \"depression\", \"schizophrenia\", \"anxiety\",\n",
    "    \"trauma_stressor_related\", \"somatic_symptom_disorder\", \"psychotic_other\",\n",
    "] + LLM_COLS\n",
    "\n",
    "TARGET = LABELS\n",
    "base_dir = str(PROJECT_ROOT / \"data/processed_imp/260106_split_corr_LLM_ADER/imputation\")\n",
    "\n",
    "before_splits, simpleimp_splits, missforest_splits, mice_splits, hybrid_splits = {}, {}, {}, {}, {}\n",
    "\n",
    "for t in TARGET:\n",
    "    print(f\"\\n--- Processing target: {t} ---\")\n",
    "    outcome_df = outcome_dict[t]\n",
    "    valid_patients = outcome_df[\"환자번호\"].unique()\n",
    "    df_full = df_filtered[df_filtered[\"환자번호\"].isin(valid_patients)].copy()\n",
    "\n",
    "    if t not in df_full.columns:\n",
    "        df_full = pd.merge(df_full, outcome_df[[\"환자번호\", t]], on=\"환자번호\", how=\"left\")\n",
    "\n",
    "    other_labels = [col for col in TARGET if col != t and col in df_full.columns]\n",
    "    df = df_full.drop(columns=[\"환자번호\"] + other_labels)\n",
    "\n",
    "    feature_cols = [c for c in df.columns if c != t]\n",
    "    numeric_cols = [c for c in feature_cols if c not in category_cols_for_imp]\n",
    "\n",
    "    # Train/test split\n",
    "    train_raw, test_raw = train_test_split(\n",
    "        df, test_size=0.3, stratify=df[t], random_state=SPLIT_SEED\n",
    "    )\n",
    "    before_splits[f\"{t}_train\"] = train_raw.reset_index(drop=True)\n",
    "    before_splits[f\"{t}_test\"] = test_raw.reset_index(drop=True)\n",
    "\n",
    "    # (A) Simple Imputation\n",
    "    X_tr_s = train_raw[feature_cols].copy().reset_index(drop=True)\n",
    "    X_te_s = test_raw[feature_cols].copy().reset_index(drop=True)\n",
    "    for col in feature_cols:\n",
    "        strat = \"most_frequent\" if col in category_cols_for_imp else \"mean\"\n",
    "        imp = SimpleImputer(strategy=strat)\n",
    "        X_tr_s[[col]] = imp.fit_transform(X_tr_s[[col]])\n",
    "        X_te_s[[col]] = imp.transform(X_te_s[[col]])\n",
    "    simpleimp_splits[f\"{t}_train\"] = pd.concat([X_tr_s, train_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "    simpleimp_splits[f\"{t}_test\"] = pd.concat([X_te_s, test_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # (B) MissForest\n",
    "    mf = MissForest(\n",
    "        categorical=category_cols_for_imp,\n",
    "        clf=RandomForestClassifier(n_estimators=100, random_state=MODEL_SEED, n_jobs=-1),\n",
    "        rgr=RandomForestRegressor(n_estimators=100, random_state=MODEL_SEED, n_jobs=-1),\n",
    "        verbose=False,\n",
    "    )\n",
    "    X_tr_mf = pd.DataFrame(mf.fit_transform(train_raw[feature_cols]), columns=feature_cols).reset_index(drop=True)\n",
    "    X_te_mf = pd.DataFrame(mf.transform(test_raw[feature_cols]), columns=feature_cols).reset_index(drop=True)\n",
    "    missforest_splits[f\"{t}_train\"] = pd.concat([X_tr_mf, train_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "    missforest_splits[f\"{t}_test\"] = pd.concat([X_te_mf, test_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # (C) MICE\n",
    "    X_tr_mi = train_raw[feature_cols].copy().reset_index(drop=True)\n",
    "    X_te_mi = test_raw[feature_cols].copy().reset_index(drop=True)\n",
    "    for col in category_cols_for_imp:\n",
    "        if col in X_tr_mi.columns:\n",
    "            imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "            X_tr_mi[[col]] = imputer.fit_transform(X_tr_mi[[col]])\n",
    "            X_te_mi[[col]] = imputer.transform(X_te_mi[[col]])\n",
    "    mice = IterativeImputer(random_state=MODEL_SEED)\n",
    "    mice_num = [c for c in numeric_cols if c in X_tr_mi.columns]\n",
    "    X_tr_mi[mice_num] = mice.fit_transform(X_tr_mi[mice_num])\n",
    "    X_te_mi[mice_num] = mice.transform(X_te_mi[mice_num])\n",
    "    mice_splits[f\"{t}_train\"] = pd.concat([X_tr_mi, train_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "    mice_splits[f\"{t}_test\"] = pd.concat([X_te_mi, test_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # (D) Hybrid (Simple for <=30% missing, MissForest for >30%)\n",
    "    missing_rate = train_raw[feature_cols].isna().mean()\n",
    "    simple_cols = missing_rate[missing_rate <= 0.3].index.tolist()\n",
    "    forest_cols = missing_rate[missing_rate > 0.3].index.tolist()\n",
    "    X_tr_h = train_raw[feature_cols].copy().reset_index(drop=True)\n",
    "    X_te_h = test_raw[feature_cols].copy().reset_index(drop=True)\n",
    "\n",
    "    for col in simple_cols:\n",
    "        strat = \"most_frequent\" if col in category_cols_for_imp else \"mean\"\n",
    "        imp = SimpleImputer(strategy=strat)\n",
    "        X_tr_h[[col]] = imp.fit_transform(X_tr_h[[col]])\n",
    "        X_te_h[[col]] = imp.transform(X_te_h[[col]])\n",
    "\n",
    "    if forest_cols:\n",
    "        mf2 = MissForest(\n",
    "            categorical=[c for c in category_cols_for_imp if c in forest_cols],\n",
    "            clf=RandomForestClassifier(n_estimators=100, random_state=MODEL_SEED, n_jobs=-1),\n",
    "            rgr=RandomForestRegressor(n_estimators=100, random_state=MODEL_SEED, n_jobs=-1),\n",
    "            verbose=False,\n",
    "        )\n",
    "        mf2.fit(X_tr_h[forest_cols])\n",
    "        X_tr_h[forest_cols] = pd.DataFrame(mf2.transform(X_tr_h[forest_cols]), columns=forest_cols)\n",
    "        X_te_h[forest_cols] = pd.DataFrame(mf2.transform(X_te_h[forest_cols]), columns=forest_cols)\n",
    "\n",
    "    hybrid_splits[f\"{t}_train\"] = pd.concat([X_tr_h, train_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "    hybrid_splits[f\"{t}_test\"] = pd.concat([X_te_h, test_raw[[t]].reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"\\nAll imputation methods completed.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save imputed datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for sub in [\"before_imput\", \"simple_imput\", \"missforest_imput\", \"mice_imput\", \"hybrid_imput\"]:\n",
    "    os.makedirs(os.path.join(base_dir, sub), exist_ok=True)\n",
    "\n",
    "def save_dict(splits, folder, prefix):\n",
    "    for key, df_ in splits.items():\n",
    "        target, part = key.rsplit(\"_\", 1)\n",
    "        filename = f\"{prefix}_{target}_{part}.csv\"\n",
    "        path = os.path.join(base_dir, folder, filename)\n",
    "        df_.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "save_dict(before_splits, \"before_imput\", \"before\")\n",
    "save_dict(simpleimp_splits, \"simple_imput\", \"simple\")\n",
    "save_dict(missforest_splits, \"missforest_imput\", \"missforest\")\n",
    "save_dict(mice_splits, \"mice_imput\", \"mice\")\n",
    "save_dict(hybrid_splits, \"hybrid_imput\", \"hybrid\")\n",
    "print(\"All imputed datasets saved.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}