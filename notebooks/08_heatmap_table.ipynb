{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc266e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.config import PROJECT_ROOT\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Requirements: pandas, numpy, scipy, seaborn, matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, re\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae68b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1. ì„¤ì •: íŒŒì¼ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ\n",
    "# -----------------------------------------------------------------\n",
    "# â€¼ï¸â€¼ï¸ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ì´ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. â€¼ï¸â€¼ï¸\n",
    "BASE_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# (ê³µìš©) ë„ë©”ì¸ ë¶„ë¥˜ í•¨ìˆ˜ (ë¨¼ì € ì •ì˜)\n",
    "# -----------------------------------------------------------------\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = str(feat).lower().strip()\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 1. SDoMH (Social Determinants of Mental Health) - LLM ê´€ë ¨ ë³€ìˆ˜\n",
    "    # -----------------------------------------------------------------\n",
    "    # LLM ë³€ìˆ˜ë“¤ì€ ëª¨ë‘ SDoMH ë„ë©”ì¸ìœ¼ë¡œ ë¶„ë¥˜í•˜ë˜,\n",
    "    # ì‹ë³„ì„ ìœ„í•´ (LLM) ì ‘ë¯¸ì‚¬ í˜¹ì€ ì›ë³¸ í‚¤ì›Œë“œë¥¼ ì²´í¬í•©ë‹ˆë‹¤.\n",
    "    SDOMH_KEYWORDS = [\n",
    "        \"social function\",\n",
    "        \"social isolation\",\n",
    "        \"religious\",\n",
    "        \"violence\",\n",
    "        \"impulsivity\",\n",
    "        \"abuse\",\n",
    "        \"neglect\",\n",
    "        \"divorce\",\n",
    "        \"family member\",\n",
    "        \"family support\",\n",
    "        \"psychotic\",\n",
    "        \"halluc\",\n",
    "        \"delusion\",\n",
    "        \"paranoia\",\n",
    "        \"interpersonal\",\n",
    "        \"suicide exposure\",\n",
    "        \"alcohol use\",\n",
    "        \"victimization\",\n",
    "    ]\n",
    "\n",
    "    # ë³€ìˆ˜ëª…ì— (LLM)ì´ ëª…ì‹œë˜ì–´ ìˆê±°ë‚˜ SDoMH ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°\n",
    "    if \"(llm)\" in t or any(k in t for k in SDOMH_KEYWORDS):\n",
    "        # íŠ¹ì • ê·¸ë£¹ìœ¼ë¡œ ë” ì„¸ë¶„í™”í•˜ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì„œ return ê°’ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        # ì˜ˆ: return \"SDoMH (Social)\" ë“±\n",
    "        return \"SDoMH\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2. Utilization (ì˜ë£Œ ì´ìš©)\n",
    "    # -----------------------------------------------------------------\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"er visit\",\n",
    "            \"â‰¥2 er\",\n",
    "            \">=2 er\",\n",
    "            \"â‰¥3 er\",\n",
    "            \">=3 er\",\n",
    "            \"admission\",\n",
    "            \"admit\",\n",
    "            \"er_more_two\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Utilization\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 3. Lab (ê²€ì‚¬ ìˆ˜ì¹˜)\n",
    "    # -----------------------------------------------------------------\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl\",\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"lymphocyte\",\n",
    "            \"monocyte\",\n",
    "            \"acl\",\n",
    "            \"platelet\",\n",
    "            \"creatinine\",\n",
    "            \"ast\",\n",
    "            \"alt\",\n",
    "            \"alp\",\n",
    "            \"bun\",\n",
    "            \"calcium\",\n",
    "            \"phospho\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 4. Psychometric (ì‹¬ë¦¬ ê²€ì‚¬ ì§€í‘œ)\n",
    "    # -----------------------------------------------------------------\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"asrs\",\n",
    "            \"bai\",\n",
    "            \"bhs\",\n",
    "            \"ham\",\n",
    "            \"hcl\",\n",
    "            \"perceptual\",\n",
    "            \"appq\",\n",
    "            \"bprs\",\n",
    "            \"gaf\",\n",
    "            \"suicidalidea\",\n",
    "            \"suicidalplan\",\n",
    "            \"suicidalattempt\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 5. Treatment (ì•½ë¬¼ ë° ì¹˜ë£Œ)\n",
    "    # -----------------------------------------------------------------\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "            \"risper\",\n",
    "            \"valpro\",\n",
    "            \"ssri\",\n",
    "            \"snri\",\n",
    "            \"quetiapine\",\n",
    "            \"aripip\",\n",
    "            \"olanzapine\",\n",
    "            \"divalproex\",\n",
    "            \"aripiprazole\",\n",
    "            \"lithium\",\n",
    "            \"quetiapine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 6. Sleep/Appetite (ìƒì²´ ë¦¬ë“¬)\n",
    "    # -----------------------------------------------------------------\n",
    "    if \"sleep\" in t or \"appetite\" in t or \"weight\" in t:\n",
    "        return \"Biological\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 7. Demographic/Psychosocial (ì¸êµ¬í•™ì  ì •ë³´ ë° ê¸°ì¡´ ì§„ë‹¨ëª…)\n",
    "    # -----------------------------------------------------------------\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"marry\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"edu\",\n",
    "            \"job\",\n",
    "            \"occupation\",\n",
    "            \"income\",\n",
    "            \"smoking\",\n",
    "            \"smoke\",\n",
    "            \"drinking\",\n",
    "            \"drink\",\n",
    "            \"psychiatric family\",\n",
    "            \"psy_family\",\n",
    "            \"hospitalization\",\n",
    "            \"length of stay\",\n",
    "            \"stay_day\",\n",
    "            \"bipolar\",\n",
    "            \"depression\",\n",
    "            \"schizophrenia\",\n",
    "            \"anxiety\",\n",
    "            \"somatic symptom\",\n",
    "            \"trauma_stressor\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. ë°ì´í„° ë¡œë”©\n",
    "# -----------------------------------------------------------------\n",
    "print(\"íŒŒì¼ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# (A) Load LONG files\n",
    "long_files = [\n",
    "    (\"30d\", \"label_30d__SHAP_long_Test.csv\"),\n",
    "    (\"60d\", \"label_60d__SHAP_long_Test.csv\"),\n",
    "    (\"90d\", \"label_90d__SHAP_long_Test.csv\"),\n",
    "    (\"180d\", \"label_180d__SHAP_long_Test.csv\"),\n",
    "    (\"365d\", \"label_365d__SHAP_long_Test.csv\"),\n",
    "]\n",
    "long_dfs = []\n",
    "for tp, fname in long_files:\n",
    "    file_path = BASE_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"time_point\"] = tp\n",
    "        long_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "if not long_dfs:\n",
    "    raise ValueError(\"Long íŒŒì¼ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "full_long = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "# (B) Load IMPORTANCE files\n",
    "imp_files = [\n",
    "    (\"30d\", \"label_30d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"60d\", \"label_60d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"90d\", \"label_90d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"180d\", \"label_180d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"365d\", \"label_365d__SHAP_feature_importance_Test.csv\"),\n",
    "]\n",
    "imp_dfs = []\n",
    "for tp, fname in imp_files:\n",
    "    file_path = BASE_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"time_point\"] = tp\n",
    "        imp_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: Importance íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "if not imp_dfs:\n",
    "    raise ValueError(\"Importance íŒŒì¼ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "full_imp = pd.concat(imp_dfs, ignore_index=True)\n",
    "\n",
    "print(\"íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ìƒê´€ê´€ê³„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. ìƒê´€ê´€ê³„ ê³„ì‚° ë° ë°ì´í„° ê°€ê³µ\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "def calc_correlation(g):\n",
    "    if g[\"feature_value\"].nunique() <= 1 or g[\"shap_value\"].nunique() <= 1:\n",
    "        corr = 0\n",
    "    else:\n",
    "        corr, _ = spearmanr(g[\"feature_value\"], g[\"shap_value\"])\n",
    "        if np.isnan(corr):\n",
    "            corr = 0\n",
    "    return pd.Series({\"corr_direction\": corr})\n",
    "\n",
    "\n",
    "metrics_df = (\n",
    "    full_long.groupby([\"time_point\", \"feature\"]).apply(calc_correlation).reset_index()\n",
    ")\n",
    "\n",
    "time_order = [\"30d\", \"60d\", \"90d\", \"180d\", \"365d\"]\n",
    "TOP_N = 20\n",
    "\n",
    "# (A) ì¤‘ìš”ë„ í”¼ë²— (ëª¨ë“  ë³€ìˆ˜) - ANNOTATION (TEXT) ìš©\n",
    "imp_pivot = full_imp.pivot(\n",
    "    index=\"feature\", columns=\"time_point\", values=\"mean_abs_shap\"\n",
    ").reindex(columns=time_order)\n",
    "\n",
    "# (B) Top 20 í•„í„°ë§ (ì–´ëŠ ì‹œì ì—ì„œë“  Top 20 ì•ˆì— ë“  ë³€ìˆ˜ ì¶”ì¶œ)\n",
    "top_features_union = set()\n",
    "for tp in time_order:\n",
    "    if tp in imp_pivot.columns:\n",
    "        top_in_tp = imp_pivot[tp].fillna(-1).nlargest(TOP_N).index.tolist()\n",
    "        top_features_union.update(top_in_tp)\n",
    "\n",
    "# (C) ë°©í–¥ì„± í”¼ë²— - DATA (COLOR) ìš©\n",
    "dir_pivot = metrics_df.pivot(\n",
    "    index=\"feature\", columns=\"time_point\", values=\"corr_direction\"\n",
    ").reindex(columns=time_order)\n",
    "\n",
    "# (D) í•„í„° ì ìš© ë° ì •ë ¬\n",
    "# 1. Color Data (Correlation)\n",
    "dir_top = dir_pivot.loc[list(top_features_union)].copy()\n",
    "# 2. Annotation Data (Importance)\n",
    "imp_top_filtered = imp_pivot.loc[list(top_features_union)].copy()\n",
    "\n",
    "# 3. ì •ë ¬ (í‰ê·  ì¤‘ìš”ë„ ê¸°ì¤€)\n",
    "mean_imp_overall = imp_top_filtered.mean(axis=1)\n",
    "sorted_idx = mean_imp_overall.sort_values(ascending=False).index\n",
    "\n",
    "dir_top = dir_top.loc[sorted_idx]\n",
    "imp_top_filtered = imp_top_filtered.loc[sorted_idx]  # Annotationë„ ë™ì¼í•˜ê²Œ ì •ë ¬\n",
    "\n",
    "print(f\"ì´ {len(dir_top)}ê°œì˜ ë³€ìˆ˜ë¡œ ì‹œê°í™” ë° í…Œì´ë¸” ìƒì„±ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. (ìˆ˜ì •) í”Œë¡¯ 2ê°œë¡œ ë¶„í•  (Color=Corr, Text=Importance)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# (A) ë°ì´í„° ë¶„í• \n",
    "midpoint = int(np.ceil(len(dir_top) / 2))\n",
    "# 1. Color Data\n",
    "dir_top_part1 = dir_top.iloc[:midpoint]\n",
    "dir_top_part2 = dir_top.iloc[midpoint:]\n",
    "# 2. Annotation Data\n",
    "annot_data_part1 = imp_top_filtered.iloc[:midpoint]\n",
    "annot_data_part2 = imp_top_filtered.iloc[midpoint:]\n",
    "\n",
    "print(\n",
    "    f\"í”Œë¡¯ì„ Part1 ({len(dir_top_part1)}ê°œ), Part2 ({len(dir_top_part2)}ê°œ)ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\"\n",
    ")\n",
    "\n",
    "# (B) ê³µí†µ í”Œë¡¯ ì„¤ì •\n",
    "font_sizes = {\"title\": 18, \"label\": 14, \"tick\": 12, \"annot\": 12, \"colorbar_label\": 14}\n",
    "cbar_kws_dict = {\n",
    "    \"label\": \"Spearman Correlation (Feature Value vs. SHAP Value)\",\n",
    "    \"shrink\": 0.7,\n",
    "    \"ticklocation\": \"right\",\n",
    "}\n",
    "\n",
    "# (C) í”Œë¡¯ 1 ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(12, len(dir_top_part1) * 0.5 + 2.5))\n",
    "sns.heatmap(\n",
    "    data=dir_top_part1,  # â¬…ï¸ ìƒ‰ìƒ: ìƒê´€ê´€ê³„ (dir_top)\n",
    "    annot=annot_data_part1,  # â¬…ï¸ ìˆ«ì: ì¤‘ìš”ë„ (imp_top)\n",
    "    fmt=\".3f\",  # â¬…ï¸ í¬ë§·: ì¤‘ìš”ë„ ê¸°ì¤€ (ì†Œìˆ˜ì  3ìë¦¬)\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    linewidths=1.2,\n",
    "    linecolor=\"gray\",\n",
    "    cbar_kws=cbar_kws_dict,\n",
    "    annot_kws={\"size\": font_sizes[\"annot\"], \"weight\": \"bold\", \"color\": \"black\"},\n",
    ")\n",
    "plt.title(\n",
    "    \"SHAP Directionality \\n(Color=Direction, Text=Mean|SHAP|)\",\n",
    "    fontsize=font_sizes[\"title\"],\n",
    "    fontweight=\"bold\",\n",
    "    pad=25,\n",
    ")\n",
    "plt.xlabel(\"Time Point\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.ylabel(\"Feature\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.xticks(fontsize=font_sizes[\"tick\"], rotation=0)\n",
    "plt.yticks(fontsize=font_sizes[\"tick\"])\n",
    "cbar = plt.gca().collections[0].colorbar\n",
    "cbar.set_label(\n",
    "    cbar_kws_dict[\"label\"], fontsize=font_sizes[\"colorbar_label\"], fontweight=\"bold\"\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=font_sizes[\"tick\"])\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "plt.savefig(BASE_DIR / \"Analysis1_SHAP_Heatmap_Hybrid_Part1.png\", dpi=400)\n",
    "plt.close()\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {BASE_DIR / 'Analysis1_SHAP_Heatmap_Hybrid_Part1.png'}\")\n",
    "\n",
    "# (D) í”Œë¡¯ 2 ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(12, len(dir_top_part2) * 0.5 + 2.5))\n",
    "sns.heatmap(\n",
    "    data=dir_top_part2,  # â¬…ï¸ ìƒ‰ìƒ: ìƒê´€ê´€ê³„ (dir_top)\n",
    "    annot=annot_data_part2,  # â¬…ï¸ ìˆ«ì: ì¤‘ìš”ë„ (imp_top)\n",
    "    fmt=\".3f\",  # â¬…ï¸ í¬ë§·: ì¤‘ìš”ë„ ê¸°ì¤€ (ì†Œìˆ˜ì  3ìë¦¬)\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    linewidths=1.2,\n",
    "    linecolor=\"gray\",\n",
    "    cbar_kws=cbar_kws_dict,\n",
    "    annot_kws={\"size\": font_sizes[\"annot\"], \"weight\": \"bold\", \"color\": \"black\"},\n",
    ")\n",
    "plt.title(\n",
    "    \"SHAP Directionality \\n(Color=Direction, Text=Mean|SHAP|)\",\n",
    "    fontsize=font_sizes[\"title\"],\n",
    "    fontweight=\"bold\",\n",
    "    pad=25,\n",
    ")\n",
    "plt.xlabel(\"Time Point\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.ylabel(\"Feature\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.xticks(fontsize=font_sizes[\"tick\"], rotation=0)\n",
    "plt.yticks(fontsize=font_sizes[\"tick\"])\n",
    "cbar = plt.gca().collections[0].colorbar\n",
    "cbar.set_label(\n",
    "    cbar_kws_dict[\"label\"], fontsize=font_sizes[\"colorbar_label\"], fontweight=\"bold\"\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=font_sizes[\"tick\"])\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "plt.savefig(BASE_DIR / \"Analysis1_SHAP_Heatmap_Hybrid_Part2.png\", dpi=400)\n",
    "plt.close()\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {BASE_DIR / 'Analysis1_SHAP_Heatmap_Hybrid_Part2.png'}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. CSV íŒŒì¼ ì €ì¥ (Strongest, Consistency, Domain, Symbol + Importance í†µí•©)\n",
    "# -----------------------------------------------------------------\n",
    "print(\"ìµœì¢… ìƒì„¸ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "\n",
    "# (A) ë‹¨ìˆœí™”ëœ ê¸°í˜¸ ì •ì˜ (ê¸°ì¤€ 0.3ìœ¼ë¡œ ìƒí–¥)\n",
    "def get_corr_symbol_simple(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    if val >= 0.3:\n",
    "        return \"ğŸ”º\"\n",
    "    if val <= -0.3:\n",
    "        return \"â–¼\"\n",
    "    return \"â€¢ (Neutral)\"\n",
    "\n",
    "\n",
    "dir_symbol_top = dir_top.applymap(get_corr_symbol_simple)\n",
    "# imp_top_filteredëŠ” 3ë²ˆ ì„¹ì…˜ì—ì„œ ì´ë¯¸ dir_topê³¼ ë™ì¼í•˜ê²Œ ì •ë ¬ë˜ì—ˆìŒ\n",
    "\n",
    "# (B) í†µí•© í…Œì´ë¸” ìƒì„±\n",
    "combined_table = pd.DataFrame(index=dir_top.index)\n",
    "for tp in time_order:\n",
    "    if tp in dir_symbol_top.columns:\n",
    "        combined_table[f\"{tp}_direction\"] = dir_symbol_top[tp]\n",
    "        # [ìˆ˜ì •] ì†Œìˆ˜ì  4ìë¦¬ ë°˜ì˜¬ë¦¼\n",
    "        combined_table[f\"{tp}_SHAP(mean)\"] = imp_top_filtered[tp].round(4)\n",
    "\n",
    "# (C) Strongest window, Domain ì¶”ê°€\n",
    "strongest_window = imp_top_filtered.idxmax(axis=1)\n",
    "domains = combined_table.index.map(infer_domain)\n",
    "\n",
    "# (D) Top 20 Count (n/5) ê³„ì‚° (ì»¬ëŸ¼ëª… ë³€ê²½)\n",
    "rank_pivot = imp_pivot.rank(axis=0, method=\"min\", ascending=False)\n",
    "rank_pivot_top = rank_pivot.loc[dir_top.index]\n",
    "is_top_n = rank_pivot_top <= TOP_N\n",
    "consistency_count = is_top_n.sum(axis=1)\n",
    "consistency_str = consistency_count.astype(str) + f\"/{len(time_order)}\"\n",
    "\n",
    "# [ìˆ˜ì •] ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½\n",
    "combined_table.insert(0, \"Domain\", domains)\n",
    "combined_table[\"Strongest Window\"] = strongest_window\n",
    "combined_table[\"Top 20 Count (n/5)\"] = consistency_str\n",
    "\n",
    "# (F) íŒŒì¼ë¡œ ì €ì¥\n",
    "combined_table.to_excel(BASE_DIR / \"Analysis1_SHAP_Domain_Symbol_Importance_FINAL.xlsx\")\n",
    "print(\n",
    "    f\"â˜…â˜…â˜… ìµœì¢… ìƒì„¸ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {BASE_DIR / 'Analysis1_SHAP_Domain_Symbol_Importance_FINAL.xlsx'}\"\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6. Top 20 ë„ë©”ì¸ ë¹„ìœ¨ ìš”ì•½ í…Œì´ë¸”\n",
    "# -----------------------------------------------------------------\n",
    "print(\"\\nTop 20 ë„ë©”ì¸ ìš”ì•½ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# (A) ê° ì‹œì ë³„ Top 20 ë³€ìˆ˜ ì¶”ì¶œ ë° ë„ë©”ì¸ ê³„ì‚°\n",
    "top_n_features_by_time = (\n",
    "    full_imp.groupby(\"time_point\")\n",
    "    .apply(lambda x: x.nlargest(TOP_N, \"mean_abs_shap\"))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "top_n_features_by_time[\"Domain\"] = top_n_features_by_time[\"feature\"].apply(infer_domain)\n",
    "\n",
    "# (B) ì‹œì ë³„ ë„ë©”ì¸ ì¹´ìš´íŠ¸ ì§‘ê³„\n",
    "domain_counts_pivot = (\n",
    "    top_n_features_by_time.groupby(\"time_point\")[\"Domain\"]\n",
    "    .value_counts()\n",
    "    .unstack(level=\"time_point\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "domain_counts_pivot = domain_counts_pivot.reindex(columns=time_order, fill_value=0)\n",
    "\n",
    "# (C) ë¹„ìœ¨(%) ê³„ì‚°\n",
    "domain_proportions_pivot = domain_counts_pivot / TOP_N * 100\n",
    "\n",
    "# (D) ìµœì¢… í¬ë§·íŒ…: \"Proportion % (Count)\"\n",
    "formatted_table = pd.DataFrame(\n",
    "    index=domain_counts_pivot.index, columns=domain_counts_pivot.columns\n",
    ")\n",
    "for col in formatted_table.columns:\n",
    "    counts = domain_counts_pivot[col]\n",
    "    props = domain_proportions_pivot[col]\n",
    "    formatted_table[col] = [\n",
    "        f\"{p:.1f}% ({c})\" if c > 0 else \"-\" for c, p in zip(counts, props)\n",
    "    ]\n",
    "\n",
    "# (E) ì´í•©(Total) í–‰ ì¶”ê°€\n",
    "total_counts = domain_counts_pivot.sum()\n",
    "total_row = [f\"100.0% ({c})\" for c in total_counts]\n",
    "formatted_table.loc[\"Total\"] = total_row\n",
    "\n",
    "# (F) íŒŒì¼ë¡œ ì €ì¥\n",
    "domain_table_path = BASE_DIR / \"Analysis1_SHAP_Domain_Proportions_Summary.xlsx\"\n",
    "formatted_table.to_excel(domain_table_path)\n",
    "\n",
    "print(f\"â˜…â˜…â˜… ì‹ ê·œ ë„ë©”ì¸ ë¹„ìœ¨ ìš”ì•½ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {domain_table_path}\")\n",
    "print(\"\\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f36ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1. ì„¤ì •: íŒŒì¼ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ\n",
    "# -----------------------------------------------------------------\n",
    "# â€¼ï¸â€¼ï¸ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ì´ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. â€¼ï¸â€¼ï¸\n",
    "BASE_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# (ê³µìš©) ë„ë©”ì¸ ë¶„ë¥˜ í•¨ìˆ˜ (ë¨¼ì € ì •ì˜)\n",
    "# -----------------------------------------------------------------\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = str(feat).lower().strip()\n",
    "    # ... (ì œê³µí•´ì£¼ì‹  ë„ë©”ì¸ ë¶„ë¥˜ í•¨ìˆ˜ ë‚´ìš© ì „ì²´) ...\n",
    "    LLM_ALIASES = {\n",
    "        \"Aggression (LLM)\": [\"aggression\", \"aggressive\"],\n",
    "        \"Interpersonal Conflict (LLM)\": [\n",
    "            \"interpersonal conflict\",\n",
    "            \"relationship conflict\",\n",
    "            \"family conflict\",\n",
    "            \"interpersonal\",\n",
    "        ],\n",
    "        \"Impaired Social Functioning (LLM)\": [\n",
    "            \"impaired social functioning\",\n",
    "            \"social function impairment\",\n",
    "            \"social functioning impairment\",\n",
    "        ],\n",
    "        \"Alcohol Use Issues (LLM)\": [\n",
    "            \"alcohol use issues\",\n",
    "            \"alcohol use\",\n",
    "            \"alcohol\",\n",
    "            \"drinking\",\n",
    "        ],\n",
    "        \"Hallucinations / Delusions / Paranoia (LLM)\": [\n",
    "            \"hallucinations_delusions_paranoia\",\n",
    "            \"hallucinations\",\n",
    "            \"delusions\",\n",
    "            \"paranoia\",\n",
    "            \"halluc/delusion/paranoia\",\n",
    "        ],\n",
    "        \"Lack of Support (LLM)\": [\n",
    "            \"lack family social support\",\n",
    "            \"lack of support\",\n",
    "            \"poor support\",\n",
    "            \"no support\",\n",
    "            \"social support deficit\",\n",
    "        ],\n",
    "        \"Family Loss (LLM)\": [\n",
    "            \"loss_of_family\",\n",
    "            \"family loss\",\n",
    "            \"loss / grief\",\n",
    "            \"bereavement\",\n",
    "        ],\n",
    "        \"Abuse / Sexual Victimization (LLM)\": [\n",
    "            \"abuse_and_sexual_victimization\",\n",
    "            \"abuse\",\n",
    "            \"victimization\",\n",
    "            \"sexual\",\n",
    "            \"violence\",\n",
    "            \"victim\",\n",
    "        ],\n",
    "    }\n",
    "    for dom, keys in LLM_ALIASES.items():\n",
    "        if any(k in t for k in keys) and \"llm\" in t:\n",
    "            return dom\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"er visit\",\n",
    "            \"â‰¥2 er\",\n",
    "            \">=2 er\",\n",
    "            \"â‰¥3 er\",\n",
    "            \">=3 er\",\n",
    "            \"admission\",\n",
    "            \"admit\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl\",\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"lymphocyte\",\n",
    "            \"monocyte\",\n",
    "            \"acl\",\n",
    "            \"platelet\",\n",
    "            \"creatinine\",\n",
    "            \"ast\",\n",
    "            \"alt\",\n",
    "            \"alp\",\n",
    "            \"bun\",\n",
    "            \"calcium\",\n",
    "            \"phospho\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"asrs\",\n",
    "            \"bai\",\n",
    "            \"bhs\",\n",
    "            \"ham\",\n",
    "            \"hcl\",\n",
    "            \"perceptual\",\n",
    "            \"appq\",\n",
    "            \"bprs\",\n",
    "            \"gaf\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "            \"risper\",\n",
    "            \"valpro\",\n",
    "            \"ssri\",\n",
    "            \"snri\",\n",
    "            \"quetiapine\",\n",
    "            \"aripip\",\n",
    "            \"olanzapine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"occupation\",\n",
    "            \"income\",\n",
    "            \"smoking\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "            \"length of stay\",\n",
    "            \"other psychotic\",\n",
    "            \"somatic symptom disorder\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. ë°ì´í„° ë¡œë”©\n",
    "# -----------------------------------------------------------------\n",
    "print(\"íŒŒì¼ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# (A) Load LONG files\n",
    "long_files = [\n",
    "    (\"30d\", \"label_30d__SHAP_long_Test.csv\"),\n",
    "    (\"60d\", \"label_60d__SHAP_long_Test.csv\"),\n",
    "    (\"90d\", \"label_90d__SHAP_long_Test.csv\"),\n",
    "    (\"180d\", \"label_180d__SHAP_long_Test.csv\"),\n",
    "    (\"365d\", \"label_365d__SHAP_long_Test.csv\"),\n",
    "]\n",
    "long_dfs = []\n",
    "for tp, fname in long_files:\n",
    "    file_path = BASE_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"time_point\"] = tp\n",
    "        long_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "if not long_dfs:\n",
    "    raise ValueError(\"Long íŒŒì¼ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "full_long = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "# (B) Load IMPORTANCE files\n",
    "imp_files = [\n",
    "    (\"30d\", \"label_30d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"60d\", \"label_60d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"90d\", \"label_90d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"180d\", \"label_180d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"365d\", \"label_365d__SHAP_feature_importance_Test.csv\"),\n",
    "]\n",
    "imp_dfs = []\n",
    "for tp, fname in imp_files:\n",
    "    file_path = BASE_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"time_point\"] = tp\n",
    "        imp_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: Importance íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "if not imp_dfs:\n",
    "    raise ValueError(\"Importance íŒŒì¼ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "full_imp = pd.concat(imp_dfs, ignore_index=True)\n",
    "\n",
    "print(\"íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ìƒê´€ê´€ê³„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. ìƒê´€ê´€ê³„ ê³„ì‚° ë° ë°ì´í„° ê°€ê³µ\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. ìƒê´€ê´€ê³„ ê³„ì‚° ë° ë°ì´í„° ê°€ê³µ (ì‚¬ìš©ì ì§€ì • ë³€ìˆ˜ í•„í„°ë§ ì ìš©)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# (A) ìƒê´€ê´€ê³„ ê³„ì‚° í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)\n",
    "def calc_correlation(g):\n",
    "    if g[\"feature_value\"].nunique() <= 1 or g[\"shap_value\"].nunique() <= 1:\n",
    "        corr = 0\n",
    "    else:\n",
    "        corr, _ = spearmanr(g[\"feature_value\"], g[\"shap_value\"])\n",
    "        if np.isnan(corr):\n",
    "            corr = 0\n",
    "    return pd.Series({\"corr_direction\": corr})\n",
    "\n",
    "\n",
    "print(\"ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "metrics_df = (\n",
    "    full_long.groupby([\"time_point\", \"feature\"]).apply(calc_correlation).reset_index()\n",
    ")\n",
    "\n",
    "time_order = [\"30d\", \"60d\", \"90d\", \"180d\", \"365d\"]\n",
    "\n",
    "# (B) ì „ì²´ í”¼ë²— í…Œì´ë¸” ìƒì„±\n",
    "# 1. Annotation Data (Importance)\n",
    "imp_pivot = full_imp.pivot(\n",
    "    index=\"feature\", columns=\"time_point\", values=\"mean_abs_shap\"\n",
    ").reindex(columns=time_order)\n",
    "\n",
    "# 2. Color Data (Correlation)\n",
    "dir_pivot = metrics_df.pivot(\n",
    "    index=\"feature\", columns=\"time_point\", values=\"corr_direction\"\n",
    ").reindex(columns=time_order)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# [ìˆ˜ì •] Top N ìë™ ì¶”ì¶œ ëŒ€ì‹  ì‚¬ìš©ì ì§€ì • ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# â€¼ï¸â€¼ï¸ ì—¬ê¸°ì— ì‹œê°í™”í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ ì´ë¦„ì„ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš” â€¼ï¸â€¼ï¸\n",
    "TARGET_FEATURES = [\n",
    "    \"â‰¥2 ER Visits\",\n",
    "    \"â‰¥3 Admissions\",\n",
    "    \"ANC (BL201815)\",\n",
    "    \"CRP (C-Reactive Protein) (BL3140)\",\n",
    "    \"Glucose, Fasting (BL3118)\",\n",
    "    \"Abuse/Victimization (LLM)\",\n",
    "    \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "    \"Lack of Support (LLM)\",\n",
    "    \"Interpersonal Conflict (LLM)\",\n",
    "    # ... í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ê³„ì† ì¶”ê°€í•˜ì„¸ìš”\n",
    "]\n",
    "\n",
    "# (C) ë°ì´í„° í•„í„°ë§ (ì§€ì •í•œ ë³€ìˆ˜ë§Œ ë‚¨ê¸°ê¸°)\n",
    "# ë°ì´í„°ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ì¸ì§€ í™•ì¸ (ì˜¤íƒ€ ë°©ì§€)\n",
    "available_features = imp_pivot.index.tolist()\n",
    "selected_features = [f for f in TARGET_FEATURES if f in available_features]\n",
    "missing_features = set(TARGET_FEATURES) - set(selected_features)\n",
    "\n",
    "if missing_features:\n",
    "    print(\n",
    "        f\"\\n[Warning] ë‹¤ìŒ ë³€ìˆ˜ë“¤ì€ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ì§€ ì•Šì•„ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤:\\n{missing_features}\\n\"\n",
    "    )\n",
    "\n",
    "if not selected_features:\n",
    "    raise ValueError(\"ì„ íƒëœ ë³€ìˆ˜ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ë³€ìˆ˜ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ì„ íƒëœ ë³€ìˆ˜ë¡œ ë°ì´í„°í”„ë ˆì„ í•„í„°ë§\n",
    "dir_top = dir_pivot.loc[selected_features].copy()\n",
    "imp_top_filtered = imp_pivot.loc[selected_features].copy()\n",
    "\n",
    "# (D) ì •ë ¬ (ì˜µì…˜)\n",
    "# 1. ì‚¬ìš©ìê°€ ì ì€ ë¦¬ìŠ¤íŠ¸ ìˆœì„œ ê·¸ëŒ€ë¡œ ë³´ê³  ì‹¶ë‹¤ë©´: ì•„ë˜ 3ì¤„ ì£¼ì„ ì²˜ë¦¬\n",
    "# 2. ì¤‘ìš”ë„(Mean SHAP) ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì‹¶ë‹¤ë©´: ì•„ë˜ ì½”ë“œ ìœ ì§€\n",
    "mean_imp_overall = imp_top_filtered.mean(axis=1)\n",
    "sorted_idx = mean_imp_overall.sort_values(ascending=False).index\n",
    "\n",
    "dir_top = dir_top.loc[sorted_idx]\n",
    "imp_top_filtered = imp_top_filtered.loc[sorted_idx]\n",
    "\n",
    "print(f\"ì´ {len(dir_top)}ê°œì˜ ì§€ì •ëœ ë³€ìˆ˜ë¡œ ì‹œê°í™” ë° í…Œì´ë¸” ìƒì„±ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. (ìˆ˜ì •) í†µí•© í”Œë¡¯ ê·¸ë¦¬ê¸° (í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ì €ì¥)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(f\"ì´ {len(dir_top)}ê°œì˜ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì˜ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# (A) í”Œë¡¯ ì„¤ì •\n",
    "# ë³€ìˆ˜ ê°œìˆ˜ì— ë”°ë¼ ë†’ì´ ìë™ ì¡°ì ˆ (ë³€ìˆ˜ë‹¹ 0.5ì¸ì¹˜ + ê¸°ë³¸ ì—¬ë°± 3ì¸ì¹˜)\n",
    "# ë³€ìˆ˜ê°€ ë§ì•„ì ¸ë„ ì…€ì´ ì°Œê·¸ëŸ¬ì§€ì§€ ì•Šë„ë¡ ë†’ì´ë¥¼ ë™ì ìœ¼ë¡œ í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "fig_height = len(dir_top) * 0.5 + 3\n",
    "plt.figure(figsize=(12, fig_height))\n",
    "\n",
    "font_sizes = {\"title\": 18, \"label\": 14, \"tick\": 12, \"annot\": 12, \"colorbar_label\": 14}\n",
    "\n",
    "# ì»¬ëŸ¬ë°” ì„¤ì •\n",
    "cbar_kws_dict = {\n",
    "    \"label\": \"Spearman Correlation (Feature Value vs. SHAP Value)\",\n",
    "    \"shrink\": 0.6,  # ì „ì²´ í”Œë¡¯ ë†’ì´ì— ë§ì¶° ì»¬ëŸ¬ë°” ê¸¸ì´ ë¹„ìœ¨ ì¡°ì •\n",
    "    \"ticklocation\": \"right\",\n",
    "}\n",
    "\n",
    "# (B) íˆíŠ¸ë§µ ê·¸ë¦¬ê¸° (ë¶„í•  ì—†ì´ dir_top ì „ì²´ ì‚¬ìš©)\n",
    "sns.heatmap(\n",
    "    data=dir_top,  # â¬…ï¸ ìƒ‰ìƒ: ìƒê´€ê´€ê³„ (ì „ì²´)\n",
    "    annot=imp_top_filtered,  # â¬…ï¸ ìˆ«ì: ì¤‘ìš”ë„ (ì „ì²´)\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    linewidths=1.2,\n",
    "    linecolor=\"gray\",\n",
    "    cbar_kws=cbar_kws_dict,\n",
    "    annot_kws={\"size\": font_sizes[\"annot\"], \"weight\": \"bold\", \"color\": \"black\"},\n",
    ")\n",
    "\n",
    "# (C) ë ˆì´ë¸” ë° íƒ€ì´í‹€ ì„¤ì •\n",
    "plt.title(\n",
    "    \"SHAP Directionality (Selected Features)\\n(Color=Direction, Text=Mean|SHAP|)\",\n",
    "    fontsize=font_sizes[\"title\"],\n",
    "    fontweight=\"bold\",\n",
    "    pad=25,\n",
    ")\n",
    "plt.xlabel(\"Time Point\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.ylabel(\"Feature\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.xticks(fontsize=font_sizes[\"tick\"], rotation=0)\n",
    "plt.yticks(fontsize=font_sizes[\"tick\"])\n",
    "\n",
    "# (D) ì»¬ëŸ¬ë°” í°íŠ¸ ë° ìœ„ì¹˜ ì¡°ì •\n",
    "cbar = plt.gca().collections[0].colorbar\n",
    "cbar.set_label(\n",
    "    cbar_kws_dict[\"label\"], fontsize=font_sizes[\"colorbar_label\"], fontweight=\"bold\"\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=font_sizes[\"tick\"])\n",
    "\n",
    "# (E) ì €ì¥\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # íƒ€ì´í‹€ ê³µê°„ í™•ë³´\n",
    "save_path = BASE_DIR / \"Analysis1_SHAP_Heatmap_All_Selected.png\"\n",
    "plt.savefig(save_path, dpi=400)\n",
    "plt.close()\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "# # -----------------------------------------------------------------\n",
    "# # 5. CSV íŒŒì¼ ì €ì¥ (Strongest, Consistency, Domain, Symbol + Importance í†µí•©)\n",
    "# # -----------------------------------------------------------------\n",
    "# print(\"ìµœì¢… ìƒì„¸ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "\n",
    "# # (A) ë‹¨ìˆœí™”ëœ ê¸°í˜¸ ì •ì˜ (ê¸°ì¤€ 0.3ìœ¼ë¡œ ìƒí–¥)\n",
    "# def get_corr_symbol_simple(val):\n",
    "#     if pd.isna(val):\n",
    "#         return \"\"\n",
    "#     if val >= 0.3:\n",
    "#         return \"ğŸ”º\"\n",
    "#     if val <= -0.3:\n",
    "#         return \"â–¼\"\n",
    "#     return \"â€¢ (Neutral)\"\n",
    "\n",
    "\n",
    "# dir_symbol_top = dir_top.applymap(get_corr_symbol_simple)\n",
    "# # imp_top_filteredëŠ” 3ë²ˆ ì„¹ì…˜ì—ì„œ ì´ë¯¸ dir_topê³¼ ë™ì¼í•˜ê²Œ ì •ë ¬ë˜ì—ˆìŒ\n",
    "\n",
    "# # (B) í†µí•© í…Œì´ë¸” ìƒì„±\n",
    "# combined_table = pd.DataFrame(index=dir_top.index)\n",
    "# for tp in time_order:\n",
    "#     if tp in dir_symbol_top.columns:\n",
    "#         combined_table[f\"{tp}_direction\"] = dir_symbol_top[tp]\n",
    "#         # [ìˆ˜ì •] ì†Œìˆ˜ì  4ìë¦¬ ë°˜ì˜¬ë¦¼\n",
    "#         combined_table[f\"{tp}_SHAP(mean)\"] = imp_top_filtered[tp].round(4)\n",
    "\n",
    "# # (C) Strongest window, Domain ì¶”ê°€\n",
    "# strongest_window = imp_top_filtered.idxmax(axis=1)\n",
    "# domains = combined_table.index.map(infer_domain)\n",
    "\n",
    "# # (D) Top 20 Count (n/5) ê³„ì‚° (ì»¬ëŸ¼ëª… ë³€ê²½)\n",
    "# rank_pivot = imp_pivot.rank(axis=0, method=\"min\", ascending=False)\n",
    "# rank_pivot_top = rank_pivot.loc[dir_top.index]\n",
    "# is_top_n = rank_pivot_top <= TOP_N\n",
    "# consistency_count = is_top_n.sum(axis=1)\n",
    "# consistency_str = consistency_count.astype(str) + f\"/{len(time_order)}\"\n",
    "\n",
    "# # [ìˆ˜ì •] ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½\n",
    "# combined_table.insert(0, \"Domain\", domains)\n",
    "# combined_table[\"Strongest Window\"] = strongest_window\n",
    "# combined_table[\"Top 20 Count (n/5)\"] = consistency_str\n",
    "\n",
    "# # (F) íŒŒì¼ë¡œ ì €ì¥\n",
    "# combined_table.to_excel(BASE_DIR / \"short_fig/Analysis1_SHAP_Domain_Symbol_Importance_FINAL_Summary.xlsx\")\n",
    "# print(\n",
    "#     f\"â˜…â˜…â˜… ìµœì¢… ìƒì„¸ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {BASE_DIR / 'short_fig/Analysis1_SHAP_Domain_Symbol_Importance_FINAL_Summary.xlsx'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------\n",
    "# # 6. Top 20 ë„ë©”ì¸ ë¹„ìœ¨ ìš”ì•½ í…Œì´ë¸”\n",
    "# # -----------------------------------------------------------------\n",
    "# print(\"\\nTop 20 ë„ë©”ì¸ ìš”ì•½ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# # (A) ê° ì‹œì ë³„ Top 20 ë³€ìˆ˜ ì¶”ì¶œ ë° ë„ë©”ì¸ ê³„ì‚°\n",
    "# top_n_features_by_time = (\n",
    "#     full_imp.groupby(\"time_point\")\n",
    "#     .apply(lambda x: x.nlargest(TOP_N, \"mean_abs_shap\"))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "# top_n_features_by_time[\"Domain\"] = top_n_features_by_time[\"feature\"].apply(infer_domain)\n",
    "\n",
    "# # (B) ì‹œì ë³„ ë„ë©”ì¸ ì¹´ìš´íŠ¸ ì§‘ê³„\n",
    "# domain_counts_pivot = (\n",
    "#     top_n_features_by_time.groupby(\"time_point\")[\"Domain\"]\n",
    "#     .value_counts()\n",
    "#     .unstack(level=\"time_point\")\n",
    "#     .fillna(0)\n",
    "#     .astype(int)\n",
    "# )\n",
    "# domain_counts_pivot = domain_counts_pivot.reindex(columns=time_order, fill_value=0)\n",
    "\n",
    "# # (C) ë¹„ìœ¨(%) ê³„ì‚°\n",
    "# domain_proportions_pivot = domain_counts_pivot / TOP_N * 100\n",
    "\n",
    "# # (D) ìµœì¢… í¬ë§·íŒ…: \"Proportion % (Count)\"\n",
    "# formatted_table = pd.DataFrame(\n",
    "#     index=domain_counts_pivot.index, columns=domain_counts_pivot.columns\n",
    "# )\n",
    "# for col in formatted_table.columns:\n",
    "#     counts = domain_counts_pivot[col]\n",
    "#     props = domain_proportions_pivot[col]\n",
    "#     formatted_table[col] = [\n",
    "#         f\"{p:.1f}% ({c})\" if c > 0 else \"-\" for c, p in zip(counts, props)\n",
    "#     ]\n",
    "\n",
    "# # (E) ì´í•©(Total) í–‰ ì¶”ê°€\n",
    "# total_counts = domain_counts_pivot.sum()\n",
    "# total_row = [f\"100.0% ({c})\" for c in total_counts]\n",
    "# formatted_table.loc[\"Total\"] = total_row\n",
    "\n",
    "# # (F) íŒŒì¼ë¡œ ì €ì¥\n",
    "# domain_table_path = BASE_DIR / \"Analysis1_SHAP_Domain_Proportions_Summary.xlsx\"\n",
    "# formatted_table.to_excel(domain_table_path)\n",
    "\n",
    "# print(f\"â˜…â˜…â˜… ì‹ ê·œ ë„ë©”ì¸ ë¹„ìœ¨ ìš”ì•½ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {domain_table_path}\")\n",
    "# print(\"\\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1. ì„¤ì •: íŒŒì¼ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ\n",
    "# -----------------------------------------------------------------\n",
    "# â€¼ï¸â€¼ï¸ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ì´ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. â€¼ï¸â€¼ï¸\n",
    "BASE_DIR = Path(\n",
    "    str(PROJECT_ROOT / \"results/new_analysis/modeling/step2_modeling/simple_20\")\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# (ê³µìš©) ë„ë©”ì¸ ë¶„ë¥˜ í•¨ìˆ˜ (ë¨¼ì € ì •ì˜)\n",
    "# -----------------------------------------------------------------\n",
    "def infer_domain(feat: str) -> str:\n",
    "    t = str(feat).lower().strip()\n",
    "    # ... (ì œê³µí•´ì£¼ì‹  ë„ë©”ì¸ ë¶„ë¥˜ í•¨ìˆ˜ ë‚´ìš© ì „ì²´) ...\n",
    "    LLM_ALIASES = {\n",
    "        \"Aggression (LLM)\": [\"aggression\", \"aggressive\"],\n",
    "        \"Interpersonal Conflict (LLM)\": [\n",
    "            \"interpersonal conflict\",\n",
    "            \"relationship conflict\",\n",
    "            \"family conflict\",\n",
    "            \"interpersonal\",\n",
    "        ],\n",
    "        \"Impaired Social Functioning (LLM)\": [\n",
    "            \"impaired social functioning\",\n",
    "            \"social function impairment\",\n",
    "            \"social functioning impairment\",\n",
    "        ],\n",
    "        \"Alcohol Use Issues (LLM)\": [\n",
    "            \"alcohol use issues\",\n",
    "            \"alcohol use\",\n",
    "            \"alcohol\",\n",
    "            \"drinking\",\n",
    "        ],\n",
    "        \"Hallucinations / Delusions / Paranoia (LLM)\": [\n",
    "            \"hallucinations_delusions_paranoia\",\n",
    "            \"hallucinations\",\n",
    "            \"delusions\",\n",
    "            \"paranoia\",\n",
    "            \"halluc/delusion/paranoia\",\n",
    "        ],\n",
    "        \"Lack of Support (LLM)\": [\n",
    "            \"lack family social support\",\n",
    "            \"lack of support\",\n",
    "            \"poor support\",\n",
    "            \"no support\",\n",
    "            \"social support deficit\",\n",
    "        ],\n",
    "        \"Family Loss (LLM)\": [\n",
    "            \"loss_of_family\",\n",
    "            \"family loss\",\n",
    "            \"loss / grief\",\n",
    "            \"bereavement\",\n",
    "        ],\n",
    "        \"Abuse / Sexual Victimization (LLM)\": [\n",
    "            \"abuse_and_sexual_victimization\",\n",
    "            \"abuse\",\n",
    "            \"victimization\",\n",
    "            \"sexual\",\n",
    "            \"violence\",\n",
    "            \"victim\",\n",
    "        ],\n",
    "    }\n",
    "    for dom, keys in LLM_ALIASES.items():\n",
    "        if any(k in t for k in keys) and \"llm\" in t:\n",
    "            return dom\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"er visit\",\n",
    "            \"â‰¥2 er\",\n",
    "            \">=2 er\",\n",
    "            \"â‰¥3 er\",\n",
    "            \">=3 er\",\n",
    "            \"admission\",\n",
    "            \"admit\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Utilization\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"bl\",\n",
    "            \"bl3\",\n",
    "            \"serum\",\n",
    "            \"cholesterol\",\n",
    "            \"hdl\",\n",
    "            \"ldl\",\n",
    "            \"triglyceride\",\n",
    "            \"crp\",\n",
    "            \"anc\",\n",
    "            \"sodium\",\n",
    "            \"na\",\n",
    "            \"potassium\",\n",
    "            \"hematocrit\",\n",
    "            \"mchc\",\n",
    "            \"eosinophil\",\n",
    "            \"basophil\",\n",
    "            \"lymphocyte\",\n",
    "            \"monocyte\",\n",
    "            \"acl\",\n",
    "            \"platelet\",\n",
    "            \"creatinine\",\n",
    "            \"ast\",\n",
    "            \"alt\",\n",
    "            \"alp\",\n",
    "            \"bun\",\n",
    "            \"calcium\",\n",
    "            \"phospho\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Lab\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"mmpi\",\n",
    "            \"hy\",\n",
    "            \"mf\",\n",
    "            \"vrin\",\n",
    "            \"trin\",\n",
    "            \"trinf\",\n",
    "            \"psychometric\",\n",
    "            \"audit\",\n",
    "            \"asrs\",\n",
    "            \"bai\",\n",
    "            \"bhs\",\n",
    "            \"ham\",\n",
    "            \"hcl\",\n",
    "            \"perceptual\",\n",
    "            \"appq\",\n",
    "            \"bprs\",\n",
    "            \"gaf\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Psychometric\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"olanz\",\n",
    "            \"lith\",\n",
    "            \"bzd\",\n",
    "            \"quetiap\",\n",
    "            \"antipsych\",\n",
    "            \"medic\",\n",
    "            \"benzodiazepine\",\n",
    "            \"risper\",\n",
    "            \"valpro\",\n",
    "            \"ssri\",\n",
    "            \"snri\",\n",
    "            \"quetiapine\",\n",
    "            \"aripip\",\n",
    "            \"olanzapine\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Treatment\"\n",
    "    if \"sleep\" in t:\n",
    "        return \"Sleep\"\n",
    "    if any(\n",
    "        k in t\n",
    "        for k in [\n",
    "            \"sex\",\n",
    "            \"marital\",\n",
    "            \"age\",\n",
    "            \"education\",\n",
    "            \"job\",\n",
    "            \"occupation\",\n",
    "            \"income\",\n",
    "            \"smoking\",\n",
    "            \"drinking\",\n",
    "            \"psychiatric family history\",\n",
    "            \"hospitalization period\",\n",
    "            \"length of stay\",\n",
    "            \"other psychotic\",\n",
    "            \"somatic symptom disorder\",\n",
    "        ]\n",
    "    ):\n",
    "        return \"Demographic/Psychosocial\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. ë°ì´í„° ë¡œë”©\n",
    "# -----------------------------------------------------------------\n",
    "print(\"íŒŒì¼ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# (A) Load LONG files\n",
    "long_files = [\n",
    "    (\"30d\", \"label_30d__SHAP_long_Test.csv\"),\n",
    "    (\"60d\", \"label_60d__SHAP_long_Test.csv\"),\n",
    "    (\"90d\", \"label_90d__SHAP_long_Test.csv\"),\n",
    "    (\"180d\", \"label_180d__SHAP_long_Test.csv\"),\n",
    "    (\"365d\", \"label_365d__SHAP_long_Test.csv\"),\n",
    "]\n",
    "long_dfs = []\n",
    "for tp, fname in long_files:\n",
    "    file_path = BASE_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"time_point\"] = tp\n",
    "        long_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "if not long_dfs:\n",
    "    raise ValueError(\"Long íŒŒì¼ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "full_long = pd.concat(long_dfs, ignore_index=True)\n",
    "\n",
    "# (B) Load IMPORTANCE files\n",
    "imp_files = [\n",
    "    (\"30d\", \"label_30d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"60d\", \"label_60d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"90d\", \"label_90d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"180d\", \"label_180d__SHAP_feature_importance_Test.csv\"),\n",
    "    (\"365d\", \"label_365d__SHAP_feature_importance_Test.csv\"),\n",
    "]\n",
    "imp_dfs = []\n",
    "for tp, fname in imp_files:\n",
    "    file_path = BASE_DIR / fname\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"time_point\"] = tp\n",
    "        imp_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: Importance íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "if not imp_dfs:\n",
    "    raise ValueError(\"Importance íŒŒì¼ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "full_imp = pd.concat(imp_dfs, ignore_index=True)\n",
    "\n",
    "print(\"íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ìƒê´€ê´€ê³„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. ìƒê´€ê´€ê³„ ê³„ì‚° ë° ë°ì´í„° ê°€ê³µ (ì‚¬ìš©ì ì§€ì • ë³€ìˆ˜ & ìˆœì„œ ìœ ì§€)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# (A) ìƒê´€ê´€ê³„ ê³„ì‚° í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)\n",
    "def calc_correlation(g):\n",
    "    if g[\"feature_value\"].nunique() <= 1 or g[\"shap_value\"].nunique() <= 1:\n",
    "        corr = 0\n",
    "    else:\n",
    "        corr, _ = spearmanr(g[\"feature_value\"], g[\"shap_value\"])\n",
    "        if np.isnan(corr):\n",
    "            corr = 0\n",
    "    return pd.Series({\"corr_direction\": corr})\n",
    "\n",
    "\n",
    "print(\"ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...\")\n",
    "metrics_df = (\n",
    "    full_long.groupby([\"time_point\", \"feature\"]).apply(calc_correlation).reset_index()\n",
    ")\n",
    "\n",
    "time_order = [\"30d\", \"60d\", \"90d\", \"180d\", \"365d\"]\n",
    "\n",
    "# (B) ì „ì²´ í”¼ë²— í…Œì´ë¸” ìƒì„±\n",
    "# 1. Annotation Data (Importance)\n",
    "imp_pivot = full_imp.pivot(\n",
    "    index=\"feature\", columns=\"time_point\", values=\"mean_abs_shap\"\n",
    ").reindex(columns=time_order)\n",
    "\n",
    "# 2. Color Data (Correlation)\n",
    "dir_pivot = metrics_df.pivot(\n",
    "    index=\"feature\", columns=\"time_point\", values=\"corr_direction\"\n",
    ").reindex(columns=time_order)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# [ìˆ˜ì •] Top N ìë™ ì¶”ì¶œ ëŒ€ì‹  ì‚¬ìš©ì ì§€ì • ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# â€¼ï¸â€¼ï¸ ì—¬ê¸°ì— ì‹œê°í™”í•˜ê³  ì‹¶ì€ ë³€ìˆ˜ ì´ë¦„ì„ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš” â€¼ï¸â€¼ï¸\n",
    "TARGET_FEATURES = [\n",
    "    \"â‰¥2 ER Visits\",\n",
    "    \"â‰¥3 Admissions\",\n",
    "    \"ANC (BL201815)\",\n",
    "    \"CRP (C-Reactive Protein) (BL3140)\",\n",
    "    \"Glucose, Fasting (BL3118)\",\n",
    "    \"Abuse/Victimization (LLM)\",\n",
    "    \"Halluc/Delusion/Paranoia (LLM)\",\n",
    "    \"Lack of Support (LLM)\",\n",
    "    \"Interpersonal Conflict (LLM)\",\n",
    "    # ... í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ê³„ì† ì¶”ê°€í•˜ì„¸ìš”\n",
    "]\n",
    "\n",
    "# (C) ë°ì´í„° í•„í„°ë§ (ì§€ì •í•œ ë³€ìˆ˜ë§Œ ë‚¨ê¸°ê¸°)\n",
    "available_features = imp_pivot.index.tolist()\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì€ ì›ë˜ ë¦¬ìŠ¤íŠ¸(TARGET_FEATURES)ì˜ ìˆœì„œë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "selected_features = [f for f in TARGET_FEATURES if f in available_features]\n",
    "missing_features = set(TARGET_FEATURES) - set(selected_features)\n",
    "\n",
    "if missing_features:\n",
    "    print(\n",
    "        f\"\\n[Warning] ë‹¤ìŒ ë³€ìˆ˜ë“¤ì€ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ì§€ ì•Šì•„ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤:\\n{missing_features}\\n\"\n",
    "    )\n",
    "\n",
    "if not selected_features:\n",
    "    raise ValueError(\"ì„ íƒëœ ë³€ìˆ˜ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤. ë³€ìˆ˜ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# ì„ íƒëœ ë³€ìˆ˜ & ìˆœì„œ ê·¸ëŒ€ë¡œ ë°ì´í„°í”„ë ˆì„ ì¬ì •ë ¬ (reindex ì‚¬ìš©)\n",
    "dir_top = dir_pivot.reindex(selected_features)\n",
    "imp_top_filtered = imp_pivot.reindex(selected_features)\n",
    "\n",
    "print(f\"ì´ {len(dir_top)}ê°œì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©ìê°€ ì§€ì •í•œ ìˆœì„œëŒ€ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. (ìˆ˜ì •) í†µí•© í”Œë¡¯ ê·¸ë¦¬ê¸° (í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ì €ì¥)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(f\"ì´ {len(dir_top)}ê°œì˜ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì˜ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# (A) í”Œë¡¯ ì„¤ì •\n",
    "# ë³€ìˆ˜ ê°œìˆ˜ì— ë”°ë¼ ë†’ì´ ìë™ ì¡°ì ˆ (ë³€ìˆ˜ë‹¹ 0.5ì¸ì¹˜ + ê¸°ë³¸ ì—¬ë°± 3ì¸ì¹˜)\n",
    "# ë³€ìˆ˜ê°€ ë§ì•„ì ¸ë„ ì…€ì´ ì°Œê·¸ëŸ¬ì§€ì§€ ì•Šë„ë¡ ë†’ì´ë¥¼ ë™ì ìœ¼ë¡œ í• ë‹¹í•©ë‹ˆë‹¤.\n",
    "fig_height = len(dir_top) * 0.5 + 3\n",
    "plt.figure(figsize=(12, fig_height))\n",
    "\n",
    "font_sizes = {\"title\": 18, \"label\": 14, \"tick\": 12, \"annot\": 16, \"colorbar_label\": 14}\n",
    "\n",
    "# ì»¬ëŸ¬ë°” ì„¤ì •\n",
    "cbar_kws_dict = {\n",
    "    \"label\": \"Spearman Correlation (Feature Value vs. SHAP Value)\",\n",
    "    \"shrink\": 0.6,  # ì „ì²´ í”Œë¡¯ ë†’ì´ì— ë§ì¶° ì»¬ëŸ¬ë°” ê¸¸ì´ ë¹„ìœ¨ ì¡°ì •\n",
    "    \"ticklocation\": \"right\",\n",
    "}\n",
    "\n",
    "# (B) íˆíŠ¸ë§µ ê·¸ë¦¬ê¸° (ë¶„í•  ì—†ì´ dir_top ì „ì²´ ì‚¬ìš©)\n",
    "sns.heatmap(\n",
    "    data=dir_top,  # â¬…ï¸ ìƒ‰ìƒ: ìƒê´€ê´€ê³„ (ì „ì²´)\n",
    "    annot=imp_top_filtered,  # â¬…ï¸ ìˆ«ì: ì¤‘ìš”ë„ (ì „ì²´)\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    linewidths=1.2,\n",
    "    linecolor=\"gray\",\n",
    "    cbar_kws=cbar_kws_dict,\n",
    "    annot_kws={\"size\": font_sizes[\"annot\"], \"weight\": \"bold\", \"color\": \"black\"},\n",
    ")\n",
    "\n",
    "# (C) ë ˆì´ë¸” ë° íƒ€ì´í‹€ ì„¤ì •\n",
    "plt.title(\n",
    "    \"SHAP Directionality (Selected Features)\\n(Color=Direction, Text=Mean|SHAP|)\",\n",
    "    fontsize=font_sizes[\"title\"],\n",
    "    fontweight=\"bold\",\n",
    "    pad=25,\n",
    ")\n",
    "plt.xlabel(\"Time Point\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.ylabel(\"Feature\", fontsize=font_sizes[\"label\"], fontweight=\"bold\")\n",
    "plt.xticks(fontsize=font_sizes[\"tick\"], rotation=0)\n",
    "plt.yticks(fontsize=font_sizes[\"tick\"])\n",
    "\n",
    "# (D) ì»¬ëŸ¬ë°” í°íŠ¸ ë° ìœ„ì¹˜ ì¡°ì •\n",
    "cbar = plt.gca().collections[0].colorbar\n",
    "cbar.set_label(\n",
    "    cbar_kws_dict[\"label\"], fontsize=font_sizes[\"colorbar_label\"], fontweight=\"bold\"\n",
    ")\n",
    "cbar.ax.tick_params(labelsize=font_sizes[\"tick\"])\n",
    "\n",
    "# (E) ì €ì¥\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # íƒ€ì´í‹€ ê³µê°„ í™•ë³´\n",
    "save_path = BASE_DIR / \"Analysis1_SHAP_Heatmap_All_Selected.png\"\n",
    "plt.savefig(save_path, dpi=400)\n",
    "plt.close()\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {save_path}\")\n",
    "\n",
    "# # -----------------------------------------------------------------\n",
    "# # 5. CSV íŒŒì¼ ì €ì¥ (Strongest, Consistency, Domain, Symbol + Importance í†µí•©)\n",
    "# # -----------------------------------------------------------------\n",
    "# print(\"ìµœì¢… ìƒì„¸ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "\n",
    "# # (A) ë‹¨ìˆœí™”ëœ ê¸°í˜¸ ì •ì˜ (ê¸°ì¤€ 0.3ìœ¼ë¡œ ìƒí–¥)\n",
    "# def get_corr_symbol_simple(val):\n",
    "#     if pd.isna(val):\n",
    "#         return \"\"\n",
    "#     if val >= 0.3:\n",
    "#         return \"ğŸ”º\"\n",
    "#     if val <= -0.3:\n",
    "#         return \"â–¼\"\n",
    "#     return \"â€¢ (Neutral)\"\n",
    "\n",
    "\n",
    "# dir_symbol_top = dir_top.applymap(get_corr_symbol_simple)\n",
    "# # imp_top_filteredëŠ” 3ë²ˆ ì„¹ì…˜ì—ì„œ ì´ë¯¸ dir_topê³¼ ë™ì¼í•˜ê²Œ ì •ë ¬ë˜ì—ˆìŒ\n",
    "\n",
    "# # (B) í†µí•© í…Œì´ë¸” ìƒì„±\n",
    "# combined_table = pd.DataFrame(index=dir_top.index)\n",
    "# for tp in time_order:\n",
    "#     if tp in dir_symbol_top.columns:\n",
    "#         combined_table[f\"{tp}_direction\"] = dir_symbol_top[tp]\n",
    "#         # [ìˆ˜ì •] ì†Œìˆ˜ì  4ìë¦¬ ë°˜ì˜¬ë¦¼\n",
    "#         combined_table[f\"{tp}_SHAP(mean)\"] = imp_top_filtered[tp].round(4)\n",
    "\n",
    "# # (C) Strongest window, Domain ì¶”ê°€\n",
    "# strongest_window = imp_top_filtered.idxmax(axis=1)\n",
    "# domains = combined_table.index.map(infer_domain)\n",
    "\n",
    "# # (D) Top 20 Count (n/5) ê³„ì‚° (ì»¬ëŸ¼ëª… ë³€ê²½)\n",
    "# rank_pivot = imp_pivot.rank(axis=0, method=\"min\", ascending=False)\n",
    "# rank_pivot_top = rank_pivot.loc[dir_top.index]\n",
    "# is_top_n = rank_pivot_top <= TOP_N\n",
    "# consistency_count = is_top_n.sum(axis=1)\n",
    "# consistency_str = consistency_count.astype(str) + f\"/{len(time_order)}\"\n",
    "\n",
    "# # [ìˆ˜ì •] ì»¬ëŸ¼ ìˆœì„œ ë³€ê²½\n",
    "# combined_table.insert(0, \"Domain\", domains)\n",
    "# combined_table[\"Strongest Window\"] = strongest_window\n",
    "# combined_table[\"Top 20 Count (n/5)\"] = consistency_str\n",
    "\n",
    "# # (F) íŒŒì¼ë¡œ ì €ì¥\n",
    "# combined_table.to_excel(BASE_DIR / \"short_fig/Analysis1_SHAP_Domain_Symbol_Importance_FINAL_Summary.xlsx\")\n",
    "# print(\n",
    "#     f\"â˜…â˜…â˜… ìµœì¢… ìƒì„¸ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {BASE_DIR / 'short_fig/Analysis1_SHAP_Domain_Symbol_Importance_FINAL_Summary.xlsx'}\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # -----------------------------------------------------------------\n",
    "# # 6. Top 20 ë„ë©”ì¸ ë¹„ìœ¨ ìš”ì•½ í…Œì´ë¸”\n",
    "# # -----------------------------------------------------------------\n",
    "# print(\"\\nTop 20 ë„ë©”ì¸ ìš”ì•½ í…Œì´ë¸” ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# # (A) ê° ì‹œì ë³„ Top 20 ë³€ìˆ˜ ì¶”ì¶œ ë° ë„ë©”ì¸ ê³„ì‚°\n",
    "# top_n_features_by_time = (\n",
    "#     full_imp.groupby(\"time_point\")\n",
    "#     .apply(lambda x: x.nlargest(TOP_N, \"mean_abs_shap\"))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "# top_n_features_by_time[\"Domain\"] = top_n_features_by_time[\"feature\"].apply(infer_domain)\n",
    "\n",
    "# # (B) ì‹œì ë³„ ë„ë©”ì¸ ì¹´ìš´íŠ¸ ì§‘ê³„\n",
    "# domain_counts_pivot = (\n",
    "#     top_n_features_by_time.groupby(\"time_point\")[\"Domain\"]\n",
    "#     .value_counts()\n",
    "#     .unstack(level=\"time_point\")\n",
    "#     .fillna(0)\n",
    "#     .astype(int)\n",
    "# )\n",
    "# domain_counts_pivot = domain_counts_pivot.reindex(columns=time_order, fill_value=0)\n",
    "\n",
    "# # (C) ë¹„ìœ¨(%) ê³„ì‚°\n",
    "# domain_proportions_pivot = domain_counts_pivot / TOP_N * 100\n",
    "\n",
    "# # (D) ìµœì¢… í¬ë§·íŒ…: \"Proportion % (Count)\"\n",
    "# formatted_table = pd.DataFrame(\n",
    "#     index=domain_counts_pivot.index, columns=domain_counts_pivot.columns\n",
    "# )\n",
    "# for col in formatted_table.columns:\n",
    "#     counts = domain_counts_pivot[col]\n",
    "#     props = domain_proportions_pivot[col]\n",
    "#     formatted_table[col] = [\n",
    "#         f\"{p:.1f}% ({c})\" if c > 0 else \"-\" for c, p in zip(counts, props)\n",
    "#     ]\n",
    "\n",
    "# # (E) ì´í•©(Total) í–‰ ì¶”ê°€\n",
    "# total_counts = domain_counts_pivot.sum()\n",
    "# total_row = [f\"100.0% ({c})\" for c in total_counts]\n",
    "# formatted_table.loc[\"Total\"] = total_row\n",
    "\n",
    "# # (F) íŒŒì¼ë¡œ ì €ì¥\n",
    "# domain_table_path = BASE_DIR / \"Analysis1_SHAP_Domain_Proportions_Summary.xlsx\"\n",
    "# formatted_table.to_excel(domain_table_path)\n",
    "\n",
    "# print(f\"â˜…â˜…â˜… ì‹ ê·œ ë„ë©”ì¸ ë¹„ìœ¨ ìš”ì•½ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {domain_table_path}\")\n",
    "# print(\"\\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}