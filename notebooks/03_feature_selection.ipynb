{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Feature Selection\n",
    "\n",
    "**Step 1**: Train-first split with correlation filtering\n",
    "**Step 2**: Univariate (t-test/chi-square) + LGBM importance → combined rank → top-N core features (global union)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_ind, chi2_contingency, fisher_exact\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config import PROJECT_ROOT, SPLIT_SEED, MODEL_SEED, LABELS\n",
    "from src.variables import (\n",
    "    CATEGORY_COLS, LLM_COLS, LAB_COLS, CODE_COLS, TARGET_COLS, ID_COLS\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Train-first split & correlation filtering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Paths ──\n",
    "RAW_DATA_PATH = PROJECT_ROOT / \"data/raw/ADER_windowday_dataset_number_with_llm_v2.csv\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data/new_analysis/corr0.7_filtered_data/split_result\"\n",
    "CORR_OUTPUT_DIR = PROJECT_ROOT / \"results/new_analysis/split_correlation\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CORR_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_df = pd.read_csv(RAW_DATA_PATH)\n",
    "\n",
    "# Split first, then filter (to avoid data leakage)\n",
    "train_df, test_df = train_test_split(\n",
    "    raw_df, test_size=0.3, random_state=SPLIT_SEED, stratify=raw_df[\"label_30d\"]\n",
    ")\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Missing rate > 60% removal (train-based)\n",
    "missing_rate_train = train_df.isna().mean()\n",
    "high_missing_cols = missing_rate_train[missing_rate_train > 0.60].index.tolist()\n",
    "if high_missing_cols:\n",
    "    print(f\"Removing {len(high_missing_cols)} cols with >60% missing: {high_missing_cols}\")\n",
    "    train_df = train_df.drop(columns=high_missing_cols)\n",
    "    test_df = test_df.drop(columns=high_missing_cols)\n",
    "\n",
    "# Zero variance removal (train-based)\n",
    "numeric_cols_for_var = train_df.select_dtypes(include=np.number).columns.drop(\n",
    "    ID_COLS + TARGET_COLS, errors=\"ignore\"\n",
    ")\n",
    "zero_var_cols = [col for col in numeric_cols_for_var if train_df[col].std() == 0]\n",
    "if zero_var_cols:\n",
    "    print(f\"Removing {len(zero_var_cols)} zero-variance cols: {zero_var_cols}\")\n",
    "    train_df = train_df.drop(columns=zero_var_cols)\n",
    "    test_df = test_df.drop(columns=zero_var_cols)\n",
    "\n",
    "print(f\"After filtering: Train {train_df.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Correlation > 0.7 removal (train-based, continuous cols only)\n",
    "all_cols = train_df.columns.tolist()\n",
    "exclude_cols = set(ID_COLS + TARGET_COLS + CATEGORY_COLS + LLM_COLS + CODE_COLS)\n",
    "continuous_cols = [\n",
    "    c for c in all_cols\n",
    "    if c not in exclude_cols and pd.api.types.is_numeric_dtype(train_df[c])\n",
    "]\n",
    "\n",
    "X_train_temp = train_df[continuous_cols].fillna(train_df[continuous_cols].median())\n",
    "corr_matrix = X_train_temp.corr(method=\"pearson\")\n",
    "\n",
    "THRESH = 0.70\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "pairs = upper_tri.stack().reset_index()\n",
    "pairs.columns = [\"feature_1\", \"feature_2\", \"r\"]\n",
    "pairs[\"abs_r\"] = pairs[\"r\"].abs()\n",
    "high_corr_pairs = pairs[pairs[\"abs_r\"] > THRESH].sort_values(\"abs_r\", ascending=False)\n",
    "high_corr_pairs[\"missing_rate_1\"] = high_corr_pairs[\"feature_1\"].map(missing_rate_train)\n",
    "high_corr_pairs[\"missing_rate_2\"] = high_corr_pairs[\"feature_2\"].map(missing_rate_train)\n",
    "high_corr_pairs.to_csv(CORR_OUTPUT_DIR / \"high_corr_pairs_train_based.csv\", index=False)\n",
    "\n",
    "to_drop = set()\n",
    "for _, row in high_corr_pairs.iterrows():\n",
    "    f1, f2 = row[\"feature_1\"], row[\"feature_2\"]\n",
    "    if missing_rate_train.get(f1, 0) >= missing_rate_train.get(f2, 0):\n",
    "        to_drop.add(f1)\n",
    "    else:\n",
    "        to_drop.add(f2)\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"Removing {len(to_drop)} high-correlation cols\")\n",
    "    train_df = train_df.drop(columns=sorted(to_drop))\n",
    "    test_df = test_df.drop(columns=sorted(to_drop))\n",
    "\n",
    "print(f\"Final: Train {train_df.shape}\")\n",
    "\n",
    "train_df.to_csv(OUTPUT_DIR / \"final_train_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "test_df.to_csv(OUTPUT_DIR / \"final_test_dataset.csv\", index=False, encoding=\"utf-8-sig\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Global union feature selection (univariate + LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Paths ──\n",
    "DATA_DIR = PROJECT_ROOT / \"data/processed_imp/260114_split_corr_LLM_ADER/imputation/simple_imput\"  # ⚠️ Adjust per experiment\n",
    "OUT_DIR = PROJECT_ROOT / \"results/new_analysis/260114_qwen/Feature_Selection/simple_20/step2_FS\"  # ⚠️ Adjust per experiment\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CORE_N = 20\n",
    "FINAL_N = 20\n",
    "TOP_N_FOR_CORE = 30\n",
    "\n",
    "# For FS, CATEGORY_COLS includes CODE_COLS (sleep/appetite/weight treated as categorical here)\n",
    "FS_CATEGORY_COLS = list(CATEGORY_COLS) + [c for c in CODE_COLS if c not in CATEGORY_COLS]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_p_values(df, target):\n",
    "    results = []\n",
    "    y = df[target]\n",
    "    feature_cols = [col for col in df.columns if col != target]\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if df[col].nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "        s = df[col]\n",
    "        p_value = np.nan\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            g0, g1 = s[y == 0].dropna(), s[y == 1].dropna()\n",
    "            if len(g0) > 1 and len(g1) > 1:\n",
    "                _, p_value = ttest_ind(g0, g1, equal_var=False)\n",
    "        else:\n",
    "            ct = pd.crosstab(s, y)\n",
    "            if ct.shape[0] > 1 and ct.shape[1] > 1:\n",
    "                if ct.shape == (2, 2) and (ct.values < 5).any():\n",
    "                    _, p_value = fisher_exact(ct)\n",
    "                else:\n",
    "                    _, p_value, _, _ = chi2_contingency(ct)\n",
    "        results.append({\"feature\": col, \"p_value\": p_value})\n",
    "\n",
    "    res_df = pd.DataFrame(results).dropna().sort_values(\"p_value\").reset_index(drop=True)\n",
    "    res_df[\"p_value_rank\"] = res_df.index + 1\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def calculate_model_importance(df, target):\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target])\n",
    "    for col in X.columns:\n",
    "        if col in FS_CATEGORY_COLS:\n",
    "            X[col] = X[col].astype(\"category\")\n",
    "\n",
    "    model = LGBMClassifier(random_state=MODEL_SEED, verbose=-1, is_unbalance=True)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    res_df = pd.DataFrame({\"feature\": X.columns, \"importance\": model.feature_importances_})\n",
    "    res_df = res_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    res_df[\"model_rank\"] = res_df.index + 1\n",
    "    return res_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run feature selection for each time point (excluding LLM cols)\n",
    "all_rankings = {}\n",
    "\n",
    "for label in LABELS:\n",
    "    file_path = DATA_DIR / f\"simple_{label}_train.csv\"  # ⚠️ Adjust prefix per imputation\n",
    "    if not file_path.exists():\n",
    "        print(f\"Skipping {label}: file not found\")\n",
    "        continue\n",
    "\n",
    "    df_train = pd.read_csv(file_path)\n",
    "    features_for_selection = [col for col in df_train.columns if col not in LLM_COLS]\n",
    "    df_for_fs = df_train[features_for_selection]\n",
    "\n",
    "    p_ranks = calculate_p_values(df_for_fs, label)\n",
    "    p_ranks.to_csv(OUT_DIR / f\"univariate_results_{label}.csv\", index=False)\n",
    "\n",
    "    m_ranks = calculate_model_importance(df_for_fs, label)\n",
    "    m_ranks.to_csv(OUT_DIR / f\"model_importance_{label}.csv\", index=False)\n",
    "\n",
    "    merged = pd.merge(p_ranks, m_ranks, on=\"feature\", how=\"outer\").fillna(len(df_for_fs.columns))\n",
    "    merged[\"combined_rank\"] = (merged[\"p_value_rank\"] + merged[\"model_rank\"]) / 2\n",
    "    merged = merged.sort_values(\"combined_rank\").reset_index(drop=True)\n",
    "    all_rankings[label] = merged\n",
    "    merged.to_csv(OUT_DIR / f\"ranking_{label}.csv\", index=False)\n",
    "    print(f\"[{label}] Ranking complete: {len(merged)} features\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Global core features (shared across all time points)\n",
    "rank_table = pd.concat([\n",
    "    df.set_index(\"feature\")[\"combined_rank\"].rename(label)\n",
    "    for label, df in all_rankings.items()\n",
    "], axis=1)\n",
    "max_rank = max(df[\"combined_rank\"].max() for df in all_rankings.values())\n",
    "rank_table = rank_table.fillna(max_rank + 1)\n",
    "rank_table[\"mean_rank\"] = rank_table.mean(axis=1)\n",
    "\n",
    "core_features = rank_table.sort_values(\"mean_rank\").head(CORE_N).index.tolist()\n",
    "\n",
    "# Save per-label final feature sets (all use the same core features)\n",
    "for label in LABELS:\n",
    "    if label not in all_rankings:\n",
    "        continue\n",
    "    pd.DataFrame({\"feature\": core_features}).to_csv(\n",
    "        OUT_DIR / f\"final_features_{label}.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "pd.DataFrame({\"feature\": core_features}).to_csv(\n",
    "    OUT_DIR / \"core_features.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCore features ({len(core_features)}): {core_features}\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}